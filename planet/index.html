
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Planet HTCondor - HTCondor Project</title>
  <meta name="author" content="HTCondor Project">

  
  <meta name="description" content="Most people set personal and professional goals. If you work in software, your near-term professional goals might sound like this: Ship the next &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://htcondor.github.com/planet/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="/javascripts/ender.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/planet/atom.xml" rel="alternate" title="HTCondor Project" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  

</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">HTCondor Project</a></h1>
  
    <h2>The website and blog for the HTCondor project on github.</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/planet/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:htcondor.github.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/planet">Planet HTCondor</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      

<br>
<h1 align="center"><u> Planet HTCondor </u></h1>

<div class=\"blog-index\">

  <article>
    <header>
      <h1 class="entry-title"><a href="http://chapeau.freevariable.com/2015/05/process-goals.html">Planning your career like a racing season</a></h1>
      <p class="meta">
        <time datetime="2015-05-14T21:04:20Z" pubdate data-updated="true">May 14<span>th</span>, 2015  &nbsp; &mdash; &nbsp; William Benton</time>
      </p>
    </header>
    <div class="entry-content"><p>Most people set personal and professional goals.  If you work in software, your near-term professional goals might sound like this:</p>

<ul>
<li>Ship the next version of our team&rsquo;s product without any undiscovered crashing bugs</li>
<li>Improve the performance of the caching layer by 10%</li>
<li>Become a committer on some particular open-source project</li>
</ul>


<p>Those probably sound pretty reasonable, right?  They aren&rsquo;t vague wishes, they&rsquo;re goals.  They probably aren&rsquo;t totally unrealistic, and you&rsquo;ll know you&rsquo;ve achieved them when some objective, measurable thing happens.  (They&rsquo;re <a href="http://en.wikipedia.org/wiki/SMART_criteria">SMART</a>, in other words.)</p>

<p>We&rsquo;ll come back to these goals.  First, though, I&rsquo;d like digress and talk about bike racing, and specifically about the kinds of goals an amateur racer might set for a year.  Maybe those would look like this:</p>

<ul>
<li>Stay healthy and avoid injuries all season</li>
<li>Improve my one-minute power output by 0.5 W/kg</li>
<li>Upgrade my racing category</li>
</ul>


<p>These are more or less analogous to the professional goals.  They&rsquo;re objective, measurable, and reasonable.  However, they&rsquo;re also at least partially out of your control.  You can&rsquo;t totally prevent yourself from getting sick or crashing.  Your one-minute power might be right at the limits of your genetic capabilities (or maybe your season shapes up so that it makes more sense to focus on improving other aspects of your fitness).  And the category upgrade is up to an official in your local cycling association &mdash; you can get an automatic upgrade by winning races, but whether or not you win is also largely out of your control.</p>

<p>I first read about the idea of <a href="http://velonews.competitor.com/2013/10/training-center/setting-goals-finding-your-perfect-process_306088"><em>process and outcome goals</em></a> in Joe Friel&rsquo;s <em>The Cyclist&rsquo;s Training Bible</em>, but I&rsquo;ve found them extremely helpful in talking about professional goals as well.  The idea is that there are two kinds of goals:  outcome goals, which are results, and process goals, which relate to how you prepare and execute.  Of course, you&rsquo;re ultimately interested in outcomes (winning races, shipping great products, becoming recognized as an authority in your field), but outcomes are always to some extent out of your control.</p>

<p>Instead of focusing on outcomes for goals, it makes more sense to focus on what you can control:  the processes that support those outcomes.  So, in the bike case, you could recast the outcome goals as:</p>

<ul>
<li>Make sure I get at least 7 hours of sleep a night and avoid sketchy racers in corners or sprints</li>
<li>Do plyometric exercises at least twice a week in the offseason</li>
<li>Identify key races that I can do well in and make sure to show up for them well-rested and with adequate nutrition; then focus on avoiding particular tactical mistakes I&rsquo;ve made in past races</li>
</ul>


<p>The software goals I mentioned at the beginning of the article are also outcome goals.  You can&rsquo;t guarantee that your product won&rsquo;t have any undiscovered crashing bugs, but you can sure put processes in place so that you&rsquo;re more likely to discover crashing bugs before you ship.  Similarly, a 10% performance improvement sounds great, but it might require unjustifiable effort or be unachievable (conversely, it might turn out that 10% improvement is insufficient to improve the behavior of the whole system)!  Community leadership positions always involve political as well as meritocratic considerations; while you can make a case for yourself as a good candidate, you can&rsquo;t force people to vote for you.</p>

<p>Recast as process goals, the software goals might look like this:</p>

<ul>
<li>Ensure that no new commit to our product introduces untested code, use property-based tests for all internal data structures, and incorporate <a href="http://en.wikipedia.org/wiki/Fuzz_testing">fuzzing</a> of every public interface into the QA process.</li>
<li>Profile the caching layer under a range of conditions and document the hot spots, identifying algorithmic and implementation-level avenues for performance improvement.  Add performance testing to CI.</li>
<li>Participate in the project community, answering questions on the mailing list every week and submitting at least one nontrivial patch a week.</li>
</ul>


<p>It&rsquo;s easy to imagine the rest of your career and think about outcomes you want to see:  earning a prestigious position, achieving acknowledged impact in your department or company, or writing a popular framework (or book).<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>  But those goals are often pretty far off from what you need to do to get there.  Setting process goals supports the outcomes that you want to achieve.  But the really remarkable thing is that they do so much more than that.  By focusing on improving your preparation and execution, you&rsquo;re making yourself a better engineer (or bike racer) even if you don&rsquo;t get the outcomes you want right away &mdash; and making it more likely you&rsquo;ll see great outcomes in the future.</p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p>Outcome goals are a particularly dangerous trap if you&rsquo;re coming from an environment like academia or open-source development, where outcome-related metrics like tenure, citation count, and number of GitHub &ldquo;stars&rdquo; are never far from your evaluation of your own career.  (The frictionless recognitions afforded by social media &ldquo;likes&rdquo; and &ldquo;favorites&rdquo; bring this headache to a much broader audience.)<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

</div>
  </article>

  <article>
    <header>
      <h1 class="entry-title"><a href="http://erikerlandson.github.com/blog/2015/05/06/parallel-k-medoids-using-scala-parseq/">Parallel K-Medoids Using Scala ParSeq</a></h1>
      <p class="meta">
        <time datetime="2015-05-06T23:33:00Z" pubdate data-updated="true">May 6<span>th</span>, 2015  &nbsp; &mdash; &nbsp; Erik Erlandson</time>
      </p>
    </header>
    <div class="entry-content"><p>Scala supplies a <a href="http://docs.scala-lang.org/overviews/parallel-collections/overview.html">parallel collections library</a> that was designed to make it easy for a programmer to add parallel computing over the elements in a collection.  In this post, I will describe a case study of applying Scala's parallel collections to cleanly implement multithreading support for training a K-Medoids clustering model.</p>

<h3>Motivation</h3>

<p><a href="http://en.wikipedia.org/wiki/K-medoids">K-Medoids clustering</a> is a relative of K-Means clustering that does not require an algebra over input data elements.  That is, K-Medoids requires only a distance metric defined on elements in the data space, and can cluster objects which do not have a well-defined concept of addition or division that is necessary for computing the <a href="http://en.wikipedia.org/wiki/Centroid">centroids</a> required by K-Means.  For example, K-Medoids can cluster character strings, which have a notion of <a href="http://en.wikipedia.org/wiki/Edit_distance">distance</a>, but no notion of summation that could be used to compute a geometric centroid.</p>

<p>This additional generality comes at a cost.  The medoid of a collection of elements is the member of the collection that minimizes some function F of the distances from that element to all the other elements in the collection.  For example, F might be the sum of distances from one element to all the elements, or perhaps the maximum distance, etc.  <strong>It is not hard to see that the cost of computing a medoid of (n) elements is quadratic in (n)</strong>: Evaluating F is linear in (n) and F in turn must be evaluated with respect to each element.  Furthermore, unlike centroid-based computations used in K-Means, computing a medoid does not naturally lend itself to common scale-out computing formalisms such as Spark RDDs, due to the full-cross-product nature of the computation.</p>

<p>With this in mind, a more traditional multithreading approach is a good candidate to achieve some practical parallelism on modern multi-core hardware.  I'll demonstrate that this is easy to implement in Scala with parallel sequences.</p>

<h3>Non-Parallel Code</h3>

<p>Consider a baseline non-parallel implementation of K-Medoids, as in the following example skeleton code.  (A working version of this code, under review at the time of this post, can be <a href="https://github.com/erikerlandson/silex/blob/parseq_blog/src/main/scala/com/redhat/et/silex/cluster/KMedoids.scala">viewed here</a>)</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span>A skeleton K-Medoids implementation </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">class</span> <span class="nc">KMedoids</span><span class="o">&lt;</span><span class="n">a</span> <span class="n">href</span><span class="o">=</span><span class="s">&quot;k:%20Int,%20metric:%20(T,%20T&quot;</span><span class="o">&gt;</span><span class="n">T</span><span class="o">&lt;/</span><span class="n">a</span><span class="o">&gt;</span> <span class="k">=&gt;</span> <span class="nc">Double</span><span class="o">)</span> <span class="o">{&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span>  <span class="c1">// Train a K-Medoids cluster on some input data</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">train</span><span class="o">&lt;</span><span class="n">a</span> <span class="n">href</span><span class="o">=</span><span class="s">&quot;data:%20Seq[T]&quot;</span><span class="o">&gt;</span><span class="n">T</span><span class="o">&lt;/</span><span class="n">a</span><span class="o">&gt;</span> <span class="o">{&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="k">var</span> <span class="n">current</span> <span class="k">=</span> <span class="c1">// randomly select k data elements as initial cluster</span>
</span><span class='line'>
</span><span class='line'><span class="k">var</span> <span class="n">model_converged</span> <span class="k">=</span> <span class="kc">false</span>
</span><span class='line'><span class="k">while</span> <span class="o">(!</span><span class="n">model_converged</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>  <span class="c1">// assign each element to its closest medoid</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">clusters</span> <span class="k">=</span> <span class="n">data</span><span class="o">.</span><span class="n">groupBy</span><span class="o">(</span><span class="n">medoidIdx</span><span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">current</span><span class="o">)).</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1">// recompute the medoid from the latest cluster elements</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">next</span> <span class="k">=</span> <span class="n">benchmark</span><span class="o">(</span><span class="s">&quot;medoids&quot;</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">clusters</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">medoid</span><span class="o">)</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>  <span class="n">model_converged</span> <span class="k">=</span> <span class="c1">// test for model convergence</span>
</span><span class='line'>
</span><span class='line'>  <span class="n">current</span> <span class="k">=</span> <span class="n">next</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span>  <span class="o">}&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span>  <span class="c1">// Return the medoid of some collection of elements</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">medoid</span><span class="o">(</span><span class="n">data</span><span class="k">:</span> <span class="kt">Seq</span><span class="o">[</span><span class="kt">T</span><span class="o">])</span> <span class="k">=</span> <span class="o">{&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="n">benchmark</span><span class="o">(</span><span class="n">s</span><span class="s">&quot;medoid: n= ${data.length}&quot;</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>  <span class="n">data</span><span class="o">.</span><span class="n">minBy</span><span class="o">(</span><span class="n">medoidCost</span><span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">data</span><span class="o">)</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span>  <span class="o">}&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span>  <span class="c1">// The sum of an element&#39;s distance to all the elements in its cluster</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">medoidCost</span><span class="o">(</span><span class="n">e</span><span class="k">:</span> <span class="kt">T</span><span class="o">,</span> <span class="n">data</span><span class="k">:</span> <span class="kt">Seq</span><span class="o">[</span><span class="kt">T</span><span class="o">])</span> <span class="k">=</span> <span class="n">data</span><span class="o">.</span><span class="n">iterator</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">metric</span><span class="o">(</span><span class="n">e</span><span class="o">,</span> <span class="k">_</span><span class="o">)).</span><span class="n">sum</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span>  <span class="c1">// Index of the closest medoid to an element</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">medoidIdx</span><span class="o">(</span><span class="n">e</span><span class="k">:</span> <span class="kt">T</span><span class="o">,</span> <span class="n">mv</span><span class="k">:</span> <span class="kt">Seq</span><span class="o">[</span><span class="kt">T</span><span class="o">])</span> <span class="k">=</span> <span class="n">mv</span><span class="o">.</span><span class="n">iterator</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">metric</span><span class="o">(</span><span class="n">e</span><span class="o">,</span> <span class="o">&lt;</span><span class="n">em</span><span class="o">&gt;)).</span><span class="n">zipWithIndex</span><span class="o">.</span><span class="n">min</span><span class="o">.&lt;/</span><span class="n">em</span><span class="o">&gt;</span><span class="mi">2</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span>  <span class="c1">// Output a benchmark timing of some expression</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">benchmark</span><span class="o">&lt;</span><span class="n">a</span> <span class="n">href</span><span class="o">=</span><span class="s">&quot;label:%20String&quot;</span><span class="o">&gt;</span><span class="n">T</span><span class="o">&lt;/</span><span class="n">a</span><span class="o">&gt;(</span><span class="n">blk</span><span class="k">:</span> <span class="o">=&gt;</span> <span class="n">T</span><span class="o">)</span> <span class="k">=</span> <span class="o">{&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="k">val</span> <span class="n">t0</span> <span class="k">=</span> <span class="nc">System</span><span class="o">.</span><span class="n">nanoTime</span>
</span><span class='line'><span class="k">val</span> <span class="n">t</span> <span class="k">=</span> <span class="n">blk</span>
</span><span class='line'><span class="k">val</span> <span class="n">sec</span> <span class="k">=</span> <span class="o">(</span><span class="nc">System</span><span class="o">.</span><span class="n">nanoTime</span> <span class="o">-</span> <span class="n">t0</span><span class="o">)</span> <span class="o">/</span> <span class="mi">1</span><span class="n">e9</span>
</span><span class='line'><span class="n">println</span><span class="o">(</span><span class="n">f</span><span class="s">&quot;Run time for $label = $sec%.1f&quot;</span><span class="o">);</span> <span class="nc">System</span><span class="o">.</span><span class="n">out</span><span class="o">.</span><span class="n">flush</span>
</span><span class='line'><span class="n">t</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>If we run the code above (de-skeletonized), then we might see something like this output from our benchmarking, where I clustered a dataset of 40,000 randomly-generated (x,y,z) points by Gaussian sampling around 5 chosen centers.  (This data is numeric, but I provide only a distance metric on the points.  K-Medoids has no knowledge of the data except that it can run the given metric function on it):</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span>One iteration of a clustering run (k = 5) </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Run time for medoid: n= 8299 = 7.7
</span><span class='line'>Run time for medoid: n= 3428 = 1.2
</span><span class='line'>Run time for medoid: n= 12581 = 17.0
</span><span class='line'>Run time for medoid: n= 5731 = 3.3
</span><span class='line'>Run time for medoid: n= 9961 = 10.2
</span><span class='line'>Run time for medoids = 39.8</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>Observe that cluster sizes are generally not the same, and we can see the time per cluster varying quadratically with respect to cluster size.</p>

<h3>A First Take On Parallel K-Medoids</h3>

<p>Studying our non-parallel code above, we can see that the computation of each new medoid is independent, which makes it a likely place to inject some parallelism. A Scala sequence can be transformed into a corresponding parallel sequence using the <code>par</code> method, and so parallelizing our code is literally this simple:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span>Parallelizing a collection with .par </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span>  <span class="c1">// recompute the medoid from the latest cluster elements</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">next</span> <span class="k">=</span> <span class="n">benchmark</span><span class="o">(</span><span class="s">&quot;medoids&quot;</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">clusters</span><span class="o">.</span><span class="n">par</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">medoid</span><span class="o">).</span><span class="n">seq</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>In this block, I also apply <code>.seq</code> at the end, which is not always necessary but can avoid type mismatches between <code>Seq[T]</code> and <code>ParSeq[T]</code> under some circumstances.</p>

<p>In my case I also wish to exercise some control over the threading used by the parallelism, and so I explicitly assign a <code>ForkJoinPool</code> thread pool to the sequence:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span>Set the threading used by a Scala ParSeq </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span>  <span class="c1">// establish a thread pool for use by K-Medoids</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">threadPool</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">ForkJoinPool</span><span class="o">(</span><span class="n">numThreads</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1">// ...</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1">// recompute the medoid from the latest cluster elements</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">next</span> <span class="k">=</span> <span class="n">benchmark</span><span class="o">(</span><span class="s">&quot;medoids&quot;</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">pseq</span> <span class="k">=</span> <span class="n">clusters</span><span class="o">.</span><span class="n">par</span>
</span><span class='line'>    <span class="n">pseq</span><span class="o">.</span><span class="n">tasksupport</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">ForkJoinTaskSupport</span><span class="o">(</span><span class="n">threadPool</span><span class="o">)</span>
</span><span class='line'>    <span class="n">pseq</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">medoid</span><span class="o">).</span><span class="n">seq</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>Minor grievance: it would be nice if Scala supported some 'in-line' methods, like <code>seq.par(n)...</code> and <code>seq.par(threadPool)...</code>, instead of requiring the programmer to break the flow of the code to invoke <code>tasksupport =</code>, which returns <code>Unit</code>.</p>

<p>Now that we've parallelized our K-Medoids training, we should see how well it responds to additional threads.  I ran the above parallelized version using <code>{1, 2, 4, 8, 16, 32}</code> threads, on a machine with 40 cores, so that my benchmarking would not be impacted by attempting to run more threads than there are cores to support them.  I also ran two versions of test data.  The first I generated with clusters of equal size (5 clusters of ~8000 elements), and the second with one cluster being twice as large (1 cluster of ~13300 and 4 clusters of ~6700).  Following is a plot of throughput (iterations / second) versus threads:</p>

<p><img class="left" src="/assets/images/parseq/by_cluster_1.png" title="Throughput As A Function Of Threads" ></p>

<p>In the best of all possible worlds, our throughput would increase linearly with the number of threads; double the threads, double our iterations per second.  Instead, our throughput starts to increase nicely as we add threads, but hits a hard ceiling at 8 threads.  It is not hard to see why: our parallelism is limited by the number of elements in our collection of clusters.  In our case that is k = 5, and so we reach our ceiling at 8 threads, the first thread number >= 5.  Furthermore, we see that when the size of clusters is unequal, the throughput suffers even more.  The time required to complete the clustering is dominated by the most expensive element.  In our case, the cluster that is twice the size of other clusters:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span>Run time is dominated by largest cluster </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Run time for medoid: n= 6695 = 5.1
</span><span class='line'>Run time for medoid: n= 6686 = 5.2
</span><span class='line'>Run time for medoid: n= 6776 = 5.3
</span><span class='line'>Run time for medoid: n= 6682 = 5.4
</span><span class='line'>Run time for medoid: n= 13161 = 19.9
</span><span class='line'>Run time for medoids = 19.9</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<h3>Take 2: Improving The Use Of Threads</h3>

<p>Fortunately it is not hard to improve on this situation.  If parallelizing by cluster is too coarse, we can try pushing our parallelism down one level of granularity.  In our case, that means parallelizing the outer loop of our medoid function, and it is just as easy as before:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span>Parallelize the outer loop of medoid computation </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'>  <span class="c1">// Return the medoid of some collection of elements</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">medoid</span><span class="o">(</span><span class="n">data</span><span class="k">:</span> <span class="kt">Seq</span><span class="o">[</span><span class="kt">T</span><span class="o">])</span> <span class="k">=</span> <span class="o">{&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="n">benchmark</span><span class="o">(</span><span class="n">s</span><span class="s">&quot;medoid: n= ${data.length}&quot;</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">pseq</span> <span class="k">=</span> <span class="n">data</span><span class="o">.</span><span class="n">par</span>
</span><span class='line'>  <span class="n">pseq</span><span class="o">.</span><span class="n">tasksupport</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">ForkJoinTaskSupport</span><span class="o">(</span><span class="n">threadPool</span><span class="o">)</span>
</span><span class='line'>  <span class="n">pseq</span><span class="o">.</span><span class="n">minBy</span><span class="o">(</span><span class="n">medoidCost</span><span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">data</span><span class="o">)</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span>  <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>Note that I retained the previous parallelism at the cluster level, otherwise the algorithm would execute parallel medoids, but one cluster at a time.  Also observe that we are applying the same thread pool we supplied to the ParSeq at the cluster level.  Scala's parallel logic can utilize the same thread pool at multiple granularities without blocking.  This makes it very clean to control the total number of threads used by some computation, by simply re-using the same threadpool across all points of parallelism.</p>

<p>Now, when we re-run our experiment, we see that our throughput continues to increase as we add threads.  The following plot illustrates the throughput increasing in comparison to the previous ceiling, and also that throughput is less sensitive to the cluster size, as threads can be allocated flexibly across clusters as they are available:</p>

<p><img class="left" src="/assets/images/parseq/all_1.png" title="Thread utilization improves at finer granularity" ></p>

<p>I hope this short case study has demonstrated how easy it is to add multithreading to computations with Scala parallel sequences, and some considerations for making the best use of available threads.  Happy Parallel Programming!</p>
</div>
  </article>

  <article>
    <header>
      <h1 class="entry-title"><a href="http://chapeau.freevariable.com/2015/04/elasticsearch-and-spark-1-dot-3.html">Elasticsearch and Spark 1.3</a></h1>
      <p class="meta">
        <time datetime="2015-04-30T20:34:37Z" pubdate data-updated="true">Apr 30<span>th</span>, 2015  &nbsp; &mdash; &nbsp; William Benton</time>
      </p>
    </header>
    <div class="entry-content"><p><a href="https://www.elastic.co/products/elasticsearch">Elasticsearch</a> has offered Hadoop <code>InputFormat</code> and <code>OutputFormat</code> implementations for quite some time.  These made it possible to process Elasticsearch indices with Spark just as you would any other Hadoop data source.  Here&rsquo;s an example of this in action, taken from <a href="http://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html">Elastic&rsquo;s documentation</a>:</p>

<figure class='code'><figcaption><span>Elasticsearch in Spark via Hadoop</span><a href='http://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html'>link</a></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">val</span> <span class="n">conf</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">JobConf</span><span class="o">()</span>
</span><span class='line'><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="o">(</span><span class="s">&quot;es.resource&quot;</span><span class="o">,</span> <span class="s">&quot;radio/artists&quot;</span><span class="o">)</span>
</span><span class='line'><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="o">(</span><span class="s">&quot;es.query&quot;</span><span class="o">,</span> <span class="s">&quot;?q=me*&quot;</span><span class="o">)</span>
</span><span class='line'><span class="k">val</span> <span class="n">esRDD</span> <span class="k">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">hadoopRDD</span><span class="o">(</span><span class="n">conf</span><span class="o">,</span>
</span><span class='line'>                <span class="n">classOf</span><span class="o">[</span><span class="kt">EsInputFormat</span><span class="o">[</span><span class="kt">Text</span>, <span class="kt">MapWritable</span><span class="o">]],</span>
</span><span class='line'>                <span class="n">classOf</span><span class="o">[</span><span class="kt">Text</span><span class="o">],</span> <span class="n">classOf</span><span class="o">[</span><span class="kt">MapWritable</span><span class="o">]))</span>
</span><span class='line'><span class="k">val</span> <span class="n">docCount</span> <span class="k">=</span> <span class="n">esRDD</span><span class="o">.</span><span class="n">count</span><span class="o">();</span>
</span></code></pre></td></tr></table></div></figure>


<p>However, the latest versions of the Elasticsearch client offers more idiomatic Spark support to generate RDDs (and Spark data frames) directly from Elasticsearch indices without explicitly dealing with the Hadoop formatting machinery.  This support is extremely cool, but the documentation is still a bit sparse and I hit a few snags using it.  In this post, I&rsquo;ll cover the problems I encountered and explain how to make it work.</p>

<h3>Dependencies</h3>

<p>Native Spark support is available in <code>elasticsearch-hadoop</code> version 2.1.0.  If you&rsquo;d tried to set this up last week, you&rsquo;d have needed to run against a snapshot build to get support for Spark data frames.  Fortunately, the most recent beta release of <code>elasticsearch-hadoop</code> includes this support.  Add the following library dependency to your project:</p>

<figure class='code'><figcaption><span>sbt dependency configuration</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="n">libraryDependencies</span> <span class="o">+=</span> <span class="s">&quot;org.elasticsearch&quot;</span> <span class="o">%%</span> <span class="s">&quot;elasticsearch-spark&quot;</span> <span class="o">%</span> <span class="s">&quot;2.1.0.Beta4&quot;</span>
</span></code></pre></td></tr></table></div></figure>


<p>Since we won&rsquo;t be using the lower-level Hadoop support, that&rsquo;s the only library we&rsquo;ll need.</p>

<h3>Configuration</h3>

<p>We need to supply some configuration in order to create RDDs directly from indices.  <code>elasticsearch-spark</code> expects this configuration to be available as parameters on our Spark context object.  Here&rsquo;s an example of how you&rsquo;d do that in a vanilla Spark application:</p>

<figure class='code'><figcaption><span>Configuring an ES node list  </span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">val</span> <span class="n">conf</span> <span class="k">=</span> <span class="k">new</span> <span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">spark</span><span class="o">.</span><span class="nc">SparkConf</span><span class="o">()</span>
</span><span class='line'> <span class="o">.</span><span class="n">setMaster</span><span class="o">(</span><span class="s">&quot;local[*]&quot;</span><span class="o">)</span>
</span><span class='line'> <span class="o">.</span><span class="n">setAppName</span><span class="o">(</span><span class="s">&quot;es-example&quot;</span><span class="o">)</span>
</span><span class='line'> <span class="o">.</span><span class="n">set</span><span class="o">(</span><span class="s">&quot;es.nodes&quot;</span><span class="o">,</span> <span class="s">&quot;localhost&quot;</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'><span class="k">val</span> <span class="n">spark</span> <span class="k">=</span> <span class="k">new</span> <span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">spark</span><span class="o">.</span><span class="nc">SparkContext</span><span class="o">(</span><span class="n">conf</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>Note that since the default value for <code>es.nodes</code> is <code>localhost</code>, you don&rsquo;t need to set it at all if you&rsquo;re running against a local Elasticsearch instance.  If you were running Elasticsearch behind a reverse proxy running on <code>proxyhost</code> on port 80, you might specify <code>proxyhost:80</code> instead (and you would probably also want to set <code>es.nodes.discovery</code> to <code>false</code> so your client wouldn&rsquo;t discover nodes that weren&rsquo;t reachable outside of the private network). There are <a href="http://www.elastic.co/guide/en/elasticsearch/hadoop/current/configuration.html">other options you can set</a>, but this is the most critical.</p>

<p>If you&rsquo;re using the <a href="http://silex.freevariable.com/latest/api/#com.redhat.et.silex.app.AppCommon">Spark app skeleton</a> provided by <a href="http://silex.freevariable.com/">the Silex library</a>, you can add Elasticsearch configuration in config hooks, as follows:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">import</span> <span class="nn">com.redhat.et.silex.app.AppCommon</span>
</span><span class='line'><span class="k">import</span> <span class="nn">org.elasticsearch.spark._</span>
</span><span class='line'>
</span><span class='line'><span class="k">object</span> <span class="nc">IndexCount</span> <span class="k">extends</span> <span class="nc">AppCommon</span> <span class="o">{</span>
</span><span class='line'>  <span class="n">addConfig</span> <span class="o">{</span> <span class="n">conf</span> <span class="k">=&gt;</span>
</span><span class='line'>    <span class="n">conf</span>
</span><span class='line'>     <span class="o">.</span><span class="n">set</span><span class="o">(</span><span class="s">&quot;es.nodes&quot;</span><span class="o">,</span> <span class="n">sys</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">getOrElse</span><span class="o">(</span><span class="s">&quot;ES_NODES&quot;</span><span class="o">,</span> <span class="s">&quot;localhost&quot;</span><span class="o">))</span>
</span><span class='line'>     <span class="o">.</span><span class="n">set</span><span class="o">(</span><span class="s">&quot;es.nodes.discovery&quot;</span><span class="o">,</span> <span class="s">&quot;false&quot;</span><span class="o">)</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">appName</span> <span class="k">=</span> <span class="s">&quot;example ES app&quot;</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">def</span> <span class="n">appMain</span><span class="o">(</span><span class="n">args</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">args</span> <span class="k">match</span> <span class="o">{</span>
</span><span class='line'>      <span class="k">case</span> <span class="nc">Array</span><span class="o">(</span><span class="n">index</span><span class="o">,</span> <span class="n">indexType</span><span class="o">)</span> <span class="k">=&gt;</span>
</span><span class='line'>        <span class="k">val</span> <span class="n">resource</span> <span class="k">=</span> <span class="n">s</span><span class="s">&quot;$index/$indexType&quot;</span>
</span><span class='line'>        <span class="k">val</span> <span class="n">count</span> <span class="k">=</span> <span class="n">context</span><span class="o">.</span><span class="n">esRDD</span><span class="o">(</span><span class="n">resource</span><span class="o">).</span><span class="n">count</span><span class="o">()</span>
</span><span class='line'>        <span class="nc">Console</span><span class="o">.</span><span class="n">println</span><span class="o">(</span><span class="n">s</span><span class="s">&quot;$resource has $count documents&quot;</span><span class="o">)</span>
</span><span class='line'>      <span class="k">case</span> <span class="k">_</span> <span class="k">=&gt;</span>
</span><span class='line'>        <span class="nc">Console</span><span class="o">.</span><span class="n">println</span><span class="o">(</span><span class="s">&quot;usage: IndexCount index type&quot;</span><span class="o">)</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<h3>Generating RDDs</h3>

<p><code>elasticsearch-spark</code> offers implicit conversions to make RDDs directly from Elasticsearch resources.  (Careful readers may have already seen one in action!)  The following code example assumes that you&rsquo;ve initialized <code>spark</code> as a <code>SparkContext</code> with whatever Elasticsearch configuration data you need to set:</p>

<figure class='code'><figcaption><span>Generating RDDs from ES resources</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="c1">// bring implicit conversions into scope to extend our</span>
</span><span class='line'><span class="c1">// SparkContext with the esRDD method, which creates a</span>
</span><span class='line'><span class="c1">// new RDD backed by an ES index or query</span>
</span><span class='line'><span class="k">import</span> <span class="nn">org.elasticsearch.spark._</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// telemetry-20080915/sar is an ES index of system telemetry data</span>
</span><span class='line'><span class="c1">// (collected from the sar tool)</span>
</span><span class='line'><span class="k">val</span> <span class="n">sarRDD</span> <span class="k">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">esRDD</span><span class="o">(</span><span class="s">&quot;telemetry-20080915/sar&quot;</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// =&gt; sarRDD: org.apache.spark.rdd.RDD[(String, scala.collection.Map[String,AnyRef])] = ScalaEsRDD[4] at RDD at AbstractEsRDD.scala:17</span>
</span></code></pre></td></tr></table></div></figure>


<p>This is far more convenient than using the Hadoop <code>InputFormat</code>, but the interface still leaves a little to be desired.  Our data may have a sensible schema, but the result we&rsquo;re getting back is as general as possible:  just a collection of key-value pairs, where the keys are strings and the values are maps from strings to arbitrary (and arbitrarily-typed) values.  If we want more explicit structure in our data, we&rsquo;ll need to bring it in to Spark in a different way.</p>

<h3>Generating data frames</h3>

<p>In order to benefit from the structure in our data, we can generate data frames that store schema information.  The most recent release of <code>elasticsearch-spark</code> supports doing this directly.  Here&rsquo;s what that looks like, again assuming that <code>spark</code> is an appropriately-configured <code>SparkContext</code>:</p>

<figure class='code'><figcaption><span>Generating data frames from ES resources </span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="c1">// bring implicit conversions into scope to extend our</span>
</span><span class='line'><span class="c1">// SQLContexts with the esDF method, which creates a</span>
</span><span class='line'><span class="c1">// new data frame backed by an ES index or query</span>
</span><span class='line'><span class="k">import</span> <span class="nn">org.elasticsearch.spark.sql._</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// construct a SQLContext</span>
</span><span class='line'><span class="k">val</span> <span class="n">sqlc</span> <span class="k">=</span> <span class="k">new</span> <span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="nc">SQLContext</span><span class="o">(</span><span class="n">spark</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// telemetry-20080915/sar is an ES index of system telemetry data</span>
</span><span class='line'><span class="c1">// (collected from the sar tool)</span>
</span><span class='line'><span class="k">val</span> <span class="n">sarDF</span> <span class="k">=</span> <span class="n">sqlc</span><span class="o">.</span><span class="n">esDF</span><span class="o">(</span><span class="s">&quot;telemetry-20080915/sar&quot;</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'><span class="c1">// =&gt; org.apache.spark.sql.DataFrame = [_metadata: struct&lt;file-date:timestamp,generated-by:string,generated-by-version:string,machine:string,nodename:string,number-of-cpus:int,release:string,sysdata-version:float,sysname:string&gt;, cpu-load: string, cpu-load-all: string, disk: string, filesystems: string, hugepages: struct&lt;hugfree:bigint,hugused:bigint,hugused-percent:double&gt;, interrupts: string, interrupts-processor: string, io: struct&lt;io-reads:struct&lt;bread:double,rtps:double&gt;,io-writes:struct&lt;bwrtn:double,wtps:double&gt;,tps:double&gt;, kernel: struct&lt;dentunusd:bigint,file-nr:bigint,inode-nr:bigint,pty-nr:bigint&gt;, memory: struct&lt;active:bigint,buffers:bigint,bufpg:double,cached:bigint,campg:double,commit:bigint,commit-percent:double,dirty:bigint,frmpg:double,inactive:bigint,memfree:bigint,memu...</span>
</span></code></pre></td></tr></table></div></figure>


<p>Once we have a data frame for our ES resource, we can run queries against it in Spark:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="n">sardf</span><span class="o">.</span><span class="n">registerTempTable</span><span class="o">(</span><span class="s">&quot;sardf&quot;</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'><span class="n">sqlc</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">&quot;select _metadata from sardf&quot;</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<h3>Next steps</h3>

<p>Elasticsearch&rsquo;s Spark support offers <a href="http://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html">many additional capabilities</a>, including storing Spark RDDs to Elasticsearch and generating RDDs from queries.  Check out the upstream documentation for more!</p>
</div>
  </article>

  <article>
    <header>
      <h1 class="entry-title"><a href="http://chapeau.freevariable.com/2015/04/spark-apps-and-ci.html">Effective continuous integration for Spark projects</a></h1>
      <p class="meta">
        <time datetime="2015-04-21T20:57:24Z" pubdate data-updated="true">Apr 21<span>st</span>, 2015  &nbsp; &mdash; &nbsp; William Benton</time>
      </p>
    </header>
    <div class="entry-content"><p><a href="http://silex.freevariable.com">Silex</a> is a small library of helper code intended to make it easier to build real-world Spark applications;<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> most of it is factored out from applications we&rsquo;ve developed internally at Red Hat.  We have a couple of long-term goals for the Silex project:</p>

<ul>
<li>We want to make it very easy for us to spin up on new data-processing problems without spending a lot of time dealing with accidental complexity, and</li>
<li>we want to have a generally-useful standard library atop Spark that provides primitives and solutions to simplify common tasks and reflects best practices as we discover them.</li>
</ul>


<p>Part of having a library that people will want to use is having a quality test suite and continuous integration.  There are some tricks to developing automated tests for Spark applications, but even keeping those in mind may not be sufficient to let you test your Spark apps in a hosted CI environment.  This post will show you how to bridge the gap.</p>

<h3>Continuous integration challenges</h3>

<p>Since we want to make it easy to review contributions to Silex, I set up <a href="https://travis-ci.org/willb/silex">Travis CI</a> to watch branches and pull requests.  Travis CI is a great service, but if you&rsquo;re used to running tests locally, you might have some problems running Spark-based tests under hosted CI environments.  Here&rsquo;s what we learned:</p>

<ul>
<li>When running tests locally, creating a Spark context with <code>local[*]</code> (in order to use all of the available cores on your machine) might be a good idea.  However, a hosted CI environment may offer you a lot of cores but relatively little memory, so your tests might be killed because each Spark executor has nontrivial memory overhead.  Instead, consider limiting your code to use fewer cores.</li>
<li>If you&rsquo;re using Kryo serialization and have set the <code>spark.kryoserializer.buffer.mb</code> property to something large (perhaps because you often have serialization buffer crashes in production), don&rsquo;t be surprised if you run out of memory while running in CI.  Spark doesn&rsquo;t share serializers between threads, so you could be allocating a huge buffer for each thread even if your test code doesn&rsquo;t need to serialize anything all that large.<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup></li>
<li>Spark SQL (at least as of version 1.3.1) defaults to creating 200 partitions for shuffles.  This is probably a good starting place for real-world data, but it&rsquo;s overkill for test cases running on a single machine.  Furthermore, since each partition has some extra memory overhead, it&rsquo;s another possible cause of OOMEs.</li>
</ul>


<h3>Fixing the problems</h3>

<p>Silex provides an <a href="http://silex.freevariable.com/latest/api/#com.redhat.et.silex.app.AppCommon">application skeleton trait</a>, and we use a <a href="https://github.com/willb/silex/blob/c61995689ca6c76e920af64d7fe9306cdcbcc98d/src/main/scala/com/redhat/et/silex/app/app.scala#L121">class extending this trait</a> to package up Spark and SQL contexts for test cases.  That class isn&rsquo;t currently part of the public Silex API, but you can see what it looks like here:</p>

<figure class='code'><figcaption><span>excerpt from app.scala</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">private</span> <span class="o">[</span><span class="kt">silex</span><span class="o">]</span> <span class="k">class</span> <span class="nc">TestConsoleApp</span><span class="o">(</span><span class="k">val</span> <span class="n">suppliedMaster</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=</span> <span class="s">&quot;local[2]&quot;</span><span class="o">)</span> <span class="k">extends</span> <span class="nc">AppCommon</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">master</span> <span class="k">=</span> <span class="n">suppliedMaster</span>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">appName</span> <span class="k">=</span> <span class="s">&quot;console&quot;</span>
</span><span class='line'>
</span><span class='line'>  <span class="n">addConfig</span><span class="o">(</span> <span class="o">{(</span><span class="n">conf</span><span class="k">:</span> <span class="kt">SparkConf</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="o">(</span><span class="s">&quot;spark.kryoserializer.buffer.mb&quot;</span><span class="o">,</span> <span class="s">&quot;2&quot;</span><span class="o">)})</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">def</span> <span class="n">appMain</span><span class="o">(</span><span class="n">args</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span> <span class="o">{</span>
</span><span class='line'>    <span class="c1">// this never runs</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>As you can see, we allow users to specify a Spark master URL but default to using two cores locally.  Furthermore, we use the <code>addConfig</code> function from <code>AppCommon</code> &mdash; which takes a <code>SparkConf</code> and returns a modified <code>SparkConf</code> &mdash; to ensure that our Kryo buffer size is the Spark default of 2 mb, rather than the larger Silex default.</p>

<p>If you&rsquo;re used to writing test code that exercises Spark, you probably already have boilerplate (using something like ScalaTest&rsquo;s <code>BeforeAndAfterEach</code>) to set up and tear down a Spark context for each test case.  We set the Spark SQL property to control parallelism in data frame and SQL shuffles in the <a href="https://github.com/willb/silex/blob/3155174e52c392086b44c438fbf5c29d7af9fd3e/src/test/scala/com/redhat/et/silex/testing/sparkFixtures.scala">test setup code</a> itself:</p>

<figure class='code'><figcaption><span>excerpt from app.scala</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">import</span> <span class="nn">org.scalatest._</span>
</span><span class='line'>
</span><span class='line'><span class="k">import</span> <span class="nn">com.redhat.et.silex.app.TestConsoleApp</span>
</span><span class='line'>
</span><span class='line'><span class="k">trait</span> <span class="nc">PerTestSparkContext</span> <span class="k">extends</span> <span class="nc">BeforeAndAfterEach</span> <span class="o">{</span>
</span><span class='line'>  <span class="n">self</span><span class="k">:</span> <span class="kt">BeforeAndAfterEach</span> <span class="kt">with</span> <span class="kt">Suite</span> <span class="o">=&gt;</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">private</span> <span class="k">var</span> <span class="n">app</span><span class="k">:</span> <span class="kt">TestConsoleApp</span> <span class="o">=</span> <span class="kc">null</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">def</span> <span class="n">context</span> <span class="k">=</span> <span class="n">app</span><span class="o">.</span><span class="n">context</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">sqlContext</span> <span class="k">=</span> <span class="n">app</span><span class="o">.</span><span class="n">sqlContext</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">beforeEach</span><span class="o">()</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">app</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">TestConsoleApp</span><span class="o">()</span>
</span><span class='line'>    <span class="nc">System</span><span class="o">.</span><span class="n">clearProperty</span><span class="o">(</span><span class="s">&quot;spark.master.port&quot;</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">app</span><span class="o">.</span><span class="n">sqlContext</span><span class="o">.</span><span class="n">setConf</span><span class="o">(</span><span class="s">&quot;spark.sql.shuffle.partitions&quot;</span><span class="o">,</span> <span class="s">&quot;10&quot;</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">app</span><span class="o">.</span><span class="n">context</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">afterEach</span><span class="o">()</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">app</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">stop</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<h3>Conclusion</h3>

<p>Good CI is one of those things that makes it fun to develop software, but the accidental differences between a hosted environment and your local environment can be totally frustrating.  By taking into account the constraints you&rsquo;re likely to see when running in a resource-limited container on a CI server, you can help to ensure that your tests only fail when your code is actually broken.</p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p>&ldquo;Silex&rdquo; is Latin for &ldquo;flint,&rdquo; which seemed like a good name for something that was intended to help people make things with sparks.<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
<li id="fn:2">
<p>Thanks to <a href="http://erikerlandson.github.io">Erik Erlandson</a> for noticing this (it even affected him in a test case he was running locally).<a href="#fnref:2" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

</div>
  </article>

  <article>
    <header>
      <h1 class="entry-title"><a href="manual/v8.3.5/10_3Development_Release.html"> HTCondor 8.3.5 released! ( April 20, 2015 )</a></h1>
      <p class="meta">
        <time datetime="2015-04-20T05:00:00Z" pubdate data-updated="true">Apr 20<span>th</span>, 2015  &nbsp; &mdash; &nbsp; HTCondor Team</time>
      </p>
    </header>
    <div class="entry-content">The HTCondor team is pleased to announce the release of HTCondor 8.3.5.
A development series release contains new features that are under
development. This release contains all of the bug fixes from the 8.2.8
stable release.
A few of the enhancements in this release include:
new features that increase the power of job specification in the
submit description file;
RPMs for Red Hat Enterprise Linux 6 and 7 are modularized and only
distributed via our YUM repository;
The new condor-all RPM requires
the other HTCondor RPMs of a typical HTCondor installation.
Further details can be found in the
Version History.
HTCondor 8.3.5 binaries and source code are available from our
Downloads page.
</div>
  </article>

  <article>
    <header>
      <h1 class="entry-title"><a href="http://chapeau.freevariable.com/2015/04/natural-join-for-spark-dataframes.html">Natural join for data frames in Spark</a></h1>
      <p class="meta">
        <time datetime="2015-04-08T21:21:21Z" pubdate data-updated="true">Apr 8<span>th</span>, 2015  &nbsp; &mdash; &nbsp; William Benton</time>
      </p>
    </header>
    <div class="entry-content"><p><a href="http://en.wikipedia.org/wiki/Relational_algebra#Natural_join_.28.E2.8B.88.29">Natural join</a> is a useful special case of the relational join operation (and is extremely common when denormalizing data pulled in from a relational database).  Spark&rsquo;s DataFrame API provides an expressive way to specify arbitrary joins, but it would be nice to have some machinery to make the simple case of natural join as easy as possible.  Here&rsquo;s what a natural join needs to do: [...]</p>
</div>
  </article>

  <article>
    <header>
      <h1 class="entry-title"><a href="manual/v8.2.8/10_3Stable_Release.html"> HTCondor 8.2.8 released! ( April 7, 2015 )</a></h1>
      <p class="meta">
        <time datetime="2015-04-07T05:00:00Z" pubdate data-updated="true">Apr 7<span>th</span>, 2015  &nbsp; &mdash; &nbsp; HTCondor Team</time>
      </p>
    </header>
    <div class="entry-content">The HTCondor team is pleased to announce the release of HTCondor version 8.2.8.
A stable series release contains significant bug and security fixes.
This version contains:
a bug fix to reconnect a TCP session when an HTCondorView collector restarts;
a bug fix to avoid starting too many jobs, only to kill some chosen at random.
A complete list of fixed bugs can be found in the
Version History.
HTCondor 8.2.8 binaries and source code are available from our
Downloads page.
</div>
  </article>

  <article>
    <header>
      <h1 class="entry-title"><a href="http://chapeau.freevariable.com/2015/04/spark-sql-repl.html">Interactively using Spark SQL and DataFrames from sbt projects</a></h1>
      <p class="meta">
        <time datetime="2015-04-02T16:12:17Z" pubdate data-updated="true">Apr 2<span>nd</span>, 2015  &nbsp; &mdash; &nbsp; William Benton</time>
      </p>
    </header>
    <div class="entry-content"><p>One of the great things about Apache Spark is that you can experiment with new analyses interactively.  In the past, I&rsquo;ve used the <code>sbt</code> console to try out new data transformations and models; the console is especially convenient since you <a href="http://chapeau.freevariable.com/2014/09/interactive-sbt.html">can set it up as a custom Scala REPL with your libraries loaded and some test fixtures already created</a>. [...]</p>
</div>
  </article>

  <article>
    <header>
      <h1 class="entry-title"><a href="http://erikerlandson.github.com/blog/2015/03/31/hygienic-closures-for-scala-function-serialization/">Hygienic Closures for Scala Function Serialization</a></h1>
      <p class="meta">
        <time datetime="2015-03-31T13:06:00Z" pubdate data-updated="true">Mar 31<span>st</span>, 2015  &nbsp; &mdash; &nbsp; Erik Erlandson</time>
      </p>
    </header>
    <div class="entry-content"><p>In most use cases of Scala closures, what you see is what you get, but there are exceptions where looks can be deceiving and this can have a big impact on closure serialization.  Closure serialization is of more than academic interest.  Tools like Apache Spark cannot operate without serializing functions over the network.  In this post I'll describe some scenarios where closures include more than what is evident in the code, and then a technique for preventing unwanted inclusions. [...]</p>
</div>
  </article>

  <article>
    <header>
      <h1 class="entry-title"><a href="http://www.news.wisc.edu/23594"> HTCondor helps astronomers with the hydrogen location problem ( March 26, 2015 )</a></h1>
      <p class="meta">
        <time datetime="2015-03-26T05:00:00Z" pubdate data-updated="true">Mar 26<span>th</span>, 2015  &nbsp; &mdash; &nbsp; HTCondor Team</time>
      </p>
    </header>
    <div class="entry-content">This  UW Madison news article discusses how a new computational approach permits evaluation of hydrogen data using software, which may replace the time consuming manual approach. Putting HTCondor into the mix scales well given the vast quantities of data expected as Square Kilometer Array radio telescope is realized.
</div>
  </article>

  <article>
    <header>
      <h1 class="entry-title"><a href="http://research.cs.wisc.edu/htcondor/HTCondorWeek2015/"> HTCondor Week 2015 Registration Open (March 25, 2015 )</a></h1>
      <p class="meta">
        <time datetime="2015-03-25T05:00:00Z" pubdate data-updated="true">Mar 25<span>th</span>, 2015  &nbsp; &mdash; &nbsp; HTCondor Team</time>
      </p>
    </header>
    <div class="entry-content">We invite HTCondor users, administrators, and developers to HTCondor Week 2015, our annual HTCondor user conference, in beautiful Madison, Wisconsin, May 19-22, 2015. HTCondor Week features tutorials and talks from HTCondor developers, administrators, and users.  It also provides an opportunity for one-on-one or small group collaborations throughout the week.
</div>
  </article>

  <article>
    <header>
      <h1 class="entry-title"><a href="manual/v8.3.4/10_3Development_Release.html"> HTCondor 8.3.4 released! ( March 5, 2015 )</a></h1>
      <p class="meta">
        <time datetime="2015-03-05T06:00:00Z" pubdate data-updated="true">Mar 5<span>th</span>, 2015  &nbsp; &mdash; &nbsp; HTCondor Team</time>
      </p>
    </header>
    <div class="entry-content">The HTCondor team is pleased to announce the release of HTCondor 8.3.4.
This development release contains a single bug fix for a problem introduced
in version 8.3.3 that can cause jobs to not be matched to resources when the
condor_schedd is flocking.
This 8.3.4 release also has a known issue which prevents a mixed mode,
IPv4 and IPv6 HTCondor installation from starting jobs. If using IPv4
and IPv6 in mixed mode communication is required, please continue using
HTCondor version 8.3.2. The issue will be fixed in version 8.3.5.
Further details can be found in the
Version History.
HTCondor 8.3.4 binaries and source code are available from our
Downloads page.
</div>
  </article>

  <article>
    <header>
      <h1 class="entry-title"><a href="http://derekweitzel.blogspot.com/2015/01/htcondor-cached-caching-for-htc-part-2.html">HTCondor CacheD: Caching for HTC - Part 2</a></h1>
      <p class="meta">
        <time datetime="2015-01-25T15:59:00Z" pubdate data-updated="true">Jan 25<span>th</span>, 2015  &nbsp; &mdash; &nbsp; Derek Weitzel</time>
      </p>
    </header>
    <div class="entry-content"></div>
  </article>

  <article>
    <header>
      <h1 class="entry-title"><a href="http://erikerlandson.github.com/blog/2015/01/24/monadic-break-and-continue-for-scala-sequence-comprehensions/">Monadic 'break' and 'continue' for Scala Sequence Comprehensions</a></h1>
      <p class="meta">
        <time datetime="2015-01-24T18:54:00Z" pubdate data-updated="true">Jan 24<span>th</span>, 2015  &nbsp; &mdash; &nbsp; Erik Erlandson</time>
      </p>
    </header>
    <div class="entry-content"><p>Author's note: I've since received some excellent feedback from the Scala community, which I included in some <a href="#notes">end notes</a>. [...]</p>
</div>
  </article>

  <article>
    <header>
      <h1 class="entry-title"><a href="http://derekweitzel.blogspot.com/2015/01/condor-cached-caching-for-htc-part-1.html">Condor CacheD: Caching for HTC - Part 1</a></h1>
      <p class="meta">
        <time datetime="2015-01-22T16:00:00Z" pubdate data-updated="true">Jan 22<span>nd</span>, 2015  &nbsp; &mdash; &nbsp; Derek Weitzel</time>
      </p>
    </header>
    <div class="entry-content"></div>
  </article>

  <article>
    <header>
      <h1 class="entry-title"><a href="http://erikerlandson.github.com/blog/2014/09/11/faster-random-samples-with-gap-sampling/">Faster Random Samples With Gap Sampling</a></h1>
      <p class="meta">
        <time datetime="2014-09-11T14:57:00Z" pubdate data-updated="true">Sep 11<span>th</span>, 2014  &nbsp; &mdash; &nbsp; Erik Erlandson</time>
      </p>
    </header>
    <div class="entry-content"><p>Generating a random sample of a collection is, logically, a O(np) operation, where (n) is the sample size and (p) is the sampling probability.  For example, extracting a random sample, without replacement, from an array might look like this in pseudocode: [...]</p>
</div>
  </article>

  <article>
    <header>
      <h1 class="entry-title"><a href="http://timothysc.github.com/blog/2014/09/08/mesos-breeze/">Getting Started with Mesos on Fedora 21 and CentOS 7</a></h1>
      <p class="meta">
        <time datetime="2014-09-08T15:00:00Z" pubdate data-updated="true">Sep 8<span>th</span>, 2014  &nbsp; &mdash; &nbsp; Timothy St. Clair</time>
      </p>
    </header>
    <div class="entry-content"><p><img class="left" src="http://timothysc.github.com/images/mesos_logo.png"> [...]</p>
</div>
  </article>

  <article>
    <header>
      <h1 class="entry-title"><a href="http://erikerlandson.github.com/blog/2014/09/03/matryoshka-class-construction-from-the-scala-iterator-drop-method/">The Scala Iterator 'drop' Method Generates a Matryoshka Class Nesting</a></h1>
      <p class="meta">
        <time datetime="2014-09-04T00:23:00Z" pubdate data-updated="true">Sep 4<span>th</span>, 2014  &nbsp; &mdash; &nbsp; Erik Erlandson</time>
      </p>
    </header>
    <div class="entry-content"><p>The Scala Iterator <code>drop</code> method has a complexity bug that shows up when one calls <code>drop</code> repeatedly, for example when traversing over an iterator in a loop. [...]</p>
</div>
  </article>

  <article>
    <header>
      <h1 class="entry-title"><a href="http://derekweitzel.blogspot.com/2014/06/gpus-on-osg.html">GPUs on the OSG</a></h1>
      <p class="meta">
        <time datetime="2014-06-25T18:36:00Z" pubdate data-updated="true">Jun 25<span>th</span>, 2014  &nbsp; &mdash; &nbsp; Derek Weitzel</time>
      </p>
    </header>
    <div class="entry-content"></div>
  </article>

</div>
<aside class="sidebar">
  <section>
    <h1>Planet HTCondor Members</h1>
    
      <a href="http://chapeau.freevariable.com/"> William Benton</a><br>
    
      <a href="http://erikerlandson.github.com/"> Erik Erlandson</a><br>
    
      <a href="http://timothysc.github.com/"> Timothy St. Clair</a><br>
    
      <a href="http://research.cs.wisc.edu/htcondor"> HTCondor Team</a><br>
    
      <a href="http://derekweitzel.blogspot.com"> Derek Weitzel</a><br>
    
  </section>
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2015 - HTCondor Project -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
