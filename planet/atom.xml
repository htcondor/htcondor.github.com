<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[Planet HTCondor Meta Feed]]></title>
  <link href="http://htcondor.github.com/planet/atom.xml" rel="self"/>
  <link href="http://htcondor.github.com/"/>
  <updated>2015-02-22T03:44:12-07:00</updated>
  <id>http://htcondor.github.com/planet/atom.xml</id>
  <author>
    <name><![CDATA[HTCondor Project]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
    <entry>
      <title type="html"><![CDATA[Your next favorite collaboration tool]]></title>
      <link href="http://chapeau.freevariable.com/2015/02/shared-decks.html"/>
      <updated>2015-02-13T16:50:17Z</updated>
      <id>http://chapeau.freevariable.com/2015/02/shared-decks</id>
      <author>
        <name><![CDATA[William Benton]]></name>
        <uri>http://chapeau.freevariable.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>Last night I had a crazy realization:  I could probably replace the majority of what my team hopes to accomplish with standup meetings, design documents, project management apps, and social code sharing features with a single tool.</p>

<p>Before I reveal what this tool is, let&rsquo;s consider why most team collaboration is suboptimal.  Everyone who has been in standup meetings or read groupwide status reports has probably seen the following status-update antipatterns; in fact, unless you&rsquo;re a better teammate than I, you&rsquo;ve probably been guilty of each of these yourself:</p>

<ul>
<li>&ldquo;This week I read 253 emails in my inbox, 1478 emails from mailing list folders, and sampled 75 messages from my spam folder to make sure none of them were spuriously filed.  I also wrote a net 498 lines of code, brewed eight pots of coffee and five shots of espresso, spent 246 minutes in meetings, and urinated twenty-one times.&rdquo;</li>
<li>&ldquo;I&rsquo;m still working on the same thing I was working on last week.  None of you are close enough to this problem to know why it is so tough and I&rsquo;m too defeated to explain it to you, so feel free to assume that I am either heroically deep in the weeds or have gotten totally ratholed on something irrelevant.&rdquo;</li>
<li>&ldquo;I discovered that this graph search problem could be better modeled as a language-recognition query, and spent some time investigating efficient representations  before coming up with a really clever approach involving boolean satisfiability &mdash; there was a paper in the SIGYAWN &lsquo;75 proceedings that used a similar technique; maybe you read it too &mdash; &hellip; [<em>fifteen minutes pass</em>] &hellip; and improved performance by 0.57%.&rdquo;</li>
</ul>


<p>It doesn&rsquo;t seem like it should be, but getting the level of detail right in status updates is <em>hard</em>, and it&rsquo;s hard for stupid reasons.  Your big accomplishments for the week are easy to explain and you don&rsquo;t want to make it seem like you did nothing, so you start accounting for every second since last Monday.  It&rsquo;s too depressing to consider that the bug that&rsquo;s been keeping you awake for the last few weeks still isn&rsquo;t solved and you don&rsquo;t want to spend time talking about all of the dim trails you investigated, so you&rsquo;ll Eeyore it up to get out of the meeting and back to work.  You&rsquo;re so deep in a problem that you think everyone else needs to know about a bunch of tiny details and can&rsquo;t tell that they&rsquo;ve all fallen asleep.</p>

<p>To some extent, all of these seemingly-different problems stem from <a href="http://randsinrepose.com/archives/context-reports/">too much status and not enough context</a>.  But even if we recast status reports as context updates, we&rsquo;ve traded the problem of finding an appropriate level of status detail for finding an appropriate amount of context.  How do we decide what an appropriate amount of context is?</p>

<p>Think of the best and worst talks you&rsquo;ve ever seen.  A good talk doesn&rsquo;t exhaustively describe the work the author did, but it places it in enough context so that the audience has some way to evaluate the claims, get interested, and decide if the work is worth investigating further.  A bad talk is vague, disorganized, gets bogged down in irrelevant details, or only makes sense to people who already know everything about its subject.</p>

<p>Now think about the best talks you&rsquo;ve ever given.  Giving a good talk &mdash; even more so than writing a good article or essay &mdash; hones and clarifies your argument.  You have to step out of your bubble and see your work from the perspective of your audience, because you have just a few minutes to catch their attention before they tune out and start checking their email while waiting for the next speaker.  Now, I&rsquo;ve given a lot of lectures, academic talks, and conference presentations, but I&rsquo;ve found that giving informal, internal tech talks on some interesting aspect of something I&rsquo;ve been working on has paid off disproportionately.</p>

<p>So I don&rsquo;t want my team to give each other status updates.  Instead, I want us to give regular short talks.  And the medium that I want us to use as we develop these talks is a shared slide deck, updated throughout the week.<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>  In spite of the amazingly persistent ubiquity of speakers reading through their wall-of-text decks like apparatchiks in dystopian fiction, slides aren&rsquo;t a substitute for talks.  However, a good slide deck supports a good talk, and designing a slide deck helps to make sure that you have the right amount of context in your talk, because the wrong amount of context in a talk leaves some obvious odors in the deck:</p>

<ul>
<li>Oh, you need to drop down to 18 point type to cover everything you wanted here?  That&rsquo;s probably a clue that you&rsquo;re covering something that would be better addressed in further discussion or a subsequent deep dive.</li>
<li>Does this slide feel totally bare?  Maybe you need more background here.</li>
<li>Could you draw a picture and replace this paragraph?  Could you write a sentence and replace this code excerpt?</li>
<li>If you had to explain this to a customer or a executive &mdash; really, to anyone who&rsquo;s sharp and engaged but not a domain expert &mdash; right now, where would you start?  How would you keep them interested?</li>
</ul>


<p>My team is doing data science, so a lot of what we wind up delivering turns out to be presentations and visual explanations.  As a result, this practice could be even more beneficial for us than it might be for other groups.  Rigorously working out explanations among ourselves is certain to pay off in the future, when we have to present results internally or externally.  By using a shared deck, we can annotate with questions, link to more detail elsewhere, and rapidly iterate on our presentations as we have improved results or come up with clearer explanations.</p>

<p><em>(Thanks to <a href="https://github.com/rnowling">RJ Nowling</a>, who suggested the idea of a regular &ldquo;analytics review&rdquo; and got me thinking along these lines.)</em></p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p>I take talks and slides very seriously and was initially resistant to using web-based slideware: most slideware is optimized for making ugly, bullet-laden decks and even the best web-based office suites &mdash; although they are amazing technical achievements &mdash; are still sort of clunky to use compared to desktop applications.  But the flexibility of having a collaboratively-edited deck is huge.<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[ HTCondor Week 2015 announced! ( February 11, 2015 )]]></title>
      <link href="http://research.cs.wisc.edu/htcondor/HTCondorWeek2015/"/>
      <updated>2015-02-11T06:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[HTCondor Week 2015 is May 19â22, 2015 in Madison, Wisconsin.  Join other users, administrators, and deveopers for the opportunity to exchange ideas and experiences, to learn about the latest research, to experience live demos, and to influence our short and long term research and development directions.
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[ HTCondor 8.2.7 released! ( February 10, 2015 )]]></title>
      <link href="manual/v8.2.7/10_3Stable_Release.html"/>
      <updated>2015-02-10T06:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[The HTCondor team is pleased to announce the release of HTCondor version 8.2.7.
A stable series release contains significant bug and security fixes.
This version contains:
sendmail is used by default for sending notifications (CVE-2014-8126);
corrected input validation, which prevents daemon crashes;
an update, such that grid jobs work within the current Google Compute Engine;
a bug fix to prevent an infinite loop in the python bindings;
a bug fix to prevent infinite recursion when evaluating ClassAd attributes.
A complete list of fixed bugs can be found in the
Version History.
HTCondor 8.2.7 binaries and source code are available from our
Downloads page.
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[ HTCondor assisted the paleobiology application of DeepDive ( February 5, 2015 )]]></title>
      <link href="http://www.news.wisc.edu/23330"/>
      <updated>2015-02-05T06:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[This  UW Madison news article describes the paleobiology application of the DeepDive system, which used text-mining techniques to extract data from publications and build a database. The quality of the database contents equaled that achieved by scientists. HTCondor helped to provide the million hours of compute time needed to build the database from tens of thousands of publications.
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[HTCondor CacheD: Caching for HTC - Part 2]]></title>
      <link href="http://derekweitzel.blogspot.com/2015/01/htcondor-cached-caching-for-htc-part-2.html"/>
      <updated>2015-01-25T15:59:00Z</updated>
      <id>tag:blogger.com,1999:blog-3007054864987759910.post-5260378956420164105</id>
      <author>
        <name><![CDATA[Derek Weitzel]]></name>
        <uri>http://derekweitzel.blogspot.com</uri>
      </author>
      <content type="html"><![CDATA[In the <a href="http://derekweitzel.blogspot.com/2015/01/condor-cached-caching-for-htc-part-1.html">previous post</a>, I discussed why we decided to make the HTCondor CacheD. &nbsp;This time, we will discuss the operation and design of the CacheD, as well as show an example utilizing a BLAST database.<br /><br />It is important to note that the CacheD is still very much "dissertation-ware." &nbsp;It functions enough to demonstrate the improvements, but not enough to be put into production.<br /><br /><h4>The Cache</h4><div>The fundamental unit that the CacheD works with is an immutable set of files in a cache. &nbsp;A user creates and uploads files into the cache. &nbsp;Once the upload is complete, the cache is committed and may not be altered at any time. &nbsp;The cache has a set of metadata associated with it as well, stored as classads in a durable storage database (using the same techniques as the SchedD job queue).</div><div><br /></div><div>The cache has a 'lease', or an expiration date. &nbsp;This lease is a given amount of time that the cache is guaranteed to be available from a particular CacheD. &nbsp;When creating the cache, the user provides a requested cache lifetime. &nbsp;The CacheD can either accept or reject the requested cache lifetime. &nbsp;Once the cache's lifetime expires, it can be deleted by the CacheD and is no longer guaranteed to be available. &nbsp;The user may request to extend the lifetime of a cache after it has already been committed, which the CacheD may or may not accept.</div><div><br /></div><div>The cache also has properties similar to a job. &nbsp;For example, the cache can have it's own set of requirements for which nodes it can be replicated to. &nbsp;By default, a cache is initialized with the requirement that a CacheD has enough disk space to hold the cache. &nbsp;Analogous to the HTCondor matching with jobs, the cache can have requirements, and the CacheD can have requirements. &nbsp;A CacheD requirements may be that the node has enough disk space to hold the matched cache. &nbsp;This two way matching guarantees that any local policies are enforced.</div><div><br /></div><div>The requirements attribute is especially useful when the user aligns the cache's requirements with the jobs that require the data. &nbsp;For example, if the user knows that their processing requires nodes with 8GB of ram available, then there is no point is replicating the cache to a node with less than 8GB of ram.</div><div><br /></div><h4>The CacheD</h4><div>The CacheD is the daemon that manages caches on the local node. &nbsp;Each CacheD is considered a peer to all other CacheD's, there is no further coordination daemon. &nbsp;Each cache serves multiple functions:</div><div><ol><li>Respond to user requests to create, query, update, and delete caches.</li><li>Send replication requests to CacheD's that match each cache's requirements.</li><li>Respond to replication requests from other CacheD's. &nbsp;Matching is done on the cache before transferring the data</li></ol><div>The CacheD keeps a database storing the metadata for each cache. &nbsp;The database is stored using the same techniques as the SchedD uses for jobs to maintain a durable database store. &nbsp;It also maintains a directory containing all of the caches stored on the node.</div></div><div><br /></div><div>The CacheD's user interface is primarily through python bindings, at least for the time being.</div><div><br /></div><h4>CacheD Usage</h4><div>The CacheD is used in conjunction with glideins. &nbsp;The CacheD is started along with other glidein daemons such as the HTCondor StartD.</div><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://4.bp.blogspot.com/-0t61LNEhNP8/VMK3q55wm3I/AAAAAAAAC28/CdyFV9FXD3I/s1600/CachedInitialization.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" src="http://4.bp.blogspot.com/-0t61LNEhNP8/VMK3q55wm3I/AAAAAAAAC28/CdyFV9FXD3I/s1600/CachedInitialization.png" height="344" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Initialization of cache as well as initial replication requests</td></tr></tbody></table><div>The user initializes the cache by creating and uploading it to the user's submit machine. &nbsp;The CacheD connects to remote CacheD's, sending replication requests.</div><div class="separator" style="clear: both; text-align: center;"></div><div><br /></div><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://1.bp.blogspot.com/-aZhjvd3F_M0/VMK8YMswJ9I/AAAAAAAAC3M/_pCf-OX1Q8k/s1600/CachedReplication.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" src="http://1.bp.blogspot.com/-aZhjvd3F_M0/VMK8YMswJ9I/AAAAAAAAC3M/_pCf-OX1Q8k/s1600/CachedReplication.png" height="314" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">The BitTorrent communication between nodes after accepting the replication request</td></tr></tbody></table><div>Once the CacheD's accept the replication request(s), BitTorrent protocol allows for communication between all nodes inside the cluster, as well with the user's submit machine. &nbsp;This graph only shows a single cluster, but this could be replicated to many clusters as well.</div><div><br /></div><h4>In Action&nbsp;</h4><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://2.bp.blogspot.com/-AVc8_0W-tSk/VMLASPT5t8I/AAAAAAAAC3U/IDN7RG2DvkI/s1600/verbose_group.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" src="http://2.bp.blogspot.com/-AVc8_0W-tSk/VMLASPT5t8I/AAAAAAAAC3U/IDN7RG2DvkI/s1600/verbose_group.png" height="308" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Partial graph showing data transfers. &nbsp;Due to overflowing the event queue, not all downloads are captured.</td></tr></tbody></table><div>The above graph shows the data transfer using the BitTorrent protocol between the nodes that have accepted the replication request and the Cache Origin, which is an external node. &nbsp;In this example, only 5 remote CacheD's where started on the cluster. &nbsp;Because of all of the traffic between nodes, this level of detail graph becomes unreadable very quickly when increasing the number of remote CacheD's.</div><div><br /></div><div>You will notice that the Cache Origin only transfers to 2 nodes inside the cluster. &nbsp;The BitTorrent protocol is complicated and difficult to predict, therefore this could be caused by many factors. &nbsp;For example, the two nodes could have found the CacheD origin first, therefore being the first nodes to download it. &nbsp;The other nodes would then have found the internal cluster nodes with portions of the cache, and begun to download from it.<br /><br />It is important to note that even though the ~15GB cache is transferred to all 5 nodes, totalling 75GB of transferred cache, only ~15Gb is transferred from the cache origin, and all of the rest of the transfers are between nodes in the cluster.<br /><br /><h4>Up Next</h4></div><div>In Part 3 of the series, I will look at timings of the transfers using data analysis of trial runs. &nbsp;As a hint, the BitTorrent protocol is slower than direct transfers for 1 to 1 transfers. &nbsp;But it really shines when increasing the number of downloaders and seeders.</div><div><br /></div>]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Monadic 'break' and 'continue' for Scala Sequence Comprehensions]]></title>
      <link href="http://erikerlandson.github.com/blog/2015/01/24/monadic-break-and-continue-for-scala-sequence-comprehensions/"/>
      <updated>2015-01-24T18:54:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2015/01/24/monadic-break-and-continue-for-scala-sequence-comprehensions</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>Author's note: I've since received some excellent feedback from the Scala community, which I included in some <a href="#notes">end notes</a>.</p>

<p>Author's note the 2nd: I later realized I could apply an implicit conversion and mediator class to preserve the traditional ordering: the code has been updated with that approach.</p>

<p>Author's note the 3rd: This concept has been submitted to the Scala project as JIRA <a href="https://issues.scala-lang.org/browse/SI-9120">SI-9120</a> (PR <a href="https://github.com/scala/scala/pull/4275">#4275</a>)</p>

<p>Scala <a href="http://docs.scala-lang.org/tutorials/tour/sequence-comprehensions.html">sequence comprehensions</a> are an excellent functional programming idiom for looping in Scala.  However, sequence comprehensions encompass much more than just looping -- they represent a powerful syntax for manipulating <em>all</em> monadic structures<a href="#ref1">[1]</a>.</p>

<p>The <code>break</code> and <code>continue</code> looping constructs are a popular framework for cleanly representing multiple loop halting and continuation conditions at differing stages in the execution flow.  Although there is no native support for <code>break</code> or <code>continue</code> in Scala control constructs, it is possible to implement them in a clean and idiomatic way for sequence comprehensions.</p>

<p>In this post I will describe a lightweight and easy-to-use implementation of <code>break</code> and <code>continue</code> for use in Scala sequence comprehensions (aka <code>for</code> statements).  The entire implementation is as follows:</p>

<pre><code>object BreakableGenerators {
  import scala.language.implicitConversions

  type Generator[+A] = Iterator[A]
  type BreakableGenerator[+A] = BreakableIterator[A]

  // Generates a new breakable generator from any traversable object.
  def breakable[A](t1: TraversableOnce[A]): Generator[BreakableGenerator[A]] =
    List(new BreakableIterator(t1.toIterator)).iterator

  // Mediates boolean expression with 'break' and 'continue' invocations
  case class BreakableGuardCondition(cond: Boolean) {
    // Break the looping over one or more breakable generators, if 'cond' 
    // evaluates to true.
    def break(b: BreakableGenerator[_], bRest: BreakableGenerator[_]*): Boolean = {
      if (cond) {
        b.break
        for (x &lt;- bRest) { x.break }
      }
      !cond
    }

    // Continue to next iteration of enclosing generator if 'cond' 
    // evaluates to true.
    def continue: Boolean = !cond
  }

  // implicit conversion of boolean values to breakable guard condition mediary
  implicit def toBreakableGuardCondition(cond: Boolean) =
    BreakableGuardCondition(cond)

  // An iterator that can be halted via its 'break' method.  Not invoked directly
  class BreakableIterator[+A](itr: Iterator[A]) extends Iterator[A] {
    private var broken = false
    private[BreakableGenerators] def break { broken = true }

    def hasNext = !broken &amp;&amp; itr.hasNext
    def next = itr.next
  }
}
</code></pre>

<p>The approach is based on a simple subclass of <code>Iterator</code> -- <code>BreakableIterator</code> -- that can be halted by 'breaking' it.  The function <code>breakable(&lt;traversable-object&gt;)</code> returns an Iterator over a single <code>BreakableIterator</code> object.  Iterators are monad-like structures in that they implement <code>map</code> and <code>flatMap</code>, and so its output can be used with <code>&lt;-</code> at the start of a <code>for</code> construct in the usual way.  Note that this means the result of the <code>for</code> statement will also be an Iterator.</p>

<p>Whenever the boolean expression for an <code>if</code> guard is followed by either <code>break</code> or <code>continue</code>, it is implicitly converted to a "breakable guard condition" that supports those methods.  The function <code>break</code> accepts one or more instances of <code>BreakableIterator</code>.  If it evaluates to <code>true</code>, the loops embodied by the given iterators are immediately halted via the associated <code>if</code> guard, and the iterators are halted via their <code>break</code> method.  The <code>continue</code> function is mostly syntactic sugar for a standard <code>if</code> guard, simply with the condition inverted.</p>

<p>Here is a simple example of <code>break</code> and <code>continue</code> in use:</p>

<pre><code>object Main {
  import BreakableGenerators._

  def main(args: Array[String]) {

    val r = for (
      // generate a breakable sequence from some sequential input
      loop &lt;- breakable(1 to 1000);
      // iterate over the breakable sequence
      j &lt;- loop;
      // print out at each iteration
      _ = { println(s"iteration j= $j") };
      // continue to next iteration when 'j' is even
      if { j % 2 == 0 } continue;
      // break out of the loop when 'j' exceeds 5
      if { j &gt; 5 } break(loop)
    ) yield {
      j
    }
    println(s"result= ${r.toList}")
  }
}
</code></pre>

<p>We can see from the resulting output that <code>break</code> and <code>continue</code> function in the usual way.  The <code>continue</code> clause ignores all subsequent code when <code>j</code> is even.  The <code>break</code> clause halts the loop when it sees its first value > 5, which is 7.  Only odd values &lt;= 5 are output from the <code>yield</code> statement:</p>

<pre><code>$ scalac -d /home/eje/class monadic_break.scala
$ scala -classpath /home/eje/class Main
iteration j= 1
iteration j= 2
iteration j= 3
iteration j= 4
iteration j= 5
iteration j= 6
iteration j= 7
result= List(1, 3, 5)
</code></pre>

<p>Breakable iterators can be nested in the way one would expect.  The following example shows an inner breakable loop nested inside an outer one:</p>

<pre><code>object Main {
  import BreakableGenerators._

  def main(args: Array[String]) {
    val r = for (
      outer &lt;- breakable(1 to 7);
      j &lt;- outer;
      _ = { println(s"outer  j= $j") };
      if { j % 2 == 0 } continue;
      inner &lt;- breakable(List("a", "b", "c", "d", "e"));
      k &lt;- inner;
      _ = { println(s"    inner  j= $j  k= $k") };
      if { k == "d" } break(inner);
      if { j == 5  &amp;&amp;  k == "c" } break(inner, outer)
    ) yield {
      (j, k)
    }
    println(s"result= ${r.toList}")
  }
}
</code></pre>

<p>The output demonstrates that the inner loop breaks whenever <code>k=="d"</code>, and so <code>"e"</code> is never present in the <code>yield</code> result.  When <code>j==5</code> and <code>k=="c"</code>, both the inner and outer loops are broken, and so we see that there is no <code>(5,"c")</code> pair in the result, nor does the outer loop ever iterate over 6 or 7:</p>

<pre><code>$ scalac -d /home/eje/class monadic_break.scala
$ scala -classpath /home/eje/class Main
outer  j= 1
    inner  j= 1  k= a
    inner  j= 1  k= b
    inner  j= 1  k= c
    inner  j= 1  k= d
outer  j= 2
outer  j= 3
    inner  j= 3  k= a
    inner  j= 3  k= b
    inner  j= 3  k= c
    inner  j= 3  k= d
outer  j= 4
outer  j= 5
    inner  j= 5  k= a
    inner  j= 5  k= b
    inner  j= 5  k= c
result= List((1,a), (1,b), (1,c), (3,a), (3,b), (3,c), (5,a), (5,b))
</code></pre>

<p>Using <code>break</code> and <code>continue</code> with <code>BreakableIterator</code> for sequence comprehensions is that easy.  Enjoy!</p>

<p><a name="notesname" id="notes"></a></p>

<h5>Notes</h5>

<p>The helpful community on freenode #scala made some excellent observations:</p>

<p>1: Iterators in Scala are not strictly monadic -- it would be more accurate to say they're "things with a flatMap and map method, also they can use filter or withFilter sometimes."  However, I personally still prefer to think of them as "monadic in spirit if not law."</p>

<p>2: The <code>break</code> function, as described in this post, is not truly functional in the sense of referential transparency, as the invocation <code>if break(loop) { condition }</code> involves a side-effect on the variable <code>loop</code>.  I would say that it does maintain "scoped functionality."  That is, the break in non-referential transparency is scoped by the variables in question.  The <code>for</code> statement containing them is referentially transparent with respect to its inputs (provided no other code is breaking referential transparency, of course).</p>

<h5>References</h5>

<p><a name="ref1name" id="ref1">[1] </a><em><a href="http://www.manning.com/bjarnason/">Functional Programming in Scala</a></em>, Paul Chiusano and Runar Bjarnason, (section 6.6)</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Condor CacheD: Caching for HTC - Part 1]]></title>
      <link href="http://derekweitzel.blogspot.com/2015/01/condor-cached-caching-for-htc-part-1.html"/>
      <updated>2015-01-22T16:00:00Z</updated>
      <id>tag:blogger.com,1999:blog-3007054864987759910.post-1889975382858537261</id>
      <author>
        <name><![CDATA[Derek Weitzel]]></name>
        <uri>http://derekweitzel.blogspot.com</uri>
      </author>
      <content type="html"><![CDATA[]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[ HTCondor 8.3.2 released! ( December 23, 2014 )]]></title>
      <link href="manual/v8.3.2/10_3Development_Release.html"/>
      <updated>2014-12-23T06:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[The HTCondor team is pleased to announce the release of HTCondor 8.3.2.
This development series release contains new features that are under
development. This release contains all of the bug fixes from the 8.2.6
stable release.
This new version contains:
the next installment of IPv4/IPv6 mixed mode support: a submit node can
simultaneously interact with an IPv4 and an IPv6 HTCondor pool;
scalability improvements: a reduced memory foot-print of daemons,
a reduced number of TCP connections between submit and execute machines,
and an improved responsiveness from a busy condor_schedd to queries.
A complete list of new features and fixed bugs can be found in the
Version History.
HTCondor 8.3.2 binaries and source code are available from our
Downloads page.
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[ HTCondor 8.2.6 released! ( December 16, 2014 )]]></title>
      <link href="manual/v8.2.6/10_3Stable_Release.html"/>
      <updated>2014-12-16T06:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[The HTCondor Team is pleased to announce the release of HTCondor
version 8.2.6.
This new version contains:
a bug fix to the log rotation of the condor_schedd on Linux platforms;
transfer_input_files now works for directories on Windows platforms;
a correction of the flags passed to the mail program on Linux platforms;
a RHEL 7 platform fix of a directory permission that prevented daemons from starting.
A complete list of fixed bugs can be found in the
Version History.
HTCondor 8.2.6 binaries and source code are available from our
Downloads page.
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Caveat censor]]></title>
      <link href="http://chapeau.freevariable.com/2014/12/caveat-censor.html"/>
      <updated>2014-12-02T16:12:22Z</updated>
      <id>http://chapeau.freevariable.com/2014/12/caveat-censor</id>
      <author>
        <name><![CDATA[William Benton]]></name>
        <uri>http://chapeau.freevariable.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>Over eight years ago, <a href="http://rwmj.wordpress.com">Richard WM Jones</a> wrote a <a href="http://web.archive.org/web/20071013190833/http://blog.merjis.com/2006/11/08/practical-ocaml/">great but disheartening article about his experience serving as a technical reviewer</a> for <a href="http://www.amazon.com/Practical-OCaml-Joshua-B-Smith/dp/159059620X">an infamous book about OCaml</a>.  The post made quite an impression on me at the time and I&rsquo;ve often recalled it over the years whenever opportunities to do prepress reviews have landed in my inbox.  Briefly, Jones was asked to use terrible tools (Microsoft Word) to deal with stilted, error-ridden prose surrounding unidiomatic and broken code.  For his trouble, he got an hourly wage that would have represented a slight raise over what I made mowing my neighbors&#8217; lawns in high school. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Spark performance talk at ApacheCon EU]]></title>
      <link href="http://chapeau.freevariable.com/2014/11/apachecon.html"/>
      <updated>2014-11-18T10:12:40Z</updated>
      <id>http://chapeau.freevariable.com/2014/11/apachecon</id>
      <author>
        <name><![CDATA[William Benton]]></name>
        <uri>http://chapeau.freevariable.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>I&rsquo;ll be speaking <a href="http://apacheconeu2014.sched.org/event/d4dac3bfd7e7ec1ed42bc4fb349a070b#.VGsb-UQo4d8">later this afternoon at ApacheCon EU</a>.  The title of my talk is &ldquo;Iteratively Improving Spark Application Performance.&rdquo; The great thing about Apache Spark is that simple prototype applications are very easy to develop, and even a first attempt at realizing a new analysis will usually work well enough so that it&rsquo;s not frustrating to evaluate it on real data.  However, simple prototypes can often exhibit performance problems that aren&rsquo;t obvious until you know where to look. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Algebraic types and schema inference]]></title>
      <link href="http://chapeau.freevariable.com/2014/11/algebraic-types.html"/>
      <updated>2014-11-03T01:55:10Z</updated>
      <id>http://chapeau.freevariable.com/2014/11/algebraic-types</id>
      <author>
        <name><![CDATA[William Benton]]></name>
        <uri>http://chapeau.freevariable.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>My <a href="http://chapeau.freevariable.com/2014/10/fedmsg-and-spark.html">last post</a> covered some considerations for using Spark SQL on a real-world JSON dataset.  In particular, schema inference can suffer when you&rsquo;re ingesting a dataset of heterogeneous objects.  In this post, I&rsquo;d like to sketch out some ways to connect schema inference to type inference, in order to point to automated solutions to some of the problems we&rsquo;ve seen. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[fedmsg data and Spark SQL]]></title>
      <link href="http://chapeau.freevariable.com/2014/10/fedmsg-and-spark.html"/>
      <updated>2014-10-31T16:28:16Z</updated>
      <id>http://chapeau.freevariable.com/2014/10/fedmsg-and-spark</id>
      <author>
        <name><![CDATA[William Benton]]></name>
        <uri>http://chapeau.freevariable.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>In this post, I&rsquo;ll briefly introduce <a href="http://www.fedmsg.com/en/latest/">fedmsg</a>, the federated message bus developed as part of the Fedora project&rsquo;s infrastructure, and discuss how to ingest fedmsg data for processing with Spark SQL.  While I hope you&rsquo;ll find the analytic possibilities of fedmsg data as interesting as I do, this post will also cover some possible pitfalls you might run into while ingesting complex JSON data or the contents of large SQL databases into Spark SQL more generally. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Faster Random Samples With Gap Sampling]]></title>
      <link href="http://erikerlandson.github.com/blog/2014/09/11/faster-random-samples-with-gap-sampling/"/>
      <updated>2014-09-11T14:57:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2014/09/11/faster-random-samples-with-gap-sampling</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>Generating a random sample of a collection is, logically, a O(np) operation, where (n) is the sample size and (p) is the sampling probability.  For example, extracting a random sample, without replacement, from an array might look like this in pseudocode: [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Getting Started with Mesos on Fedora 21 and CentOS 7]]></title>
      <link href="http://timothysc.github.com/blog/2014/09/08/mesos-breeze/"/>
      <updated>2014-09-08T15:00:00Z</updated>
      <id>http://timothysc.github.com/blog/2014/09/08/mesos-breeze</id>
      <author>
        <name><![CDATA[Timothy St. Clair]]></name>
        <uri>http://timothysc.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p><img class="left" src="http://timothysc.github.com/images/mesos_logo.png"> [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[The Scala Iterator 'drop' Method Generates a Matryoshka Class Nesting]]></title>
      <link href="http://erikerlandson.github.com/blog/2014/09/03/matryoshka-class-construction-from-the-scala-iterator-drop-method/"/>
      <updated>2014-09-04T00:23:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2014/09/03/matryoshka-class-construction-from-the-scala-iterator-drop-method</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>The Scala Iterator <code>drop</code> method has a complexity bug that shows up when one calls <code>drop</code> repeatedly, for example when traversing over an iterator in a loop. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Implementing Parallel Prefix Scan as a Spark RDD Transform]]></title>
      <link href="http://erikerlandson.github.com/blog/2014/08/12/implementing-parallel-prefix-scan-as-a-spark-rdd-transform/"/>
      <updated>2014-08-12T18:37:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2014/08/12/implementing-parallel-prefix-scan-as-a-spark-rdd-transform</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>In my <a href="/blog/2014/08/09/implementing-an-rdd-scanleft-transform-with-cascade-rdds/">previous post</a>, I described how to implement the Scala <code>scanLeft</code> function as an RDD transform.  By definition <code>scanLeft</code> invokes a sequential-only prefix scan algorithm; it does not assume that either its input function <code>f</code> or its initial-value <code>z</code> can be applied in a parallel fashion.   Its companion function <code>scan</code>, however, computes a <em>parallel</em> prefix scan.  In this post I will describe an implementation of parallel prefix <code>scan</code> as an RDD transform. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Implementing an RDD scanLeft Transform With Cascade RDDs]]></title>
      <link href="http://erikerlandson.github.com/blog/2014/08/09/implementing-an-rdd-scanleft-transform-with-cascade-rdds/"/>
      <updated>2014-08-09T16:10:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2014/08/09/implementing-an-rdd-scanleft-transform-with-cascade-rdds</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>In Scala, sequence (and iterator) data types support the <code>scanLeft</code> method for computing a sequential prefix scan on sequence elements: [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[GPUs on the OSG]]></title>
      <link href="http://derekweitzel.blogspot.com/2014/06/gpus-on-osg.html"/>
      <updated>2014-06-25T18:36:00Z</updated>
      <id>tag:blogger.com,1999:blog-3007054864987759910.post-6396978665247392483</id>
      <author>
        <name><![CDATA[Derek Weitzel]]></name>
        <uri>http://derekweitzel.blogspot.com</uri>
      </author>
      <content type="html"><![CDATA[]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Submitting jobs to HTCondor using Python]]></title>
      <link href="http://osgtech.blogspot.com/2014/03/submitting-jobs-to-htcondor-using-python.html"/>
      <updated>2014-03-19T20:32:00Z</updated>
      <id>tag:blogger.com,1999:blog-8803173202887660937.post-689352737761121268</id>
      <author>
        <name><![CDATA[Brian Bockelman]]></name>
        <uri>http://osgtech.blogspot.com</uri>
      </author>
      <content type="html"><![CDATA[]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Moving from a Globus to an HTCondor Compute Element]]></title>
      <link href="http://derekweitzel.blogspot.com/2014/02/moving-from-globus-to-htcondor-compute.html"/>
      <updated>2014-02-27T23:05:00Z</updated>
      <id>tag:blogger.com,1999:blog-3007054864987759910.post-7950648301232917905</id>
      <author>
        <name><![CDATA[Derek Weitzel]]></name>
        <uri>http://derekweitzel.blogspot.com</uri>
      </author>
      <content type="html"><![CDATA[]]></content>
    </entry>
  
</feed>
