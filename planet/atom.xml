<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[Planet HTCondor Meta Feed]]></title>
  <link href="http://htcondor.github.com/planet/atom.xml" rel="self"/>
  <link href="http://htcondor.github.com/"/>
  <updated>2015-10-22T03:40:10-07:00</updated>
  <id>http://htcondor.github.com/planet/atom.xml</id>
  <author>
    <name><![CDATA[HTCondor Project]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
    <entry>
      <title type="html"><![CDATA[Pacing technical talks]]></title>
      <link href="http://chapeau.freevariable.com/2015/10/pacing-technical-talks.html"/>
      <updated>2015-10-21T16:17:01Z</updated>
      <id>http://chapeau.freevariable.com/2015/10/pacing-technical-talks</id>
      <author>
        <name><![CDATA[William Benton]]></name>
        <uri>http://chapeau.freevariable.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>Delivering a technical talk has a lot in common with running a half-marathon or biking a 40k time trial.  You&rsquo;re excited and maybe a little nervous, you&rsquo;re prepared to go relatively hard for a relatively long time, and you&rsquo;re acutely aware of the clock.  In both situations, you might be tempted to take off right from the gun, diving into your hardest effort (or most technical material), but this is a bad strategy.</p>

<p>By going out too hard in the half-marathon, you&rsquo;ll be running on adrenaline instead of on your aerobic metabolism, will burn matches by working hard before warming up fully, and ultimately won&rsquo;t be able to maintain your best possible pace because you&rsquo;ll be spent by the second half of the race.  Similarly, in the talk, your impulse might be to get right to the most elegant and intricate parts of your work immediately after introducing yourself, but if you get there without warming up the audience first, you&rsquo;ll lose most of them along the way.  In both cases, your perception of what you&rsquo;re doing is warped by energy and nerves; the <em>right</em> pace will feel sluggish and awkward; and starting too fast will put you in a hole that will be nearly impossible to recover from.</p>

<p>Delivering a technical talk successfully has a lot in common with choosing an appropriate pacing strategy for an endurance event:  <a href="http://www.runnersworld.com/ask-coach-jenny/how-to-pace-your-first-half-or-full-marathon">by starting out slower than you think you need to, you&rsquo;ll be able to go faster at the end</a>.  Most runners<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> will be able to maintain a higher average pace by doing <a href="https://en.wikipedia.org/wiki/Negative_split">negative splits</a>.  In a race, this means you start out slower than your desired average pace and gradually ramp up over the course of the race so that by the end, you&rsquo;re going faster than your desired average pace.  By starting out easy, your cardiovascular system will warm up, your connective tissue will get used to the stress of pounding on the pavement, and your muscles will start buffering lactic acid; this will reduce muscle fatigue and save your anaerobic energy for the final sprint.</p>

<p>You can apply the general strategy of negative splits to a talk as well.  Instead of warming up cold muscles and your aerobic energy systems before making them work, you&rsquo;re preparing a group of smart people to learn why they should care about your topic before making them think about it too much.  Start off slow:  provide background, context, and examples.  Unless you&rsquo;re a very experienced speaker, this will feel agonizingly slow at first.</p>

<p>It&rsquo;s understandable that it might feel remedial and boring to you to explain why your work is relevant.  After all, you&rsquo;re deep in your topic and have probably long since forgotten what it was like to learn about it for the first time.  Examples and visual explanations might seem like a waste of time before you get to your clever implementation, elegant proof, or sophisticated model.  You have some <em>serious detail</em> to cover, after all!  Your audience, however, isn&rsquo;t prepared for that detail yet.  If you skip the warm-up and go straight to that detail, you&rsquo;ll lose audience engagement, and it&rsquo;s nearly impossible to recover from that; it&rsquo;ll certainly prevent you from covering as much as you might have otherwise wanted to.</p>

<p>Remember that your audience are smart people who chose to attend your talk instead of sitting out in the hall.  They&rsquo;d probably rather be learning something from you than halfheartedly reading email.  But they also almost certainly don&rsquo;t know as much about your topic as you do.  Ease them in to it, warm them up, and give them plenty of context first.  You&rsquo;ll be able to cover more ground that way.</p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p>Pacing in cycling time trials can be a little more complicated depending on the terrain and wind but in general being able to finish stronger than you started is still desirable.<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Notes from Flink Forward]]></title>
      <link href="http://chapeau.freevariable.com/2015/10/notes-from-flink-forward.html"/>
      <updated>2015-10-20T15:48:44Z</updated>
      <id>http://chapeau.freevariable.com/2015/10/notes-from-flink-forward</id>
      <author>
        <name><![CDATA[William Benton]]></name>
        <uri>http://chapeau.freevariable.com/</uri>
      </author>
      <content type="html"><![CDATA[<p><a data-flickr-embed="true"  href="https://www.flickr.com/photos/willb/22238437291/in/dateposted-public/" title="Brandenburger Tor lightshow"><img src="https://farm1.staticflickr.com/739/22238437291_22636e4a72_b.jpg" width="1024" height="819" alt="Brandenburger Tor lightshow"></a><script async src="//embedr.flickr.com/assets/client-code.js" charset="utf-8"></script></p>

<p>I was in Berlin last week for <a href="http://flink-forward.org">Flink Forward</a>, the inaugural Apache Flink conference.  I&rsquo;m still learning about Flink, and Flink Forward was a great place to learn more.  In this post, I&rsquo;ll share some of what I consider its coolest features and highlight some of the talks I especially enjoyed.  Videos of the talks should all be online soon, so you&rsquo;ll be able to check them out as well.</p>

<h2>Background</h2>

<p>Apache Flink is a data processing framework for the JVM that is most popular for streaming workloads with high throughput and low latency, although it is also general enough to support batch processing.  Flink has a pleasant collection-style API, offers stateful elementwise transformations on streams (think of a <code>fold</code> function), can be configured to support fault-tolerance with exactly-once delivery, and does all of this while achieving extremely high performance.  Flink is especially attractive for use in contemporary multitenant environments because it manages its own memory and thus Flink jobs can run well in containers on overprovisioned systems (where CPU cycles may be relatively abundant but memory may be strictly constrained).</p>

<h2>Keynotes and lightning talks</h2>

<p>Kostas Tzoumas and Stephan Ewan (both of data Artisans) shared a keynote in which they presented the advancements in Flink 0.10 (to be released soon) and shared the roadmap for the next release, which will be Flink 1.0.  The most interesting parts of this keynote for me were the <a href="http://data-artisans.com/batch-is-a-special-case-of-streaming/">philosophical arguments for the generality and importance of stream processing in contemporary event-driven data applications</a>.  Many users of batch-processing systems simulate streaming workflows by explicitly encoding windows in the structure of their input data (e.g., by using one physical file or directory to correspond to a day, month, or year worth of records) or by using various workarounds inspired by technical limitations (e.g., the &ldquo;lambda architecture&rdquo; or bespoke but narrowly-applicable stream processors).  However, mature stream processing frameworks not only enable a wide range of applications that process live events, but they also are general enough to handle batch workloads as a special case (i.e., by processing a stream with only one window).<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup></p>

<p>Of course, the workarounds that data engineers have had to adopt to handle streaming data in batch systems are only necessary given an absence of mature stream processing frameworks.  The streaming space has improved a great deal recently, and this talk gave a clear argument that Flink was mature enough for demanding and complex applications.  Flink offers a flexible treatment of time: events can be processed immediately (one at a time), in windows based on when the events arrived at the processor, or in windows based on when the events were actually generated (even if they arrived out of order).  Flink supports failure recovery with exactly-once delivery but also offers extremely high throughput and low latency:  a basic Flink stream processing application offers two orders of magnitude more throughput than an equivalent Storm application.  Flink also provides a batch-oriented API with a collection-style interface and an optimizing query planner.</p>

<p>After the keynote, there were several lightning talks.  Lightning talks at many events are self-contained (and often speculative, provocative, or describing promising work in progress).  However, these lightning talks were abbreviated versions of talks on the regular program.  In essence, they were ads for talks to see later (think of how academic CS conference talks are really ads for papers to read later).  This was a cool idea and definitely helped me navigate a two-track program that was full of interesting abstracts.</p>

<h2>Everyday Flink</h2>

<p>Michael Häusler of ResearchGate gave a talk in which he talked about the process of evaluating new data processing frameworks, focusing in particular on determining whether a framework makes simple tasks simple.  (Another step, following Alan Kay&rsquo;s famous formulation, is determining whether or not a framework makes complex tasks possible.)  The &ldquo;simple task&rdquo; that Häusler set out to solve was finding the top 5 coauthors for every author in a database of publications; he implemented this task in Hive (running on Tez), Hadoop MapReduce, and Flink.  Careful readers will note that this is not really a fair fight:  SQL and HiveQL do not admit straightforward implementations of top-<em>k</em> queries and MapReduce applications are not known for elegant and terse codebases; indeed, Häusler acknowledged as much.  However, it was still impressive to see how little code was necessary to solve this problem with Flink, especially when contrasted with the boilerplate of MapReduce or all of the machinery to implement a user-defined aggregate function to support top-<em>k</em> in Hive.  The Flink solution was also twice as fast as the custom MapReduce implementation, which was in turn faster than Hive on Tez.</p>

<h2>Declarative Machine Learning with the Samsara DSL</h2>

<p>Sebastian Schelter introduced Samsara, a DSL for machine learning and linear algebra.  Samsara supports in-memory vectors (both dense and sparse), in-memory matrices, and distributed row matrices, and provides an R-like syntax embedded in Scala for operations.  The distributed row matrices are a unique feature of Samsara; they support only a subset of matrix operations (i.e., ones that admit efficient distributed implementations) and go through a process of algebraic optimization (including generating logical and physical plans) to minimize communication during execution.  Samsara can target Flink, Spark, and H2O.</p>

<h2>Streaming and parallel decision trees in Flink</h2>

<p>Training decision trees in batch frameworks requires a view of the entire learning set (and sufficient training data to generate a useful tree).  In streaming applications, each event is seen only once, the classifier must be available immediately (even if there is little data to train on) and the classifier should take feedback into account in real time.  In this talk, Anwar Rizal of Amadeus presented a technique for training decision trees on streaming data by building and aggregating approximate histograms for incoming features and labels.</p>

<h2>Juggling with bits and bytes &mdash; how Apache Flink operates on binary data</h2>

<p>Applications using the Java heap often exhibit appalling memory efficiency; the heap footprint of Java library data structures can be <a href="https://www.cs.virginia.edu/kim/publicity/pldi09tutorials/memory-efficient-java-tutorial.pdf">75% overhead or more</a>.  Since data processing applications frequently create, manipulate, and serialize many objects &mdash; some of which may be quite short-lived &mdash; there are potentially significant performance pitfalls to using the JVM directly for memory allocation.  In this talk, Fabian Hueske of data Artisans presented Flink&rsquo;s approach:  combining a custom memory-management and serialization stack with algorithms that operate directly on compressed data.  Flink jobs are thus more memory-efficient than programs that use the Java heap directly, exhibit predictable space usage, and handle running out of memory gracefully by spilling intermediate results to disk.  In addition, Flink&rsquo;s use of database-style algorithms to sort, filter, and join compressed data reduces computation and communication costs.</p>

<h2>Stateful Stream Processing</h2>

<p>Data processing frameworks like Flink and Spark support collection-style APIs where distributed collections or streams can be processed with operations like <code>map</code>, <code>filter</code>, and so on.  In addition to these, it is useful to support transformations that include <em>state</em>, analogously to the <code>fold</code> function on local collections.  Of course, <code>fold</code> by itself is fairly straightforward, but a reliable <code>fold</code>-style operation that can recover in the face of worker failures is more interesting.  In this talk, Márton Balassi and Gábor Hermann presented an overview of several different approaches to supporting reliable stream processing with state:  the approaches used by Flink (both versions 0.9.1 and 0.10), Spark Streaming, Samza, and Trident.  As one might imagine, Spark Streaming and Samza get a lot of mileage out of delegating to underlying models (immutable RDDs in Spark&rsquo;s case and a reliable unified log in Samza&rsquo;s).  Flink&rsquo;s approach of using distributed snapshots exhibits good performance and enables exactly-once semantics, but it also seems simpler to use than alternatives.  This has become a recurring theme in my investigation of Flink:  technical decisions that are advertised as improving performance (latency, throughput, etc.) also, by happy coincidence, admit a more elegant programming model.</p>

<h2>Fault-tolerance and job recovery in Apache Flink</h2>

<p>This talk was an apt chaser for the Stateful Stream Processing talk.  Till Rohrmann presented Flink&rsquo;s approaches to checkpointing and recovery, showing how Flink can be configured to support at-most-once delivery (the weakest guarantee), at-least-once delivery, or exactly-once delivery (the strongest guarantee).  The basic approach Flink uses for checkpointing operator state is the <a href="https://en.wikipedia.org/wiki/Snapshot_algorithm">Chandy-Lamport snapshot algorithm</a>, which enables consistent distributed snapshots in a way that is transparent to the application programmer.  This approach also enables configurable tradeoffs between throughput and snapshot interval, but it&rsquo;s far faster (and nicer to use) than Storm&rsquo;s approach in any case.  Recovering operator state is only part of the fault-tolerance picture, though; Till&rsquo;s talk also introduced Flink&rsquo;s approach for supporting a highly-available Job Manager.</p>

<h2>Other talks worth checking out</h2>

<p>Here are a few talks that I&rsquo;d like to briefly call out as worth watching:</p>

<ul>
<li><a href="http://flink-forward.org/?session=automatic-detection-of-web-trackers-at-telefonica-research">&ldquo;Automatic detection of web trackers&rdquo;</a>, presented by Vasia Kalavri, was a cool application of graph processing in Flink.</li>
<li><a href="http://flink-forward.org/?session=applying-kappa-architecture-in-the-telecom-industry">&ldquo;Applying Kappa architecture in the telecom industry&rdquo;</a>, presented by Ignacio Mulas Viela, showed how to put a realistic streaming topology into production.</li>
<li>In <a href="http://flink-forward.org/?session=a-tale-of-squirrels-and-storms">&ldquo;A tale of squirrels and storms&rdquo;</a>, Matthias Sax introduced the Storm compatibility layer for Flink, enabling users to run Storm topologies on Flink with minimal code changes.</li>
<li><a href="http://flink-forward.org/?session=notions-of-time-how-apache-flink-handles-time-and-windows">Aljoscha Krettek&rsquo;s talk</a> covered the different approaches Flink supports for defining windows over streams.</li>
<li>My colleague <a href="https://www.flickr.com/photos/willb/22227982835/">Suneel Marthi</a> presented on a <a href="http://flink-forward.org/?session=tbd-3">Flink port of the BigPetStore big data application blueprints</a>.</li>
</ul>


<h2>General notes</h2>

<p><a data-flickr-embed="true"  href="https://www.flickr.com/photos/willb/22322707802/in/dateposted-public/" title="Kulturbrauerei"><img src="https://farm1.staticflickr.com/771/22322707802_8b4fa7662e_b.jpg" width="1024" height="820" alt="Kulturbrauerei"></a><script async src="//embedr.flickr.com/assets/client-code.js" charset="utf-8"></script></p>

<p>The data Artisans team and the Flink community clearly put a lot of hard work towards making this a really successful conference.  The <a href="https://en.wikipedia.org/wiki/Kulturbrauerei">venue</a> (pictured above) was unique and cool, the overall vibe was friendly and technical, and I didn&rsquo;t see a single talk that I regretted attending.  (This is high praise indeed for a technical conference; I may have been lucky, but I suspect it&rsquo;s more likely that the committee picked a good program.)  I especially appreciated the depth of technical detail in the talks by Flink contributors on the second afternoon, covering both design tradeoffs and implementation decisions.  I&rsquo;m hoping to be back for a future iteration.</p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p>Indeed, <a href="http://flink-forward.org/?session=stream-and-batch-processing-in-one-system-apache-flinks-streaming-data-flow-engine">as we saw later in the conference</a>, the difference between &ldquo;batch&rdquo; workloads and &ldquo;streaming&rdquo; workloads can be as simple as a set of policy decisions by the scheduler.<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[ HTCondor 8.5.0 released! ( October 12, 2015 )]]></title>
      <link href="manual/v8.5.0/10_3Development_Release.html"/>
      <updated>2015-10-12T05:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[The HTCondor team is pleased to announce the release of HTCondor 8.5.0.
This development series release contains new features that are under
development. This release contains all of the bug fixes from the 8.4.0
stable release.

Enhancements in the release include:
multiple enhancements to the python bindings;
the condor_schedd no longer changes the ownership of spooled job files;
spooled job files are visible to only the user account by default;
the condor_startd records when jobs are evicted by preemption or draining.

Further details can be found in the
Version History.
HTCondor 8.5.0 binaries and source code are available from our
Downloads page.
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[A Library of Binary Tree Algorithms as Mixable Scala Traits]]></title>
      <link href="http://erikerlandson.github.com/blog/2015/09/26/a-library-of-binary-tree-algorithms-as-mixable-scala-traits/"/>
      <updated>2015-09-26T19:43:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2015/09/26/a-library-of-binary-tree-algorithms-as-mixable-scala-traits</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>In this post I am going to describe some work I've done recently on a system of Scala traits that support tree-based collection algorithms prefix-sum, nearest key query and value increment in a mixable format, all backed by Red-Black balanced tree logic, which is also a fully inheritable trait.</p>

<p>This post eventually became a bit more sprawling and "tl/dr" than I was expecting, so by way of apology, here is a table of contents with links:</p>

<ol>
<li><a href="#motivation">Motivating Use Case</a></li>
<li><a href="#overview">Library Overview</a></li>
<li><a href="#redblack">A Red-Black Tree Base Class</a></li>
<li><a href="#nodemap">Node Inheritance Example: NodeMap[K,V]</a></li>
<li><a href="#orderedmaplike">Collection Trait Example: OrderedMapLike[K,V,IN,M]</a></li>
<li><a href="#orderedmap">Collection Example: OrderedMap[K,V]</a></li>
<li><a href="#mixing">Finale: Trait Mixing</a></li>
</ol>


<p><a name="motivation"></a></p>

<h5>A Motivating Use Case</h5>

<p>The skeptical programmer may be wondering what the point of Yet Another Map Collection really is, much less an entire class hierarchy.  The use case that inspired this work was my project of implementing the <a href="https://github.com/tdunning/t-digest/blob/master/docs/t-digest-paper/histo.pdf">t-digest algorithm</a>.  Discussion of t-digest is beyond the scope of this post, but suffice it to say that constructing a t-digest requires the maintenance of a collection of "cluster" objects, that needs to satisfy the following several properties:</p>

<ol>
<li>an entry contains one <strong>or more</strong> cluster objects at a given numeric location</li>
<li>entries are maintained in a numeric key order</li>
<li>entries will be frequently inserted and deleted, in arbitrary order</li>
<li>given a numeric key value, be able to find the entry nearest to that value</li>
<li>given a key, compute a <a href="https://en.wikipedia.org/wiki/Prefix_sum">prefix-sum</a> for that value</li>
<li>all of the above should be bounded by logarithmic time complexity</li>
</ol>


<p>Propreties 2,3 and 6 are commonly satisfied by a map structure backed by some variety of balanced tree representation, of which the best-known is the <a href="https://en.wikipedia.org/wiki/Red%E2%80%93black_tree">Red-Black tree</a>.</p>

<p>Properties 1, 4 and 5 are more interesting.  Property 1 -- representing a collection of multiple objects at each entry -- can be accomplished in a generalizable way by noting that a collection is representable as a monoid, and so supporting values that can be incremented with respect to a <a href="http://twitter.github.io/algebird/index.html#com.twitter.algebird.Monoid">user-supplied monoid relation</a> can satisfy property-1, but also can support many other kinds of update, including but not limited to classical numeric incrementing operations.</p>

<p>Properties 4 and 5 -- nearest-entry queries and prefix-sum queries -- are also both supportable in logarithmic time using a tree data structure, provided that tree is balanced.  Again, the details of the algorithms are out of the current scope, however they are not extremely complex, and their implementations are available in the code.</p>

<p>A reader with their software engineering hat on will notice that these properties are <em>orthogonal</em>.  A programmer might be interested in a data structure supporting any one of them, or in some mixed combination.   This kind of situation fairly shouts "Scala traits" (or, alternatively, interfaces in Java, etc).  With that idea in mind, I designed a system of Scala collection traits that support all of the above properties, in a pure trait form that is fully "mixable" by the programmer, so that one can use exactly the properties needed, but not pay for anything else.</p>

<p><a name="overview"></a></p>

<h5>Library Overview</h5>

<p>The source files containing the code discussed in the remainder of this post are available <a href="https://github.com/erikerlandson/silex/tree/blog/rbtraits/src/main/scala/com/redhat/et/silex/maps">here</a>, and the unit testing files are <a href="https://github.com/erikerlandson/silex/tree/blog/rbtraits/src/test/scala/com/redhat/et/silex/maps">here</a>.  At the time of this post the tree algorithm trait system is a <a href="https://github.com/willb/silex/pull/35">PR against the silex project</a>.</p>

<p>The library consists broadly of 3 kinds of traits:</p>

<ul>
<li>tree node traits -- implement core tree support for some functionality</li>
<li>collection traits -- provide additional collection API methods the user</li>
<li>collections -- instantiate a usable incarnation of a collection</li>
</ul>


<p>For the programmer who wishes to either create a trait mixture, or add new mixable traits, the collections also function as reference implementations.</p>

<p>The three tables that follow summarize the currently available traits of each kind listed above.  They are (at the time of this posting) all under the package namespace <code>com.redhat.et.silex.maps</code>:</p>

<p><head><style>
table, th, td {
border: 1px solid black;
border-collapse: collapse;
}
th, td {
padding: 10px;
}
th {
text-align: center;
}
</style></head></p>

<table>
<caption>Tree Node Traits</caption>
<tr><td>trait</td><td>sub-package</td><td>description</td></tr>
<tr><td>Node[K]</td> <td>redblack.tree</td><td>Fundamental Red-Black tree functionality</td></tr>
<tr><td>NodeMap[K,V]</td><td>ordered.tree</td><td>Support a mapping from keys to values</td></tr>
<tr><td>NodeNear[K]</td><td>nearest.tree</td><td>Nearest-entry query (key-only)</td></tr>
<tr><td>NodeNearMap[K,V]</td><td>nearest.tree</td><td>Nearest-entry query for key/value maps</td></tr>
<tr><td>NodeInc[K,V]</td><td>increment.tree</td><td>Increment values w.r.t. a monoid</td></tr>
<tr><td>NodePS[K,V,P]</td><td>prefixsum.tree</td><td>Prefix sum queries by key (w.r.t. a monoid)</td></tr>
</table>




<br>


<table>
<caption>Collection Traits</caption>
<tr><td>trait</td><td>sub-package</td><td>description</td></tr>
<tr><td>OrderedSetLike[K,IN,M]</td><td>ordered</td><td>ordered set of keys</td></tr>
<tr><td>OrderedMapLike[K,V,IN,M]</td><td>ordered</td><td>ordered key/value map</td></tr>
<tr><td>NearestSetLike[K,IN,M]</td><td>nearest</td><td>nearest entry query on keys</td></tr>
<tr><td>NearestMapLike[K,V,IN,M]</td><td>nearest</td><td>nearest entry query on key/value map</td></tr>
<tr><td>IncrementMapLike[K,V,IN,M]</td><td>increment</td><td>increment values w.r.t a monoid</td></tr>
<tr><td>PrefixSumMapLike[K,V,P,IN,M]</td><td>prefixsum</td><td>prefix sum queries w.r.t. a monoid</td></tr>
</table>




<br>


<table>
<caption>Concrete Collections</caption>
<tr><td>trait</td><td>sub-package</td><td>description</td></tr>
<tr><td>OrderedSet[K]</td><td>ordered</td><td>ordered set</td></tr>
<tr><td>OrderedMap[K,V]</td><td>ordered</td><td>ordered key/value map</td></tr>
<tr><td>NearestSet[K]</td><td>nearest</td><td>ordered set with nearest-entry query</td></tr>
<tr><td>NearestMap[K,V]</td><td>nearest</td><td>ordred map with nearest-entry query</td></tr>
<tr><td>IncrementMap[K,V]</td><td>increment</td><td>ordered map with value increment w.r.t. a monoid</td></tr>
<tr><td>PrefixSumMap[K,V,P]</td><td>prefixsum</td><td>ordered map with prefix sum query w.r.t. a monoid</td></tr>
</table>




<br>


<p>The following diagram summarizes the organization and inheritance relationships of the classes.</p>

<p><img src="/assets/images/rbtraits/rbtraits.png" alt="diagram" /></p>

<p><a name="redblack"></a></p>

<h5>A Red/Black Tree Base Class</h5>

<p>The most fundamental trait in this hierarchy is the trait that embodies Red-Black balancing; a "red-black-ness" trait, as it were.  This trait supplies the axiomatic tree operations of insertion, deletion and key lookup, where the Red-Black balancing operations are encapsulated for insertion (due to <a href="http://journals.cambridge.org/action/displayAbstract?fromPage=online&amp;aid=44273">Chris Okasaki</a>) and deletion (due to <a href="http://www.cs.kent.ac.uk/people/staff/smk/redblack/rb.html">Stefan Kahrs</a>)  Note that Red-Black trees do not assume a separate value, as in a map, but require only keys (thus implementing an ordered set over the key type):</p>

<p>``` scala
object tree {
  /<em>* The color (red or black) of a node in a Red/Black tree </em>/
  sealed trait Color
  case object R extends Color
  case object B extends Color</p>

<p>  /<em>* Defines the data payload of a tree node </em>/
  trait Data[K] {</p>

<pre><code>/** The axiomatic unit of data for R/B trees is a key */
val key: K
</code></pre>

<p>  }</p>

<p>  /** Base class of a Red/Black tree node</p>

<pre><code>* @tparam K The key type
*/
</code></pre>

<p>  trait Node[K] {</p>

<pre><code>/** The ordering that is applied to key values */
val keyOrdering: Ordering[K]

/** Instantiate an internal node. */
protected def iNode(color: Color, d: Data[K], lsub: Node[K], rsub: Node[K]): INode[K]

// ... declarations for insertion, deletion and key lookup ...

// ... red-black balancing rules ...
</code></pre>

<p>  }</p>

<p>   /<em>* Represents a leaf node in the Red Black tree system </em>/
  trait LNode[K] extends Node[K] {</p>

<pre><code>// ... basis case insertion, deletion, lookup ...
</code></pre>

<p>  }</p>

<p>  /<em>* Represents an internal node (Red or Black) in the Red Black tree system </em>/
  trait INode[K] extends Node[K] {</p>

<pre><code>/** The Red/Black color of this node */
val color: Color
/** Including, but not limited to, the key */
val data: Data[K]
/** The left sub-tree */
val lsub: Node[K]
/** The right sub-tree */
val rsub: Node[K]

// ... implementations for insertion, deletion, lookup ...
</code></pre>

<p>  }
}
<code>``
I will assume most readers are familiar with basic binary tree operations, and the Red-Black rules are described elsewhere (I adapted them from the Scala red-black implementation).  For the purposes of this discussion, the most interesting feature is that this is a _pure Scala trait_.  All</code>val` declarations are abstract.  This trait, by itself, cannot function without a subclass to eventually perform dependency injection.   However, this abstraction allows the trait to be inherited freely -- any programmer can inherit from this trait and get a basic Red-Black balanced tree for (nearly) free, as long as a few basic principles are adhered to for proper dependency injection.</p>

<p>Another detail to call out is the abstraction of the usual <code>key</code> with a <code>Data</code> element.  This element represents any node payload that is moved around as a unit during tree structure manipulations, such as balancing pivots.  In the case of a map-like subclass, <code>Data</code> is extended to include a <code>value</code> field as well as a <code>key</code> field.</p>

<p>The other noteworthy detail is the abstract definition <code>def iNode(color: Color, d: Data[K], lsub: Node[K], rsub: Node[K]): INode[K]</code> - this is the function called to create any new tree node.  In fact, this function, when eventually instantiated, is what performs dependency injection of other tree node fields.</p>

<p><a name="nodemap"></a></p>

<h5>Node Inheritance Example: NodeMap[K,V]</h5>

<p>A relatively simple example of node inheritance is hopefully instructive.  Here is the definition for tree nodes supporting a key/value map:</p>

<p>``` scala
object tree {
  /<em>* Trees that back a map-like object have a value as well as a key </em>/
  trait DataMap[K, V] extends Data[K] {</p>

<pre><code>val value: V
</code></pre>

<p>  }</p>

<p>  /** Base class of ordered K/V tree node</p>

<pre><code>* @tparam K The key type
* @tparam V The value type
*/
</code></pre>

<p>  trait NodeMap[K, V] extends Node[K]</p>

<p>  trait LNodeMap[K, V] extends NodeMap[K, V] with LNode[K]</p>

<p>  trait INodeMap[K, V] extends NodeMap[K, V] with INode[K] {</p>

<pre><code>val data: DataMap[K, V]
</code></pre>

<p>  }
}
```</p>

<p>Note that in this case very little is added to the red/black functionality already provided by <code>Node[K]</code>.  A <code>DataMap[K,V]</code> trait is defined to add a <code>value</code> field in addition to the <code>key</code>, and the internal node <code>INodeMap[K,V]</code> refines the type of its <code>data</code> field to be <code>DataMap[K,V]</code>.  The semantics is little more than "tree nodes now carry a value in addition to a key."</p>

<p>A tree node trait inherits from its own parent class <em>and</em> the corresponding traits for any mixed-in functionality.  So for example <code>INodeMap[K,V]</code> inherits from <code>NodeMap[K,V]</code> but also <code>INode[K]</code>.</p>

<p><a name="orderedmaplike"></a></p>

<h5>Collection Trait Example: OrderedMapLike[K,V,IN,M]</h5>

<p>Continuing with the ordered map example, here is the definition of the collection trait for an ordered map:</p>

<p>``` scala
trait OrderedMapLike[K, V, IN &lt;: INodeMap[K, V], M &lt;: OrderedMapLike[K, V, IN, M]]</p>

<pre><code>extends NodeMap[K, V] with OrderedLike[K, IN, M] {
</code></pre>

<p>  /<em>* Obtain a new map with a (key, val) pair inserted </em>/
  def +(kv: (K, V)) = this.insert(</p>

<pre><code>new DataMap[K, V] {
  val key = kv._1
  val value = kv._2
}).asInstanceOf[M]
</code></pre>

<p>  /<em>* Get the value stored at a key, or None if key is not present </em>/
  def get(k: K) = this.getNode(k).map(_.data.value)</p>

<p>  /<em>* Iterator over (key,val) pairs, in key order </em>/
  def iterator = nodesIterator.map(n => ((n.data.key, n.data.value)))</p>

<p>  /<em>* Container of values, in key order </em>/
  def values = valuesIterator.toIterable</p>

<p>  /<em>* Iterator over values, in key order </em>/
  def valuesIterator = nodesIterator.map(_.data.value)
}
<code>``
You can see that this trait supplies collection API methods that a Scala programmer will recognize as being standard for any map-like collection.  Note that this trait also inherits other standard methods from</code>OrderedLike[K,IN,M]<code>(common to both sets and maps) and _also_ inherits from</code>NodeMap[K,V]<code>: In other words, a collection is effectively yet another kind of tree node, with additional collection API methods mixed in.   Note also the use of "self types" (the type parameter</code>M`), which allows the collection to return objects of its own kind.  This is crucial for allowing operations like data insertion to return an object that also supports node insertion, and to maintain consistency of type across operations.  The collection type is properly "closed" with respect to its own operations.</p>

<p><a name="orderedmap"></a></p>

<h5>Collection Example: OrderedMap[K,V]</h5>

<p>To conclude the ordered map example, consider the task of defining a concrete instantiation of an ordered map:
``` scala
sealed trait OrderedMap[K, V] extends OrderedMapLike[K, V, INodeMap[K, V], OrderedMap[K, V]] {
  override def toString =</p>

<pre><code>"OrderedMap(" +
  nodesIterator.map(n =&gt; s"${n.data.key} -&gt; ${n.data.value}").mkString(", ") +
")"
</code></pre>

<p>}
<code>``
You can see that (aside from a convenience override of</code>toString<code>) the trait</code>OrderedMap[K,V]<code>is nothing more than a vehicle for instantiating a particular concrete</code>OrderedMapLike[K,V,IN,M]<code>subtype, with particular concrete types for internal node (</code>INodeMap[K,V]`) and its own self-type.</p>

<p>Things become a little more interesting inside the companion object <code>OrderedMap</code>:
``` scala
object OrderedMap {
  def key<a href="implicit%20ord:%20Ordering[K]">K</a> = new AnyRef {</p>

<pre><code>def value[V]: OrderedMap[K, V] =
  new InjectMap[K, V](ord) with LNodeMap[K, V] with OrderedMap[K, V]
</code></pre>

<p>  }
}
<code>``
Note that the object returned by the factory method is upcast to</code>OrderedMap[K,V]<code>, but in fact has the more complicated type:</code>InjectMap[K,V] with LNodeMap[K,V] with OrderedMap[K,V]<code>.  There are a couple things going on here.  The trait</code>LNodeMap[K,V]` ensures that the new object is in particular a leaf node, which embodies a new empty tree in the Red-Black tree system.</p>

<p>The type <code>InjectMap[K,V]</code> has an even more interesting purpose.  Here is its definition:
``` scala
class InjectMap<a href="val%20keyOrdering:%20Ordering[K]">K, V</a> {
  def iNode(clr: Color, dat: Data[K], ls: Node[K], rs: Node[K]) =</p>

<pre><code>new InjectMap[K, V](keyOrdering) with INodeMap[K, V] with OrderedMap[K, V] {
  // INode
  val color = clr
  val lsub = ls
  val rsub = rs
  val data = dat.asInstanceOf[DataMap[K, V]]
}
</code></pre>

<p>}
<code>``
Firstly, note that it is a bona fide _class_, as opposed to a trait.  This class is where, finally, all things abstract are made real -- "dependency injection" in the parlance of Scala idioms.  You can see that it defines the implementation of abstract method</code>iNode<code>, and that it does this by returning yet _another_</code>InjectMap[K,V]<code>object, mixed with both</code>INodeMap[K,V]<code>and</code>OrderedMap[K,V]`, thus maintaining closure with respect to all three slices of functionality: dependency injection, the proper type of internal node, and map collection methods.</p>

<p>The various abstract <code>val</code> fields <code>color</code>, <code>data</code>, <code>lsub</code> and <code>rsub</code> are all given concrete values inside of <code>iNode</code>.  Here is where the value of concrete "reference" implementations manifests.  Any fields in the relevant internal-node type must be instantiated here, and the logic of instantiation cannot be inherited while still preserving the ability to mix abstract traits.  Therefore, any programmer wishing to create a new concrete sub-class must replicate the logic for instantiating all inherited in an internal node.</p>

<p>Another example makes the implications more clear.  Here is the definition of injection for a <a href="https://github.com/erikerlandson/silex/blob/blog/rbtraits/src/test/scala/com/redhat/et/silex/maps/mixed.scala">collection that mixes in all three traits</a> for incrementable values, nearest-key queries, and prefix-sum queries:</p>

<p>``` scala
  class Inject[K, V, P](</p>

<pre><code>val keyOrdering: Numeric[K],
val valueMonoid: Monoid[V],
val prefixMonoid: IncrementingMonoid[P, V]) {
  def iNode(clr: Color, dat: Data[K], ls: Node[K], rs: Node[K]) =
  new Inject[K, V, P](keyOrdering, valueMonoid, prefixMonoid)
      with INodeTD[K, V, P] with TDigestMap[K, V, P] {
    // INode[K]
    val color = clr
    val lsub = ls.asInstanceOf[NodeTD[K, V, P]]
    val rsub = rs.asInstanceOf[NodeTD[K, V, P]]
    val data = dat.asInstanceOf[DataMap[K, V]]
    // INodePS[K, V, P]
    val prefix = prefixMonoid.inc(prefixMonoid.plus(lsub.pfs, rsub.pfs), data.value)
    // INodeNear[K, V]
    val kmin = lsub match {
      case n: INodeTD[K, V, P] =&gt; n.kmin
      case _ =&gt; data.key
    }
    val kmax = rsub match {
      case n: INodeTD[K, V, P] =&gt; n.kmax
      case _ =&gt; data.key
    }
  }
</code></pre>

<p>  }
```
Here you can see that all logic for both "basic" internal nodes and also for maintaining prefix sums, and key min/max information for nearest-entry queries, must be supplied.  If there is a singularity in this design here is where it is.  The saving grace is that it is localized into a single well defined place, and any logic can be transcribed from a proper reference implementation of whatever traits are being mixed.</p>

<p><a name="mixing"></a></p>

<h5>Finale: Trait Mixing</h5>

<p>I will conclude by showing the code for mixing tree node traits and collection traits, which is elegant.  Here are type definitions for tree nodes and collection traits that inherit from incrementable values, nearest-key queries, and prefix-sum queries, and there is almost no code except the proper inheritances:</p>

<p>``` scala
object tree {
  import com.redhat.et.silex.maps.increment.tree.<em>
  import com.redhat.et.silex.maps.prefixsum.tree.</em>
  import com.redhat.et.silex.maps.nearest.tree._</p>

<p>  trait NodeTD[K, V, P] extends NodePS[K, V, P] with NodeInc[K, V] with NodeNearMap[K, V]</p>

<p>  trait LNodeTD[K, V, P] extends NodeTD[K, V, P]</p>

<pre><code>  with LNodePS[K, V, P] with LNodeInc[K, V] with LNodeNearMap[K, V]
</code></pre>

<p>  trait INodeTD[K, V, P] extends NodeTD[K, V, P]</p>

<pre><code>  with INodePS[K, V, P] with INodeInc[K, V] with INodeNearMap[K, V] {
val lsub: NodeTD[K, V, P]
val rsub: NodeTD[K, V, P]
</code></pre>

<p>  }
}</p>

<p>// ...</p>

<p>sealed trait TDigestMap[K, V, P]
  extends IncrementMapLike[K, V, INodeTD[K, V, P], TDigestMap[K, V, P]]
  with PrefixSumMapLike[K, V, P, INodeTD[K, V, P], TDigestMap[K, V, P]]
  with NearestMapLike[K, V, INodeTD[K, V, P], TDigestMap[K, V, P]] {</p>

<p>  override def toString = // ...
}
```</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[ HTCondor 8.4.0 released! ( September 14, 2015 )]]></title>
      <link href="manual/v8.4.0/10_3Stable_Release.html"/>
      <updated>2015-09-14T05:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[The HTCondor team is pleased to announce the release of HTCondor version 8.4.0.
After a year of development, this is the first release of the new stable series.

This version contains:
a Docker Universe to run a Docker container as an HTCondor job;
the submit file can queue a job for each file found;
the submit file can contain macros;
a dry-run option to condor_submit to test the submit file without any actions;
HTCondor pools can use IPv4 and IPv6 simultaneously;
execute directories can be encrypted upon user or administrator request;
Vanilla Universe jobs can utilize periodic application-level checkpoints;
the administrator can establish job requirements;
numerous scalability changes.

Further details can be found in the
Version History.
HTCondor 8.4.0 binaries and source code are available from our
Downloads page.
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[ HTCondor 8.3.8 released! ( August 27, 2015 )]]></title>
      <link href="manual/v8.3.8/10_3Development_Release.html"/>
      <updated>2015-08-27T05:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[The HTCondor team is pleased to announce the release of HTCondor 8.3.8.
This development series release contains new features that are under
development. This release contains all of the bug fixes from the 8.2.9
stable release.

Enhancements in the release include:
a script to tune Linux kernel parameters for better scalability;
support for python bindings on Windows platforms;
a mechanism to remove Docker images from the local machine.

Static analysis tools were used to identify and fix memory leaks and
other code deficiencies.

Further details can be found in the
Version History.
HTCondor 8.3.8 binaries and source code are available from our
Downloads page.
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Lightweight Non-Negative Numerics for Better Scala Type Signatures]]></title>
      <link href="http://erikerlandson.github.com/blog/2015/08/18/lightweight-non-negative-numerics-for-better-scala-type-signatures/"/>
      <updated>2015-08-19T00:42:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2015/08/18/lightweight-non-negative-numerics-for-better-scala-type-signatures</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>In this post I want to discuss several advantages of defining lightweight non-negative numeric types in Scala, whose primary benefit is that they allow improved type signatures for Scala functions and methods.  I'll first describe the simple class definition, and then demonstrate how it can be used in function signatures and the benefits of doing so. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[The Reservoir Sampling Gap Distribution]]></title>
      <link href="http://erikerlandson.github.com/blog/2015/08/17/the-reservoir-sampling-gap-distribution/"/>
      <updated>2015-08-17T14:35:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2015/08/17/the-reservoir-sampling-gap-distribution</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>In a <a href="http://erikerlandson.github.io/blog/2014/09/11/faster-random-samples-with-gap-sampling/">previous post</a>, I showed that random Bernoulli and Poisson sampling could be made much faster by modeling the <em>sampling gap distribution</em> - that is, directly drawing random samples from the distribution of how many elements would be skipped over between actual samples taken. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Generalizing Kendall's Tau]]></title>
      <link href="http://erikerlandson.github.com/blog/2015/08/14/generalizing-kendalls-tau/"/>
      <updated>2015-08-14T21:35:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2015/08/14/generalizing-kendalls-tau</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>Recently I have been applying <a href="https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient">Kendall's Tau</a> as an evaluation metric to assess how well a regression model ranks input samples, with respect to a known correct ranking. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[ HTCondor 8.2.9 released! ( August 13, 2015 )]]></title>
      <link href="manual/v8.2.9/10_3Stable_Release.html"/>
      <updated>2015-08-13T05:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[The HTCondor team is pleased to announce the release of HTCondor version 8.2.9.
A stable series release contains significant bug and security fixes.
This version contains:
a mechanism for the preemption of dynamic slots, such that the
partitionable slot may use the dynamic slot in the match of a different job;
default configuration bug fixes for the desktop policy, such that
it can both start jobs and monitor the keyboard.

A complete list of fixed bugs can be found in the
Version History.
HTCondor 8.2.9 binaries and source code are available from our
Downloads page.
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[GPUs and adding new resources types to the HTCondor-CE]]></title>
      <link href="http://derekweitzel.blogspot.com/2015/08/gpus-and-adding-new-resources-types-to.html"/>
      <updated>2015-08-07T21:45:00Z</updated>
      <id>tag:blogger.com,1999:blog-3007054864987759910.post-6301136022425730449</id>
      <author>
        <name><![CDATA[Derek Weitzel]]></name>
        <uri>http://derekweitzel.blogspot.com</uri>
      </author>
      <content type="html"><![CDATA[]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[The more things change, the more they stay the same]]></title>
      <link href="http://derekweitzel.blogspot.com/2015/07/the-more-things-change-more-they-stay.html"/>
      <updated>2015-07-28T15:00:00Z</updated>
      <id>tag:blogger.com,1999:blog-3007054864987759910.post-8394942129716842708</id>
      <author>
        <name><![CDATA[Derek Weitzel]]></name>
        <uri>http://derekweitzel.blogspot.com</uri>
      </author>
      <content type="html"><![CDATA[]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[ HTCondor 8.3.7 released! ( July 27, 2015 )]]></title>
      <link href="manual/v8.3.7/10_3Development_Release.html"/>
      <updated>2015-07-27T05:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[The HTCondor team is pleased to announce the release of HTCondor 8.3.7.
This development series release contains new features that are under
development.

Enhancements in the release include:
default configuration settings have been updated to reflect current usage;
the ability to preempt dynamic slots, such that a job may match with a partitionable slot;
the ability to limit the number of jobs per submission and the number of jobs per owner by setting configuration variables.

Further details can be found in the
Version History.
HTCondor 8.3.7 binaries and source code are available from our
Downloads page.
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[fedmsg talk at Spark Summit]]></title>
      <link href="http://chapeau.freevariable.com/2015/06/summit-fedmsg.html"/>
      <updated>2015-06-15T15:05:27Z</updated>
      <id>http://chapeau.freevariable.com/2015/06/summit-fedmsg</id>
      <author>
        <name><![CDATA[William Benton]]></name>
        <uri>http://chapeau.freevariable.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>I&rsquo;m speaking at Spark Summit today about using Spark to analyze operational data from the Fedora project.  Here are some links to further resources related to my talk: [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Using Spark ML Pipeline transformers]]></title>
      <link href="http://chapeau.freevariable.com/2015/06/using-sparks-ml-pipeline-transformers.html"/>
      <updated>2015-06-14T00:07:58Z</updated>
      <id>http://chapeau.freevariable.com/2015/06/using-sparks-ml-pipeline-transformers</id>
      <author>
        <name><![CDATA[William Benton]]></name>
        <uri>http://chapeau.freevariable.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>In this post, we&rsquo;ll see how to make a simple transformer for <a href="https://spark.apache.org/docs/latest/ml-guide.html">Spark ML Pipelines</a>.  The transformer we&rsquo;ll design will generate a sparse binary feature vector from an array-valued field representing a set. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Bokeh plots from Spark]]></title>
      <link href="http://chapeau.freevariable.com/2015/05/bokeh-plots-from-spark.html"/>
      <updated>2015-05-21T16:05:10Z</updated>
      <id>http://chapeau.freevariable.com/2015/05/bokeh-plots-from-spark</id>
      <author>
        <name><![CDATA[William Benton]]></name>
        <uri>http://chapeau.freevariable.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>This post will show you an extremely simple way to make quick-and-dirty <a href="http://bokeh.pydata.org/en/latest/">Bokeh</a> plots from data you&rsquo;ve generated in Spark, but the basic technique is generally applicable to any data that you&rsquo;re generating in some application that doesn&rsquo;t necessarily link in the Bokeh libraries. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Parallel K-Medoids Using Scala ParSeq]]></title>
      <link href="http://erikerlandson.github.com/blog/2015/05/06/parallel-k-medoids-using-scala-parseq/"/>
      <updated>2015-05-06T23:33:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2015/05/06/parallel-k-medoids-using-scala-parseq</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>Scala supplies a <a href="http://docs.scala-lang.org/overviews/parallel-collections/overview.html">parallel collections library</a> that was designed to make it easy for a programmer to add parallel computing over the elements in a collection.  In this post, I will describe a case study of applying Scala's parallel collections to cleanly implement multithreading support for training a K-Medoids clustering model. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[HTCondor CacheD: Caching for HTC - Part 2]]></title>
      <link href="http://derekweitzel.blogspot.com/2015/01/htcondor-cached-caching-for-htc-part-2.html"/>
      <updated>2015-01-25T15:59:00Z</updated>
      <id>tag:blogger.com,1999:blog-3007054864987759910.post-5260378956420164105</id>
      <author>
        <name><![CDATA[Derek Weitzel]]></name>
        <uri>http://derekweitzel.blogspot.com</uri>
      </author>
      <content type="html"><![CDATA[]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Condor CacheD: Caching for HTC - Part 1]]></title>
      <link href="http://derekweitzel.blogspot.com/2015/01/condor-cached-caching-for-htc-part-1.html"/>
      <updated>2015-01-22T16:00:00Z</updated>
      <id>tag:blogger.com,1999:blog-3007054864987759910.post-1889975382858537261</id>
      <author>
        <name><![CDATA[Derek Weitzel]]></name>
        <uri>http://derekweitzel.blogspot.com</uri>
      </author>
      <content type="html"><![CDATA[]]></content>
    </entry>
  
</feed>
