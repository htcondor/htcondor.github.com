<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[Planet HTCondor Meta Feed]]></title>
  <link href="http://htcondor.github.com/planet/atom.xml" rel="self"/>
  <link href="http://htcondor.github.com/"/>
  <updated>2016-02-24T03:42:09-07:00</updated>
  <id>http://htcondor.github.com/planet/atom.xml</id>
  <author>
    <name><![CDATA[HTCondor Project]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
    <entry>
      <title type="html"><![CDATA[ HTCondor helps to find gravitational waves ( February 22, 2016 )]]></title>
      <link href="http://www.opensciencegrid.org/osg-helps-ligo-scientists-confirm-einsteins-last-unproven-theory/"/>
      <updated>2016-02-22T06:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[This  Open Science Grid news article
discusses the role of HTCondor, Pegasus and the Open Science Grid in the
recently-announced discovery of gravitational waves by the Laser
Interferometer Gravitational-Wave Observatory (LIGO).
LIGO used a single HTCondor-based system to run computations across
LIGO Data Grid, OSG and Extreme Science and Engineering Discovery
Environment (XSEDE)-based resources, and consumed 3,956,910 compute
hours on OSG.
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[ HTCondor 8.5.2 released! ( February 18, 2016 )]]></title>
      <link href="manual/v8.5.2/10_2Development_Release.html"/>
      <updated>2016-02-18T06:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[The HTCondor team is pleased to announce the release of HTCondor 8.5.2.
This development series release contains new features that are under
development. This release contains all of the bug fixes from the 8.4.4
stable release.

Enhancements in the release include:
condor_q now defaults to showing only the current user's jobs;
condor_q -batch produces a single line report for a batch of jobs;
Docker Universe jobs now report and update memory and network usage;
immutable and protected job attributes;
improved performance when querying a HTCondor daemon's location;
Added the ability to set ClassAd attributes within the DAG file;
DAGMan now provides event timestamps in dagman.out.

Further details can be found in the
Version History.
HTCondor 8.5.2 binaries and source code are available from our
Downloads page.
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Dimensionality reduction in Spark]]></title>
      <link href="https://chapeau.freevariable.com/2016/02/dimensionality-reduction-in-spark.html"/>
      <updated>2016-02-17T01:23:17Z</updated>
      <id>https://chapeau.freevariable.com/2016/02/dimensionality-reduction-in-spark</id>
      <author>
        <name><![CDATA[William Benton]]></name>
        <uri>https://chapeau.freevariable.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>Here&rsquo;s a quick video I put together introducing infrastructure log processing in Spark.  At the end, there are a couple of nice graphs contrasting PCA and t-SNE for embedding high-dimensional log metadata into two dimensions.</p>

<div class="embed-responsive embed-responsive-16by9"><iframe class="embed-responsive-item" src='https://player.vimeo.com/video/155555333' frameborder='0' webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe></div>


<p><a href="https://vimeo.com/155555333">Log processing demo</a> from <a href="https://vimeo.com/willbenton">William Benton</a> on <a href="https://vimeo.com">Vimeo</a>.</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Efficient Multiplexing for Spark RDDs]]></title>
      <link href="http://erikerlandson.github.com/blog/2016/02/08/efficient-multiplexing-for-spark-rdds/"/>
      <updated>2016-02-08T17:09:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2016/02/08/efficient-multiplexing-for-spark-rdds</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>In this post I'm going to propose a new abstract operation on <a href="http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds">Spark RDDs</a> -- <strong>multiplexing</strong> -- that makes some categories of operations on RDDs both easier to program and in many cases much faster.</p>

<p>My main working example will be the operation of splitting a collection of data elements into N randomly-selected subsamples.  This operation is quite common in machine learning, for the purpose of dividing data into a <a href="https://en.wikipedia.org/wiki/Test_set">training and testing set</a>, or the related task of <a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics">creating folds for cross-validation</a>).</p>

<p>Consider the current standard RDD method for accomplishing this task, <code>randomSplit()</code>.  This method takes a collection of N weights, and returns N output RDDs, each of which contains a randomly-sampled subset of the input, proportional to the corresponding weight.  The <code>randomSplit()</code> method generates the jth output by running a random number generator (RNG) for each input data element and accepting all elements which are in the corresponding jth (normalized) weight range.  As a diagram, the process looks like this at each RDD partition:</p>

<p><img src="/assets/images/mux/randomsplit.png" title="Figure 1" alt="Figure 1" /></p>

<p>The observation I want to draw attention to is that to produce the N output RDDs, it has to run a random sampling over every element in the input <em>for each output</em>.  So if you are splitting into 10 outputs (e.g. for a 10-fold cross-validation), you are re-sampling your input 10 times, the only difference being that each output is created using a different acceptance range for the RNG output.</p>

<p>To see what this looks like in code, consider a simplified version of random splitting that just takes an integer <code>n</code> and always produces (n) equally-weighted outputs:</p>

<p>```scala
def splitSample<a href="rdd:%20RDD[T],%20n:%20Int,%20seed:%20Long%20=%2042">T :ClassTag</a>: Seq[RDD[T]] = {
  Vector.tabulate(n) { j =></p>

<pre><code>rdd.mapPartitions { data =&gt;
  scala.util.Random.setSeed(seed)
  data.filter { unused =&gt; scala.util.Random.nextInt(n) == j }
}
</code></pre>

<p>  }
}
```</p>

<p>(Note that for this method to operate correctly, the RNG seed must be set to the same value each time, or the data will not be correctly partitioned)</p>

<p>While this approach to random splitting works fine, resampling the same data N times is somewhat wasteful.  However, it is possible to re-organize the computation so that the input data is sampled only once.  The idea is to run the RNG once per data element, and save the element into a randomly-chosen collection.  To make this work in the RDD compute model, all N output collections reside in a single row of an <em>intermediate</em> RDD -- a "manifold" RDD.  Each output RDD then takes its data from the corresponding collection in the manifold RDD, as in this diagram:</p>

<p><img src="/assets/images/mux/multiplex.png" alt="Figure 2" /></p>

<p>If you abstract the diagram above into a generalized operation, you end up with methods that might like the following:</p>

<p>```scala
def muxPartitions<a href="n:%20Int,%20f:%20(Int,%20Iterator[T]">U :ClassTag</a> => Seq[U],
  persist: StorageLevel): Seq[RDD[U]] = {
  val mux = self.mapPartitionsWithIndex { case (id, itr) =></p>

<pre><code>Iterator.single(f(id, itr))
</code></pre>

<p>  }.persist(persist)
  Vector.tabulate(n) { j => mux.mapPartitions { itr => Iterator.single(itr.next()(j)) } }
}</p>

<p>def flatMuxPartitions<a href="n:%20Int,%20f:%20(Int,%20Iterator[T]">U :ClassTag</a> => Seq[TraversableOnce[U]],
  persist: StorageLevel): Seq[RDD[U]] = {
  val mux = self.mapPartitionsWithIndex { case (id, itr) =></p>

<pre><code>Iterator.single(f(id, itr))
</code></pre>

<p>  }.persist(persist)
  Vector.tabulate(n) { j => mux.mapPartitions { itr => itr.next()(j).toIterator } }
}
```</p>

<p>Here, the operation of sampling is generalized to any user-supplied function that maps RDD partition data into a sequence of objects that are computed in a single pass, and then multiplexed to the final user-visible outputs.  Note that these functions take a <code>StorageLevel</code> argument that can be used to control the caching level of the internal "manifold" RDD.  This typically defaults to <code>MEMORY_ONLY</code>, so that the computation can be saved and re-used for efficiency.</p>

<p>An efficient split-sampling method based on multiplexing, as described above, might be written using <code>flatMuxPartitions</code> as follows:</p>

<p>```scala
def splitSampleMux<a href="rdd:%20RDD[T],%20n:%20Int,%0A%20%20persist:%20StorageLevel%20=%20MEMORY_ONLY,%0A%20%20seed:%20Long%20=%2042">T :ClassTag</a>: Seq[RDD[T]] =
  rdd.flatMuxPartitions(n, (id: Int, data: Iterator[T]) => {</p>

<pre><code>scala.util.Random.setSeed(id.toLong * seed)
val samples = Vector.fill(n) { scala.collection.mutable.ArrayBuffer.empty[T] }
data.foreach { e =&gt; samples(scala.util.Random.nextInt(n)) += e }
samples
</code></pre>

<p>  }, persist)
```</p>

<p>To test whether multiplexed RDDs actually improve compute efficiency, I collected run-time data at various split values of <code>n</code> (from 1 to 10), for both the non-multiplexing logic (equivalent to the standard <code>randomSplit</code>) and the multiplexed version:</p>

<p><img src="/assets/images/mux/benchmark.png" title="Figure 3" alt="Figure 3" /></p>

<p>As the timing data above show, the computation required to run a non-multiplexed version grows linearly with <code>n</code>, just as predicted.  The multiplexed version, by computing the (n) outputs in a single pass, takes a nearly constant amount of time regardless of how many samples the input is split into.</p>

<p>There are other potential applications for multiplexed RDDs.  Consider the following tuple-based versions of multiplexing:</p>

<p>```scala
def muxPartitions<a href="f:%20(Int,%20Iterator[T]">U1 :ClassTag, U2 :ClassTag</a> => (U1, U2),
  persist: StorageLevel): (RDD[U1], RDD[U2]) = {
  val mux = self.mapPartitionsWithIndex { case (id, itr) =></p>

<pre><code>Iterator.single(f(id, itr))
</code></pre>

<p>  }.persist(persist)
  val mux1 = mux.mapPartitions(itr => Iterator.single(itr.next.<em>1))
  val mux2 = mux.mapPartitions(itr => Iterator.single(itr.next.</em>2))
  (mux1, mux2)
}</p>

<p>def flatMuxPartitions<a href="f:%20(Int,%20Iterator[T]">U1 :ClassTag, U2 :ClassTag</a> => (TraversableOnce[U1], TraversableOnce[U2]),
  persist: StorageLevel): (RDD[U1], RDD[U2]) = {
  val mux = self.mapPartitionsWithIndex { case (id, itr) =></p>

<pre><code>Iterator.single(f(id, itr))
</code></pre>

<p>  }.persist(persist)
  val mux1 = mux.mapPartitions(itr => itr.next.<em>1.toIterator)
  val mux2 = mux.mapPartitions(itr => itr.next.</em>2.toIterator)
  (mux1, mux2)
}
```</p>

<p>Suppose you wanted to run an input-validation filter on some data, sending the data that pass validation into one RDD, and data that failed into a second RDD, paired with information about the error that occurred.  Data validation is a potentially expensive operation.  With multiplexing, you can easily write the filter to operate in a single efficient pass to obtain both the valid stream and the stream of error-data:</p>

<p>```scala
def validate<a href="rdd:%20RDD[T],%20validator:%20T%20=>%20Boolean">T :ClassTag</a> = {
  rdd.flatMuxPartitions((id: Int, data: Iterator[T]) => {</p>

<pre><code>val valid = scala.collection.mutable.ArrayBuffer.empty[T]
val bad = scala.collection.mutable.ArrayBuffer.empty[(T, Exception)]
data.foreach { e =&gt;
  try {
    if (!validator(e)) throw new Exception("returned false")
    valid += e
  } catch {
    case err: Exception =&gt; bad += (e, err)
  }
}
(valid, bad)
</code></pre>

<p>  })
}
```</p>

<p>RDD multiplexing is currently a <a href="https://github.com/willb/silex/pull/50">PR against the silex project</a>.  The code I used to run the timing experiments above is <a href="https://github.com/erikerlandson/silex/blob/blog/muxrdd/src/main/scala/com/redhat/et/silex/sample/split.scala#L90">saved for posterity here</a>.</p>

<p>Happy multiplexing!</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[ HTCondor 8.4.4 released! ( February 2, 2016 )]]></title>
      <link href="manual/v8.4.4/10_3Stable_Release.html"/>
      <updated>2016-02-02T06:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[The HTCondor team is pleased to announce the release of HTCondor 8.4.4.
A stable series release contains significant bug fixes.

Highlights of this release are:
fixed a bug that could cause the collector to crash when DNS lookup fails;
fixed a bug that caused Condor-C jobs with short lease durations to fail;
fixed bugs that affected EC2 grid universe jobs;
fixed a bug that prevented startup if a prior version shared port file exists;
fixed a bug that could cause the condor_shadow to hang on Windows;
a few other bug fixes, consult the version history.

Further details can be found in the
Version History.
HTCondor 8.4.4 binaries and source code are available from our
Downloads page.
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[ Registration open for Barcelona Workshop for HTCondor / ARC CE ( January 28, 2016 )]]></title>
      <link href="https://indico.cern.ch/e/Spring2016HTCondorWorkshop"/>
      <updated>2016-01-28T06:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[*REGISTRATION IS NOW OPEN* (until 22nd Feb.)!! The workshop fee is 80 
EUROS (VAT included), which covers the lunches and all coffee breaks 
along the event. The list of recommended hotels, instructions for fee 
payment, and how to get to the venue is available in the workshop homepage.

Where: Barcelona, Spain, at the ALBA Synchrotron Facility
When: Monday February 29 2016 through Friday March 4 2016.
Workshop homepage: https://indico.cern.ch/e/Spring2016HTCondorWorkshop

]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[ Re-release of RPMs for HTCondor 8.4.3 and 8.5.1 ( January 8, 2016 )]]></title>
      <link href="http://research.cs.wisc.edu/htcondor"/>
      <updated>2016-01-08T06:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[The HTCondor team is re-releasing the RPMs for 8.4.3 and 8.5.1.
A recent change to correct problems with Standard Universe in the
RPM packaging resulted in unoptimized binaries to be packaged.
The new RPMs have optimized binaries.
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Using word2vec on logs]]></title>
      <link href="https://chapeau.freevariable.com/2015/12/using-word2vec-on-log-messages.html"/>
      <updated>2015-12-11T17:51:36Z</updated>
      <id>https://chapeau.freevariable.com/2015/12/using-word2vec-on-log-messages</id>
      <author>
        <name><![CDATA[William Benton]]></name>
        <uri>https://chapeau.freevariable.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>Lately, I&rsquo;ve been experimenting with <a href="http://spark.apache.org/docs/latest/mllib-feature-extraction.html#word2vec">Spark&rsquo;s implementation</a> of word2vec.  Since most of the natural-language data I have sitting around these days are service and system logs from machines at work, I thought it would be fun to see how well word2vec worked if we trained it on the text of log messages.  This is obviously pretty far from an ideal training corpus, but these brief, rich messages seem like they should have some minable content.  In the rest of this post, I&rsquo;ll show some interesting results from the model and also describe some concrete preprocessing steps to get more useful results for extracting words from the odd dialect of natural language that appears in log messages. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[The 'prepare' operation considered harmful in Algebird aggregation]]></title>
      <link href="http://erikerlandson.github.com/blog/2015/11/24/the-prepare-operation-considered-harmful-in-algebird/"/>
      <updated>2015-11-24T23:32:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2015/11/24/the-prepare-operation-considered-harmful-in-algebird</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>I want to make an argument that the Algebird <a href="http://twitter.github.io/algebird/#com.twitter.algebird.Aggregator">Aggregator</a> design, in particular its use of the <code>prepare</code> operation in a map-reduce context, has substantial inefficiencies, compared to an equivalent formulation that is more directly suited to taking advantage of Scala's <a href="http://www.scala-lang.org/api/current/index.html#scala.collection.Seq">aggregate method on collections</a> method. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Very Fast Reservoir Sampling]]></title>
      <link href="http://erikerlandson.github.com/blog/2015/11/20/very-fast-reservoir-sampling/"/>
      <updated>2015-11-20T18:27:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2015/11/20/very-fast-reservoir-sampling</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>In this post I will demonstrate how to do reservoir sampling orders of magnitude faster than the traditional "naive" reservoir sampling algorithm, using a fast high-fidelity approximation to the reservoir sampling-gap distribution. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Concrete advice about abstracts]]></title>
      <link href="https://chapeau.freevariable.com/2015/11/concrete-advice-about-abstracts.html"/>
      <updated>2015-11-16T15:43:49Z</updated>
      <id>https://chapeau.freevariable.com/2015/11/concrete-advice-about-abstracts</id>
      <author>
        <name><![CDATA[William Benton]]></name>
        <uri>https://chapeau.freevariable.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>Consider the following hypothetical conference session abstract: [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Pacing technical talks]]></title>
      <link href="https://chapeau.freevariable.com/2015/10/pacing-technical-talks.html"/>
      <updated>2015-10-21T16:17:01Z</updated>
      <id>https://chapeau.freevariable.com/2015/10/pacing-technical-talks</id>
      <author>
        <name><![CDATA[William Benton]]></name>
        <uri>https://chapeau.freevariable.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>Delivering a technical talk has a lot in common with running a half-marathon or biking a 40k time trial.  You&rsquo;re excited and maybe a little nervous, you&rsquo;re prepared to go relatively hard for a relatively long time, and you&rsquo;re acutely aware of the clock.  In both situations, you might be tempted to take off right from the gun, diving into your hardest effort (or most technical material), but this is a bad strategy. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Notes from Flink Forward]]></title>
      <link href="https://chapeau.freevariable.com/2015/10/notes-from-flink-forward.html"/>
      <updated>2015-10-20T15:48:44Z</updated>
      <id>https://chapeau.freevariable.com/2015/10/notes-from-flink-forward</id>
      <author>
        <name><![CDATA[William Benton]]></name>
        <uri>https://chapeau.freevariable.com/</uri>
      </author>
      <content type="html"><![CDATA[<p><a data-flickr-embed="true"  href="https://www.flickr.com/photos/willb/22238437291/in/dateposted-public/" title="Brandenburger Tor lightshow"><img src="https://farm1.staticflickr.com/739/22238437291_22636e4a72_b.jpg" width="1024" height="819" alt="Brandenburger Tor lightshow"></a><script async src="//embedr.flickr.com/assets/client-code.js" charset="utf-8"></script> [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[A Library of Binary Tree Algorithms as Mixable Scala Traits]]></title>
      <link href="http://erikerlandson.github.com/blog/2015/09/26/a-library-of-binary-tree-algorithms-as-mixable-scala-traits/"/>
      <updated>2015-09-26T19:43:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2015/09/26/a-library-of-binary-tree-algorithms-as-mixable-scala-traits</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>In this post I am going to describe some work I've done recently on a system of Scala traits that support tree-based collection algorithms prefix-sum, nearest key query and value increment in a mixable format, all backed by Red-Black balanced tree logic, which is also a fully inheritable trait. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Lightweight Non-Negative Numerics for Better Scala Type Signatures]]></title>
      <link href="http://erikerlandson.github.com/blog/2015/08/18/lightweight-non-negative-numerics-for-better-scala-type-signatures/"/>
      <updated>2015-08-19T00:42:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2015/08/18/lightweight-non-negative-numerics-for-better-scala-type-signatures</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>In this post I want to discuss several advantages of defining lightweight non-negative numeric types in Scala, whose primary benefit is that they allow improved type signatures for Scala functions and methods.  I'll first describe the simple class definition, and then demonstrate how it can be used in function signatures and the benefits of doing so. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[GPUs and adding new resources types to the HTCondor-CE]]></title>
      <link href="http://derekweitzel.blogspot.com/2015/08/gpus-and-adding-new-resources-types-to.html"/>
      <updated>2015-08-07T21:45:00Z</updated>
      <id>tag:blogger.com,1999:blog-3007054864987759910.post-6301136022425730449</id>
      <author>
        <name><![CDATA[Derek Weitzel]]></name>
        <uri>http://derekweitzel.blogspot.com</uri>
      </author>
      <content type="html"><![CDATA[]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[The more things change, the more they stay the same]]></title>
      <link href="http://derekweitzel.blogspot.com/2015/07/the-more-things-change-more-they-stay.html"/>
      <updated>2015-07-28T15:00:00Z</updated>
      <id>tag:blogger.com,1999:blog-3007054864987759910.post-8394942129716842708</id>
      <author>
        <name><![CDATA[Derek Weitzel]]></name>
        <uri>http://derekweitzel.blogspot.com</uri>
      </author>
      <content type="html"><![CDATA[]]></content>
    </entry>
  
</feed>
