<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[Planet HTCondor Meta Feed]]></title>
  <link href="http://htcondor.github.com/planet/atom.xml" rel="self"/>
  <link href="http://htcondor.github.com/"/>
  <updated>2015-12-14T03:22:27-07:00</updated>
  <id>http://htcondor.github.com/planet/atom.xml</id>
  <author>
    <name><![CDATA[HTCondor Project]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
    <entry>
      <title type="html"><![CDATA[Using word2vec on logs]]></title>
      <link href="http://chapeau.freevariable.com/2015/12/using-word2vec-on-log-messages.html"/>
      <updated>2015-12-01T17:51:36Z</updated>
      <id>http://chapeau.freevariable.com/2015/12/using-word2vec-on-log-messages</id>
      <author>
        <name><![CDATA[William Benton]]></name>
        <uri>http://chapeau.freevariable.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>Lately, I&rsquo;ve been experimenting with <a href="http://spark.apache.org/docs/latest/mllib-feature-extraction.html#word2vec">Spark&rsquo;s implementation</a> of word2vec.  Since most of the natural-language data I have sitting around these days are service and system logs from machines at work, I thought it would be fun to see how well word2vec worked if we trained it on the text of log messages.  This is obviously pretty far from an ideal training corpus, but these brief, rich messages seem like they should have some minable content.  In the rest of this post, I&rsquo;ll show some interesting results from the model and also describe some concrete preprocessing steps to get more useful results for extracting words from the odd dialect of natural language that appears in log messages.</p>

<h3>Background</h3>

<p><a href="http://arxiv.org/abs/1310.4546">word2vec</a> is a family of techniques for encoding words as relatively low-dimensional vectors that capture interesting semantic information.  That is, words that are synonyms are likely to have vectors that are similar (by cosine similarity).  Another really neat aspect of this encoding is that linear transformations of these vectors can expose semantic information like analogies:  for example, given a model trained on news articles, adding the vectors for &ldquo;Madrid&rdquo; and &ldquo;France&rdquo; and subtracting the vector for &ldquo;Spain&rdquo; results in a vector very close to that for &ldquo;Paris.&rdquo;</p>

<p>Spark&rsquo;s implementation of word2vec uses <a href="https://en.wikipedia.org/wiki/N-gram#Bias-versus-variance_trade-off">skip-grams</a>, so the training objective is to produce a model that, given a word, predicts the context in which it is likely to appear.</p>

<h3>Preliminaries</h3>

<p>Like the <a href="">original implementation of word2vec</a>, Spark&rsquo;s implementation uses a window of &#177;5 surrounding words (this is not user-configurable) and defaults to discarding all words that appear fewer than 5 times (this threshold is user-configurable).  Both of these assumptions seem sane for the sort of training &ldquo;sentences&rdquo; that appear in log messages, but they won&rsquo;t be sufficient.</p>

<p>Spark doesn&rsquo;t provide a lot of tools for tokenizing and preprocessing natural-language text.<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>  Simple string splitting is as ubiquitous in trivial language processing examples just as it is in trivial word count examples, but it&rsquo;s not going to give us the best results. Fortunately, there are some minimal steps we can take to start getting useful tokens out of log messages.  We&rsquo;ll look at these steps and what see what motivates them now.</p>

<h4>What is a word?</h4>

<p>Let&rsquo;s first consider what kinds of tokens might be interesting for analyzing the content of log messages.  At the very least, we might care about:</p>

<ol>
<li>dictionary words,</li>
<li>trademarks (which may or may not be dictionary words),</li>
<li>technical jargon terms (which may or may not be dictionary words),</li>
<li>service names (which may or may not be dictionary words),</li>
<li>symbolic constant names (e.g., <code>ENOENT</code> and <code>OPEN_MAX</code>),</li>
<li>pathnames (e.g., <code>/dev/null</code>), and</li>
<li>programming-language identifiers (e.g., <code>OutOfMemoryError</code> and <code>Kernel::exec</code>).</li>
</ol>


<p>For this application, we&rsquo;re less interested in the following kinds of tokens, although it is possible to imagine other applications in which they might be important:</p>

<ol>
<li>hostnames,</li>
<li>IPv4 and IPv6 addresses,</li>
<li>MAC addresses,</li>
<li>dates and times, and</li>
<li>hex hash digests.</li>
</ol>


<h4>Preprocessing steps</h4>

<p>If we&rsquo;re going to convert sequences of lines to sequences of sequences of tokens, we&rsquo;ll eventually be splitting strings.  Before we split, we&rsquo;ll collapse all runs of whitespace into single spaces so that we get more useful results when we do split.  This isn&rsquo;t strictly necessary &mdash; we could elect to split on runs of whitespace instead of single whitespace characters, or we could filter out empty strings from word sequences before training on them.  But this makes for cleaner input and it makes the subsequent transformations a little simpler.</p>

<p>Here&rsquo;s Scala code to collapse runs of whitespace into a single space:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">def</span> <span class="n">replace</span><span class="o">(</span><span class="n">r</span><span class="k">:</span> <span class="kt">scala.util.matching.Regex</span><span class="o">,</span> <span class="n">s</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span> <span class="k">=</span> <span class="o">{</span> <span class="k">_</span> <span class="k">=&gt;</span> <span class="n">r</span><span class="o">.</span><span class="n">replaceAllIn</span><span class="o">(</span><span class="k">_</span><span class="o">,</span> <span class="n">s</span><span class="o">)</span> <span class="o">}</span>
</span><span class='line'><span class="k">val</span> <span class="n">collapseSpaces</span> <span class="k">=</span> <span class="n">replace</span><span class="o">(</span><span class="k">new</span> <span class="n">scala</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">matching</span><span class="o">.</span><span class="nc">Regex</span><span class="o">(</span><span class="s">&quot;[\\s]+&quot;</span><span class="o">),</span> <span class="s">&quot; &quot;</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>The next thing we&rsquo;ll want to do is eliminate all punctuation from the ends of each word.  An appropriate definition of &ldquo;punctuation&rdquo; will depend on the sorts of tokens we wind up deciding are interesting, but I considered punctuation characters to be anything except:</p>

<ol>
<li>alphanumeric characters,</li>
<li>dashes, and</li>
<li>underscores.</li>
</ol>


<p>Whether or not we want to retain intratoken punctuation depends on the application; there are good arguments to be made for retaining colons and periods (MAC addresses, programming-language identifiers in stack traces, hostnames, etc.), slashes (paths), at-signs (email addresses), and other marks as well.  I&rsquo;ll be retaining these marks but stripping all others.  After these transformations, we can split on whitespace and get a relatively sensible set of tokens.</p>

<p>Here&rsquo;s Scala code to strip punctuation from lines:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">val</span> <span class="n">rejectedIntratokenPunctuation</span> <span class="k">=</span> <span class="k">new</span> <span class="n">scala</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">matching</span><span class="o">.</span><span class="nc">Regex</span><span class="o">(</span><span class="s">&quot;[^A-Za-z0-9-_./:@]&quot;</span><span class="o">)</span>
</span><span class='line'><span class="k">val</span> <span class="n">leadingPunctuation</span> <span class="k">=</span> <span class="k">new</span> <span class="n">scala</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">matching</span><span class="o">.</span><span class="nc">Regex</span><span class="o">(</span><span class="s">&quot;\\s[^A-Za-z0-9-_/]+&quot;</span><span class="o">)</span>
</span><span class='line'><span class="k">val</span> <span class="n">trailingPunctuation</span> <span class="k">=</span> <span class="k">new</span> <span class="n">scala</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">matching</span><span class="o">.</span><span class="nc">Regex</span><span class="o">(</span><span class="s">&quot;[^A-Za-z0-9-_/]+\\s&quot;</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'><span class="k">val</span> <span class="n">stripPunctuation</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=&gt;</span> <span class="nc">String</span> <span class="k">=</span>
</span><span class='line'>  <span class="n">replace</span><span class="o">(</span><span class="n">leadingPunctuation</span><span class="o">,</span> <span class="s">&quot; &quot;</span><span class="o">)</span> <span class="n">compose</span>
</span><span class='line'>  <span class="n">replace</span><span class="o">(</span><span class="n">trailingPunctuation</span><span class="o">,</span> <span class="s">&quot; &quot;</span><span class="o">)</span> <span class="n">compose</span>
</span><span class='line'>  <span class="n">replace</span><span class="o">(</span><span class="n">rejectedIntratokenPunctuation</span><span class="o">,</span> <span class="s">&quot;&quot;</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>In order to filter out strings of numbers, we&rsquo;ll reject all tokens that don&rsquo;t contain at least one letter.  (We could be stricter and reject all tokens that don&rsquo;t contain at least one letter that isn&rsquo;t a hex digit, but I decided to be permissive in order to avoid rejecting interesting words that only contain letters <code>A-F</code>.)</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">val</span> <span class="n">oneletter</span> <span class="k">=</span> <span class="k">new</span> <span class="n">scala</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">matching</span><span class="o">.</span><span class="nc">Regex</span><span class="o">(</span><span class="s">&quot;.*([A-Za-z]).*&quot;</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>Here&rsquo;s what our preprocessing pipeline looks like, assuming an RDD of log messages called <code>messages</code>:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">def</span> <span class="n">tokens</span><span class="o">(</span><span class="n">s</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">post</span><span class="k">:</span> <span class="kt">String</span><span class="o">=&gt;</span><span class="nc">String</span> <span class="k">=</span> <span class="n">identity</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span><span class="k">:</span> <span class="kt">Seq</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span>
</span><span class='line'>  <span class="n">collapseWhitespace</span><span class="o">(</span><span class="n">s</span><span class="o">)</span>
</span><span class='line'>    <span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&quot; &quot;</span><span class="o">)</span>
</span><span class='line'>    <span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">s</span> <span class="k">=&gt;</span> <span class="n">post</span><span class="o">(</span><span class="n">stripPunctuation</span><span class="o">(</span><span class="n">s</span><span class="o">)))</span>
</span><span class='line'>    <span class="o">.</span><span class="n">collect</span> <span class="o">{</span> <span class="k">case</span> <span class="n">token</span> <span class="k">@</span> <span class="n">oneletter</span><span class="o">(</span><span class="k">_</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">token</span> <span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="k">val</span> <span class="n">tokenSeqs</span> <span class="k">=</span> <span class="n">messages</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">line</span> <span class="k">=&gt;</span> <span class="n">tokens</span><span class="o">(</span><span class="n">line</span><span class="o">))</span>
</span></code></pre></td></tr></table></div></figure>


<p>Now we have a sequence of words for each log message and are ready to train a word2vec model.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">import</span> <span class="nn">org.apache.spark.mllib.feature.Word2Vec</span>
</span><span class='line'><span class="k">val</span> <span class="n">w2v</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Word2Vec</span>
</span><span class='line'>
</span><span class='line'><span class="k">val</span> <span class="n">model</span> <span class="k">=</span> <span class="n">w2v</span><span class="o">.</span><span class="n">fit</span><span class="o">(</span><span class="n">tokenSeqs</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>Note that there are a few things we could be doing in our preprocessing pipeline but aren&rsquo;t, like using a whitelist (for dictionary words or service names), or rejecting stopwords.  This approach is pretty basic, but it produces some interesting results in any case.</p>

<h3>Results and conclusions</h3>

<p>I evaluated the model by using it to find synonyms for (more or less) arbitrary words that appeared in log messages.  Recall that word2vec basically models words by the contexts in which they might appear; informally, synonyms are thus words with similar contexts.</p>

<ul>
<li>The top synonyms for <code>nova</code> (the OpenStack compute service) included <code>vm</code>, <a href="http://docs.openstack.org/developer/glance/"><code>glance</code></a>, <code>containers</code>, <code>instances</code>, and <code>images</code> &mdash; all of these are related to running OpenStack compute jobs.</li>
<li>The top synonyms for <code>volume</code> included <code>update</code>, <a href="https://wiki.openstack.org/wiki/Cinder"><code>cinder.scheduler.host_manager</code></a>, and several UUIDs for actual volumes.</li>
<li>The top synonyms for <code>tmpfs</code> included <code>type</code>, <code>dev</code>, <code>uses</code>, <code>initialized</code>, and <code>transition</code>.</li>
<li>The top synonyms for <code>sh</code> included <code>/usr/bin/bash</code>, <code>_AUDIT_SESSION</code>, <code>NetworkManager</code>, <code>_SYSTEMD_SESSION</code>, <code>postfixqmgr</code>.</li>
<li>The top synonyms for <code>password</code> included <code>publickey</code>, <code>Accepted</code>, <code>opened</code>, <a href="https://en.wikipedia.org/wiki/Intelligent_Platform_Management_Interface"><code>IPMI</code></a>, and the name of an internal project.</li>
</ul>


<p>These results aren&rsquo;t earth-shattering &mdash; indeed, they won&rsquo;t even tell you <a href="https://scholar.google.com/scholar?q=yelp+sentiment+analysis&amp;hl=en&amp;as_sdt=0&amp;as_vis=1&amp;oi=scholart&amp;sa=X&amp;ved=0ahUKEwj74vLpl9TJAhXLXh4KHccICIgQgQMIGzAA">where to get a decent burrito</a> &mdash;  but they&rsquo;re interesting, they&rsquo;re sensible, and they point to the effectiveness of word2vec even given a limited, unidiomatic corpus full of odd word-like tokens.  Of course, one can imagine ways to make our preprocessing more robust.  Similarly, there are certainly other ways to generate a training corpus for these words: perhaps using the set of all messages for a particular service and severity as a training sentence,  using the documentation for the services involved, using format strings present in the source or binaries for the services themselves, or some combination of these.</p>

<p>Semantic modeling of terms in log messages is obviously useful for log analytics:  it can be used as part of a pipeline to classify related log messages by topic, in feature engineering for anomaly detection, and to suggest alternate search terms for interactive queries.  However, it is a pleasant surprise that we can train a competent word2vec model for understanding log messages from the uncharacteristic utterances that comprise log messages themselves.</p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p>Spark does provide a <a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.StopWordsRemover">stopword filter for English</a> and there are external libraries to fill in some of its language-processing gaps.  In particular, I&rsquo;ve had good luck with the <a href="http://tartarus.org/~martin/PorterStemmer/">Porter stemmer</a> implementation from <a href="https://github.com/scalanlp/chalk/">Chalk</a>.<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[The 'prepare' operation considered harmful in Algebird aggregation]]></title>
      <link href="http://erikerlandson.github.com/blog/2015/11/24/the-prepare-operation-considered-harmful-in-algebird/"/>
      <updated>2015-11-24T23:32:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2015/11/24/the-prepare-operation-considered-harmful-in-algebird</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>I want to make an argument that the Algebird <a href="http://twitter.github.io/algebird/#com.twitter.algebird.Aggregator">Aggregator</a> design, in particular its use of the <code>prepare</code> operation in a map-reduce context, has substantial inefficiencies, compared to an equivalent formulation that is more directly suited to taking advantage of Scala's <a href="http://www.scala-lang.org/api/current/index.html#scala.collection.Seq">aggregate method on collections</a> method.</p>

<p>Consider the definition of aggregation in the Aggregator class:</p>

<p><code>scala
def apply(inputs: TraversableOnce[A]): C = present(reduce(inputs.map(prepare)))
</code></p>

<p>You can see that it is a standard map/reduce operation, where <code>reduce</code> is defined as a monoidal (or semigroup -- more on this later) operation. Under the hood, it boils down to an invocation of Scala's <code>reduceLeft</code> method.  The key thing to notice is that the role of <code>prepare</code> is to map a collection of data elements into the required monoids, which are then aggregated using that monoid's <code>plus</code> operation.  In other words, <code>prepare</code> converts data elements into "singleton" monoids each representing a data element.</p>

<p>Now, if the monoid in question is simple, say some numeric type, this conversion is free, or nearly so.  For example, the conversion of an integer into the "integer monoid" is a no-op.  However, there are other kinds of "non-trivial" monoids, for which the conversion of a data element into its corresponding monoid may be costly.  In this post, I will be using the monoid defined by Scala Set[Int], where the monoid <code>plus</code> operation is set union, and of course the <code>zero</code> element is the empty set.</p>

<p>Consider the process of defining an Algebird aggregator for the task of generating the set of unique elements in a data set.  The corresponding <code>prepare</code> operation is: <code>prepare(e: Int) = Set(e)</code>.  A monoid trait that encodes this idea might look like the following.  (the code I used in this post can be found <a href="https://gist.github.com/erikerlandson/d96dc553bc51e0eb5e4b">here</a>)</p>

<p>```scala
// an algebird-like monoid with the 'prepare' operation
trait PreparedMonoid[M, E] {
  val zero: M
  def plus(m1: M, m2: M): M
  def prepare(e: E): M
}</p>

<p>// a PreparedMonoid for a set of integers.  monoid operator is set union.
object intSetPrepared extends PreparedMonoid[Set[Int], Int] {
  val zero = Set.empty[Int]
  def plus(m1: Set[Int], m2: Set[Int]) = m1 ++ m2
  def prepare(e: Int) = Set(e)
}</p>

<p>implicit class SeqWithMapReduce<a href="seq:%20Seq[E]">E</a> {
  // algebird map/reduce Aggregator model
  def mrPrepared<a href="mon:%20PreparedMonoid[M,%20E]">M</a>: M = {</p>

<pre><code>seq.map(mon.prepare).reduceLeft(mon.plus)
</code></pre>

<p>  }
}
```</p>

<p>If we unpack the above code, as applied to <code>intSetPrepared</code>, we are instantiating a new Set object, containing a single value, for every single input data element.</p>

<p>But there is a potentially better model of aggregation, exemplified by the Scala <code>aggregate</code> method.  This method does not use a <code>prepare</code> operation.  It uses a zero value and a monoidal operator, which the Scala docs refer to as <code>combop</code>, but it also uses an "update" operation, that defines how to update the monoid object, directly, with a single element, referred to as <code>seqop</code> in Scala's documentation.  This idea can also be encoded as a flavor of monoid, enhanced with an <code>update</code> method:</p>

<p>```scala
// an algebird-like monoid with 'update' operation
trait UpdatedMonoid[M, E] {
  val zero: M
  def plus(m1: M, m2: M): M
  def update(m: M, e: E): M
}</p>

<p>// an equivalent UpdatedMonoid for a set of integers
object intSetUpdated extends UpdatedMonoid[Set[Int], Int] {
  val zero = Set.empty[Int]
  def plus(m1: Set[Int], m2: Set[Int]) = m1 ++ m2
  def update(m: Set[Int], e: Int) = m + e
}</p>

<p>implicit class SeqWithMapReduceUpdated<a href="seq:%20Seq[E]">E</a> {
  // map/reduce logic, taking advantage of scala 'aggregate'
  def mrUpdatedAggregate<a href="mon:%20UpdatedMonoid[M,%20E]">M</a>: M = {</p>

<pre><code>seq.aggregate(mon.zero)(mon.update, mon.plus)
</code></pre>

<p>  }
}
```</p>

<p>This arrangement promises more efficiency when aggregating w.r.t. nontrivial monoids, by avoiding the construction of "singleton" monoids for each data element.  The following demo confirms that for the Set-based monoid, it is over 10 times faster:</p>

<p>```scala
scala> :load /home/eje/scala/prepare.scala
Loading /home/eje/scala/prepare.scala...
defined module prepare</p>

<p>scala> import prepare.<em>
import prepare.</em></p>

<p>scala> val data = Vector.fill(1000000) { scala.util.Random.nextInt(10) }
data: scala.collection.immutable.Vector[Int] = Vector(7, 9, 4, 2, 7,...</p>

<p>// Verify that output is the same for both implementations:
scala> data.mrPrepared(intSetPrepared)
res0: Set[Int] = Set(0, 5, 1, 6, 9, 2, 7, 3, 8, 4)</p>

<p>// results are the same
scala> data.mrUpdatedAggregate(intSetUpdated)
res1: Set[Int] = Set(0, 5, 1, 6, 9, 2, 7, 3, 8, 4)</p>

<p>// Compare timings of prepare-based versus update-based aggregation
// (benchmark values are returned in seconds)
scala> benchmark(10) { data.mrPrepared(intSetPrepared) }
res2: Double = 0.2957673056</p>

<p>// update-based aggregation is 10 times faster
scala> benchmark(10) { data.mrUpdatedAggregate(intSetUpdated) }
res3: Double = 0.027041249300000004
```</p>

<p>It is also possible to apply Scala's <code>aggregate</code> to a monoid enhanced with <code>prepare</code>:</p>

<p>```scala
implicit class SeqWithMapReducePrepared<a href="seq:%20Seq[E]">E</a> {
  // using 'aggregate' with prepared op
  def mrPreparedAggregate<a href="mon:%20PreparedMonoid[M,%20E]">M</a>: M = {</p>

<pre><code>seq.aggregate(mon.zero)((m, e) =&gt; mon.plus(m, mon.prepare(e)), mon.plus)
</code></pre>

<p>  }
}
```</p>

<p>Although this turns out to be measurably faster than the literal map-reduce implementation, it is still not nearly as fast as the variation using <code>update</code>:</p>

<p><code>scala
scala&gt; benchmark(10) { data.mrPreparedAggregate(intSetPrepared) }
res2: Double = 0.1754636707
</code></p>

<p>Readers familiar with Algebird may be wondering about my use of monoids above, when the <code>Aggregator</code> interface is actually based on semigroups.  This is important, since building on Scala's <code>aggregate</code> function requires a zero element that semigroups do not have.  Although I believe it might be worth considering changing <code>Aggregator</code> to use monoids, another sensible option is to change the internal logic for the subclass <code>AggregatorMonoid</code>, which does require a monoid, or possibly just define a new <code>AggregatorMonoidUpdated</code> subclass.</p>

<p>A final note on compatability: note that any monoid enhanced with <code>prepare</code> can be converted into an equivalent monoid enhanced with <code>update</code>, as demonstrated by this factory function:</p>

<p>```scala
object UpdatedMonoid {
  // create an UpdatedMonoid from a PreparedMonoid
  def apply<a href="mon:%20PreparedMonoid[M,%20E]">M, E</a> = new UpdatedMonoid[M, E] {</p>

<pre><code>val zero = mon.zero
def plus(m1: M, m2: M) = mon.plus(m1, m2)
def update(m: M, e: E) = mon.plus(m, mon.prepare(e))
</code></pre>

<p>  }
}
```</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Very Fast Reservoir Sampling]]></title>
      <link href="http://erikerlandson.github.com/blog/2015/11/20/very-fast-reservoir-sampling/"/>
      <updated>2015-11-20T18:27:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2015/11/20/very-fast-reservoir-sampling</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>In this post I will demonstrate how to do reservoir sampling orders of magnitude faster than the traditional "naive" reservoir sampling algorithm, using a fast high-fidelity approximation to the reservoir sampling-gap distribution.</p>

<blockquote><p>The code I used to collect the data for this post can be viewed <a href="https://github.com/erikerlandson/silex/blob/blog/reservoir/src/main/scala/com/redhat/et/silex/sample/reservoir/reservoir.scala">here</a>.  I generated the plots using the <a href="https://github.com/quantifind/wisp">quantifind WISP</a> project.</p></blockquote>

<p>In a <a href="http://erikerlandson.github.io/blog/2014/09/11/faster-random-samples-with-gap-sampling/">previous post</a>, I showed that random Bernoulli and Poisson sampling could be made much faster by modeling the <em>sampling gap distribution</em> for the corresponding sampling distributions.  More recently, I also began exploring whether <a href="https://en.wikipedia.org/wiki/Reservoir_sampling">reservoir sampling</a> might also be optimized using the gap sampling technique, by deriving the <a href="http://erikerlandson.github.io/blog/2015/08/17/the-reservoir-sampling-gap-distribution/">reservoir sampling gap distribution</a>.  For a sampling reservoir of size (R), starting at data element (j), the probability distribution of the sampling gap is:</p>

<p><img src="/assets/images/reservoir1/figure6.png" title="Figure 1" alt="Figure 1" /></p>

<p>Modeling a sampling gap distribution is a powerful tool for optimizing a sampling algorithm, but it presupposes that you can actually draw values from that distribution substantially faster than just applying a random process to drawing each data element.  I was unable to come up with a "direct" algorithm for drawing samples from P(k) above (I suspect none exists), however I also know the CDF F(k), so it <em>is</em> possible to apply <a href="https://en.wikipedia.org/wiki/Inverse_transform_sampling">inversion sampling</a>, which runs in logarithmic time w.r.t the desired accuracy.  Although its logarithmic cost effectively guarantees that it will be a net efficiency win for sufficiently large (j), it still involves a substantial number of computations to yield its samples, and it seems unlikely to be competitive with straight "naive" reservoir sampling over many real-world data sizes, where (j) may never grow very large.</p>

<p>Well, if exact computations are too expensive, we can always look for a fast approximation.  Consider the original "first principles" formula for the sampling gap P(k):</p>

<p><img src="/assets/images/reservoir2/figure2.png" title="Figure 2" alt="Figure 2" /></p>

<p>As the figure above alludes to, if (j) is relatively large compared to (k), then values (j+1),(j+2)...(j+k) are all going to be effectively "close" to (j), and so we can replace them all with (j) as an approximation.  Note that the resulting approximation is just the PMF of the <a href="https://en.wikipedia.org/wiki/Geometric_distribution">geometric distribution</a>, with probability of success p=(R/j), and we already saw how to efficiently draw values from a geometric distribution from our experience with Bernoulli sampling.</p>

<p>Do we have any reason to hope that this approximation will be useful?  For reasons that are similar to those for Bernoulli gap sampling, it will only be efficient to employ gap sampling when the probability (R/j) becomes small enough.  From our experiences with Bernoulli sampling that is <em>at least</em> j>=2R.  So, we have some assurance that (j) itself will be never be <em>very</em> small.  What about (k)?  Note that a geometric distribution "favors" smaller values of (k) -- that is, small values of (k) have the highest probabilities.  In fact, the smaller that (j) is, the larger the probability (R/j) is, and so the more likely that (k) values that are small relative to (j) will be the frequent ones.  It is also promising that the true distribution for P(k) <em>also</em> favors smaller values of (k) (in fact it favors them even a bit more strongly than the approximation).</p>

<p>Although it is encouraging, it is also clear that my argument above is limited to heuristic hand-waving.  What does this approximation really <em>look</em> like, compared to the true distribution?  Fortunately, it is easy to plot both distributions numerically, since we now know the formulas for both:</p>

<p><img src="/assets/images/reservoir2/CDFs_R=10.png" title="Figure 3" alt="Figure 3" /></p>

<p>The plot above shows that, in fact, the geometric approximation is a <em>surprisingly good</em> approximation to the true distribution!  Furthermore, the approximation remains good as both (j) and (k) grow larger.</p>

<p>Our numeric eye-balling looks quite promising.  Is there an effective way to <em>measure</em> how good this approximation is?  One useful measure is the <a href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test">Kolmogorov-Smirnov D statistic</a>, which is just the maximum absolute error between two cumulative distributions.  Here is a plot of the D statistic for reservoir size R=10, as (j) varies across several magnitudes:</p>

<p><img src="/assets/images/reservoir2/R=10.png" title="Figure 4" alt="Figure 4" /></p>

<p>This plot is also good news: we can see that deviation, as measured by D, remains bounded at a small value (less than 0.0262).  As this is for the specific value R=10, we also want to know how things change as reservoir size changes:</p>

<p><img src="/assets/images/reservoir2/R=all.png" title="Figure 5" alt="Figure 5" /></p>

<p>The news is still good!  As reservoir size grows, the approximation only gets better: the D values get smaller as R increases, and remain asymptotically bounded as (j) increases.</p>

<p>Now we have some numeric assurance that the geometric approximation is a good one, and stays good as reservoir size grows and sampling runs get longer.  However, we should also verify that an actual implementation of the approximation works as expected.</p>

<p>Here is pseudocode for an implementation of reservoir sampling using the fast geometric approximation:</p>

<pre><code>// data is array to sample from
// R is the reservoir size
function reservoirFast(data: Array, R: Int) {
  n = data.length
  // Initialize reservoir with first R elements of data:
  res = data[0 until R]
  // Until this threshold, use traditional sampling.  This value may
  // depend on performance characteristics of random number generation and/or
  // numeric libraries:
  t = 4 * R
  j = 1 + R
  while (j &lt; n  &amp;&amp;  j &lt;= t) {
    k = randomInt(j) // random integer &gt;= 0 and &lt; j
    if (k &lt; R) res[k] = data[j]
    j = j + 1
  }
  // Once gaps become significant, it pays to do gap sampling
  while (j &lt; n) {
    // draw gap size (g) from geometric distribution with probability p = R/j
    p = R / j
    u = randomFloat() // random float &gt; 0 and &lt;= 1
    g = floor(log(u) / log(1-p))
    j = j + g
    if (j &lt; n) {
      k = randomInt(R)
      res[k] = data[j]
    }
    j = j + 1
  }
  // return the reservoir
  return res
}
</code></pre>

<p>Following is a plot that shows two-sample D statistics, comparing the distribution in sample gaps between runs of the exact "naive" reservoir sampling with the fast geometric approximation:</p>

<p><img src="/assets/images/reservoir2/D_naive_vs_fast.png" title="Figure 6" alt="Figure 6" /></p>

<p>As expected, the measured difference in sampling characteristics between naive and fast approximation are small, confirming the numeric predictions.</p>

<p>Since the point of this exercise was to achieve faster random sampling, it remains to measure what kind of speed improvements the fast approximation provides.  As a point of reference, here is a plot of run times for reservoir sampling over 10<sup>8</sup> integers:</p>

<p><img src="/assets/images/reservoir2/naive_sample_time_vs_R.png" title="Figure 7" alt="Figure 7" /></p>

<p>As expected, sample time remains constant at around 1.5 seconds, regardless of reservoir size, since the naive algorithm always samples from its RNG per each sample.</p>

<p>Compare this to the corresponding plot for the fast geometric approximation:</p>

<p><img src="/assets/images/reservoir2/gap_sample_times_vs_R.png" title="Figure 8" alt="Figure 8" /></p>

<p>Firstly, we see that the sampling times are <em>much faster</em>, as originally anticipated in my <a href="http://erikerlandson.github.io/blog/2015/08/17/the-reservoir-sampling-gap-distribution/">previous post</a> -- in the neighborhood of 3 orders of magnitude faster.  Secondly, we see that the sampling times do increase as a linear function of reservoir size.  Based on our experience with Bernoulli gap sampling, this is expected; the sampling probabilities are given by (R/j), and therefore the amount of sampling is proportional to R.</p>

<p>Another property anticipated in my previous post was that the efficiency of gap sampling should continue to increase as the amount of data sampled grows; the sampling probability being (R/j), the probability of sampling decreases as j gets larger, and so the corresponding gap sizes grow.  The following plot verifies this property, holding reservoir size R constant, and increasing the data size:</p>

<p><img src="/assets/images/reservoir2/gap_sampling_efficiency.png" title="Figure 9" alt="Figure 9" /></p>

<p>The sampling time (per million elements) decreases as the sample size grows, as predicted by the formula.</p>

<p>In conclusion, I have demonstrated that a geometric distribution can be used as a high quality approximation to the true sampling gap distribution for reservoir sampling, which allows reservoir sampling to be performed much faster than the naive algorithm while still retaining sampling quality.</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[ HTCondor 8.4.2 released! ( November 17, 2015 )]]></title>
      <link href="manual/v8.4.2/10_3Stable_Release.html"/>
      <updated>2015-11-17T06:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[The HTCondor team is pleased to announce the release of HTCondor 8.4.2.
A stable series release contains significant bug fixes.

Highlights of this release are:
a bug fix to prevent the condor_schedd from crashing;
a bug fix to honor TCP_FORWARDING_HOST;
Standard Universe works properly in RPM installations of HTCondor;
the RPM packages no longer claim to provide Globus libraries;
bug fixes to DAGMan's "maximum idle jobs" throttle;
several other bug fixes, consult the version history.


Further details can be found in the
Version History.
HTCondor 8.4.2 binaries and source code are available from our
Downloads page.
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Concrete advice about abstracts]]></title>
      <link href="http://chapeau.freevariable.com/2015/11/concrete-advice-about-abstracts.html"/>
      <updated>2015-11-16T15:43:49Z</updated>
      <id>http://chapeau.freevariable.com/2015/11/concrete-advice-about-abstracts</id>
      <author>
        <name><![CDATA[William Benton]]></name>
        <uri>http://chapeau.freevariable.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>Consider the following hypothetical conference session abstract:</p>

<blockquote><p>Much like major oral surgery, writing talk abstracts is universally acknowledged as difficult and painful.  This has never been more true than it is now, in our age of ubiquitous containerization technology.  Today&rsquo;s aggressively overprovisioned, multi-track conferences provide high-throughput information transfer in minimal venue space, but do so at a cost:  namely, they impose stingy abstract word limits.  The increasing prevalence of novel &ldquo;lightning talk&rdquo; tracks presents new challenges for aspiring presenters.  Indeed, the time it takes to read a lightning talk abstract may be a substantial fraction of the time it takes to deliver the talk!  The confluence of these factors, <em>inter alia</em>, presents an increasingly hostile environment for conference talk submissions in late 2015.  Your talk proposals must adapt to this changing landscape or face rejection.  Is there a solution?</p></blockquote>

<p>Hopefully, you recognize some key elements of subpar abstracts that you&rsquo;ve seen, reviewed, or &mdash; maybe, alas &mdash; even submitted in this example.</p>

<p>To identify what&rsquo;s <em>fundamentally</em> wrong with it, we should first consider what the primary rhetorical aims for an abstract are.  In particular, an abstract needs to</p>

<ol>
<li><em>provide context</em> so that a general audience can understand that the problem the talk addresses is interesting,</li>
<li><em>summarize</em> the content of a talk so that audiences and reviewers know what to expect, and</li>
<li><em>motivate</em> conference attendees to put the talk on their schedule (and, more immediately, motivate the program committee to accept the talk).</li>
</ol>


<p>The abstract above does none of these things, for both stylistic and structural reasons.</p>

<p>The example abstract&rsquo;s prose is generally clunky, but the main stylistic problem is its overuse of jargon and enthymemes.  If you don&rsquo;t already spend time in the same neighborhoods of the practice as the author, you probably don&rsquo;t understand all of these terms to mean the same things that the author does or agree with his or her sense of what is &ldquo;universally acknowledged.&rdquo;  It is easy to fall in to using jargon when you&rsquo;re deep in a particular problem domain:  after all, most of the people you interact with use these words and you all seem to understand each other.  However, jargon terms are essentially content-free:  they convey nothing new to specialists and are completely opaque to novices.  By propping up your writing on these empty terms instead of explaining yourself, you are excluding the cohort of your audience who doesn&rsquo;t already understand your problem and shamelessly pandering to the cohort that does.<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup></p>

<p>The main structural problem with the example abstract is that it doesn&rsquo;t actually make an argument for why the talk is interesting or worth attending; instead, it focuses on emphasizing the problems faced by abstract writers and ends with a cliffhanger.  (The cliffhanger strategy not only adds no additional content, it is also <a href="https://en.wikipedia.org/wiki/Betteridge%27s_law_of_headlines">especially risky</a>.)  A surprising number of abstracts, even accepted ones, suffer because they focus on only one or two of an abstract&rsquo;s responsibilities, but it is possible to set your abstract up for success by starting from a structure that is designed to cover all of the abstract&rsquo;s responsibilities.</p>

<p>In 1993, <a href="https://plg.uwaterloo.ca/~migod/research/beckOOPSLA.html">Kent Beck appeared on a panel</a> on how to get a paper accepted at OOPSLA.  OOPSLA (now called <a href="http://2015.splashcon.org">SPLASH</a>) was an academic conference on research and development related to object-oriented programming languages, systems and environments to support object-oriented programming, and applications developed using these technologies.  This is a particularly broad mandate, and because OOPSLA attracted so many papers on a wide range of topics, it had an extremely low acceptance rate.  (This is probably why they held a panel on getting papers accepted, but it also makes OOPSLA a good analogy for contemporary practice-focused technical conferences that often cross several areas of specialization, e.g., data processing, distributed computing, and machine learning.)</p>

<p>Beck&rsquo;s advice is worth reading even if you aren&rsquo;t writing an academic conference paper.  In particular, he suggests that you start by identifying a single &ldquo;startling sentence&rdquo; that summarizes your work and can grab the attention of the program committee.  From there, Beck advises that you adopt the following four-sentence model to structure your abstract:</p>

<ol>
<li>The first sentence is the problem you&rsquo;re trying to solve,</li>
<li>The second sentence provides context for the problem or explains its impact,</li>
<li>The third sentence is the &ldquo;startling sentence&rdquo; that is the key insight or contribution of your work, and</li>
<li>The fourth sentence shows how the key contribution of your work affects the problem.</li>
</ol>


<p>I&rsquo;ve used this template in almost every abstract I&rsquo;ve written for many years, although I sometimes devote more than a single sentence to each step.  It has successfully helped me refine abstracts for both industry conference talks and academic papers, and it more or less ensures that each abstract accomplishes what it needs to.  (If you&rsquo;re writing a talk abstract, as opposed to a paper abstract, it&rsquo;s sometimes also a good idea to add a sentence or two covering what the audience should expect to take away from your talk and why you&rsquo;re qualified to give it.)  If I am sure to consider my audience &mdash; first, an overworked program committee member, and second, a jetlagged and overstimulated conference attendee &mdash; I am far more likely to explain things clearly and eschew jargon.  As a bonus, starting from a fairly rigid structure frees me from wasting time worrying about how best to arrange my prose.</p>

<p>If we avoid jargon and start from Beck&rsquo;s structure, we can transform the mediocre example abstract from the beginning of this post into something far more effective:</p>

<blockquote><p> Contemporary multiple-track industry conferences attract speakers and attendees who specialize in distinct but related parts of the practice.  Since many authors adopt ineffective patterns from other technical abstracts they&rsquo;ve read, they may unwittingly submit talk proposals that are at best rhetorically impotent and at worst nonsensical to people who don&rsquo;t share their specialization.  By starting from a simple template, prospective speakers can dramatically improve their chances of being understood, accepted, and attended, while also streamlining the abstract-writing process.  Excellent abstracts benefit the entire community, because more people will be motivated to learn about interesting work that is outside of their immediate area of expertise.  In this talk, delivered by someone who has delivered many talks without any serious train wrecks and has also helped other people get talks accepted, you&rsquo;ll learn a straightforward technique for designing abstracts that communicate effectively to a general audience, sell your talk to the program committee, and motivate your peers to attend your talk.</p></blockquote>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p>See also <a href="http://www.cs.cmu.edu/~jrs/sins.html">Jonathan Shewchuk on &ldquo;grandmothering&rdquo;</a> or <a href="http://www.exampler.com/testing-com/writings/final-vocabulary.html">Brian Marick on Rorty&rsquo;s concept of &ldquo;final vocabulary.&rdquo;</a><a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[ HTCondor 8.4.1 released! ( October 27, 2015 )]]></title>
      <link href="manual/v8.4.1/10_3Stable_Release.html"/>
      <updated>2015-10-27T05:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[The HTCondor team is pleased to announce the release of HTCondor 8.4.1.
A stable series release contains significant bug fixes.  This release
contains all of the bug fixes from the recent HTCondor 8.2.10 release.

Highlights of this release are:
four new policy metaknobs to make configuration easier;
a bug fix to prevent condor daemons from crashing on reconfiguration;
an option natural sorting option on condor_status;
support of admin to mount certain directories into Docker containers;
many other bug fixes, consult the version history.

Further details can be found in the
Version History.
HTCondor 8.4.1 binaries and source code are available from our
Downloads page.
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[ HTCondor 8.2.10 released! ( October 22, 2015 )]]></title>
      <link href="manual/v8.2.10/10_3Stable_Release.html"/>
      <updated>2015-10-22T05:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[The HTCondor team is pleased to announce the release of HTCondor 8.2.10.
A stable series release contains significant bug fixes.

Highlights of this release are:
an updated RPM to work with SELinux on EL7 platforms;
fixes to the condor_kbdd authentication to the X server;
a fix to allow the condor_kbdd to work with shared port enabled;
avoid crashes when using more than 1024 file descriptors on EL7;
fixed a memory leak in the ClassAd split() function;
condor_vacate will error out rather than ignore conflicting arguments;
a bug fix to the JobRouter to properly process the queue on restart;
a bug fix to prevent sending spurious data on a SOAP file transfer;
a bug fix to always present jobs in order in condor_history.

A complete list of fixed bugs can be found in the
Version History.
HTCondor 8.2.10 binaries and source code are available from our
Downloads page.
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Pacing technical talks]]></title>
      <link href="http://chapeau.freevariable.com/2015/10/pacing-technical-talks.html"/>
      <updated>2015-10-21T16:17:01Z</updated>
      <id>http://chapeau.freevariable.com/2015/10/pacing-technical-talks</id>
      <author>
        <name><![CDATA[William Benton]]></name>
        <uri>http://chapeau.freevariable.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>Delivering a technical talk has a lot in common with running a half-marathon or biking a 40k time trial.  You&rsquo;re excited and maybe a little nervous, you&rsquo;re prepared to go relatively hard for a relatively long time, and you&rsquo;re acutely aware of the clock.  In both situations, you might be tempted to take off right from the gun, diving into your hardest effort (or most technical material), but this is a bad strategy. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Notes from Flink Forward]]></title>
      <link href="http://chapeau.freevariable.com/2015/10/notes-from-flink-forward.html"/>
      <updated>2015-10-20T15:48:44Z</updated>
      <id>http://chapeau.freevariable.com/2015/10/notes-from-flink-forward</id>
      <author>
        <name><![CDATA[William Benton]]></name>
        <uri>http://chapeau.freevariable.com/</uri>
      </author>
      <content type="html"><![CDATA[<p><a data-flickr-embed="true"  href="https://www.flickr.com/photos/willb/22238437291/in/dateposted-public/" title="Brandenburger Tor lightshow"><img src="https://farm1.staticflickr.com/739/22238437291_22636e4a72_b.jpg" width="1024" height="819" alt="Brandenburger Tor lightshow"></a><script async src="//embedr.flickr.com/assets/client-code.js" charset="utf-8"></script> [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[ HTCondor 8.5.0 released! ( October 12, 2015 )]]></title>
      <link href="manual/v8.5.0/10_3Development_Release.html"/>
      <updated>2015-10-12T05:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[The HTCondor team is pleased to announce the release of HTCondor 8.5.0.
This development series release contains new features that are under
development. This release contains all of the bug fixes from the 8.4.0
stable release.

Enhancements in the release include:
multiple enhancements to the python bindings;
the condor_schedd no longer changes the ownership of spooled job files;
spooled job files are visible to only the user account by default;
the condor_startd records when jobs are evicted by preemption or draining.

Further details can be found in the
Version History.
HTCondor 8.5.0 binaries and source code are available from our
Downloads page.
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[A Library of Binary Tree Algorithms as Mixable Scala Traits]]></title>
      <link href="http://erikerlandson.github.com/blog/2015/09/26/a-library-of-binary-tree-algorithms-as-mixable-scala-traits/"/>
      <updated>2015-09-26T19:43:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2015/09/26/a-library-of-binary-tree-algorithms-as-mixable-scala-traits</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>In this post I am going to describe some work I've done recently on a system of Scala traits that support tree-based collection algorithms prefix-sum, nearest key query and value increment in a mixable format, all backed by Red-Black balanced tree logic, which is also a fully inheritable trait. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[ HTCondor 8.4.0 released! ( September 14, 2015 )]]></title>
      <link href="manual/v8.4.0/10_3Stable_Release.html"/>
      <updated>2015-09-14T05:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[The HTCondor team is pleased to announce the release of HTCondor version 8.4.0.
After a year of development, this is the first release of the new stable series.

This version contains:
a Docker Universe to run a Docker container as an HTCondor job;
the submit file can queue a job for each file found;
the submit file can contain macros;
a dry-run option to condor_submit to test the submit file without any actions;
HTCondor pools can use IPv4 and IPv6 simultaneously;
execute directories can be encrypted upon user or administrator request;
Vanilla Universe jobs can utilize periodic application-level checkpoints;
the administrator can establish job requirements;
numerous scalability changes.

Further details can be found in the
Version History.
HTCondor 8.4.0 binaries and source code are available from our
Downloads page.
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Lightweight Non-Negative Numerics for Better Scala Type Signatures]]></title>
      <link href="http://erikerlandson.github.com/blog/2015/08/18/lightweight-non-negative-numerics-for-better-scala-type-signatures/"/>
      <updated>2015-08-19T00:42:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2015/08/18/lightweight-non-negative-numerics-for-better-scala-type-signatures</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>In this post I want to discuss several advantages of defining lightweight non-negative numeric types in Scala, whose primary benefit is that they allow improved type signatures for Scala functions and methods.  I'll first describe the simple class definition, and then demonstrate how it can be used in function signatures and the benefits of doing so. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[The Reservoir Sampling Gap Distribution]]></title>
      <link href="http://erikerlandson.github.com/blog/2015/08/17/the-reservoir-sampling-gap-distribution/"/>
      <updated>2015-08-17T14:35:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2015/08/17/the-reservoir-sampling-gap-distribution</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>In a <a href="http://erikerlandson.github.io/blog/2014/09/11/faster-random-samples-with-gap-sampling/">previous post</a>, I showed that random Bernoulli and Poisson sampling could be made much faster by modeling the <em>sampling gap distribution</em> - that is, directly drawing random samples from the distribution of how many elements would be skipped over between actual samples taken. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[GPUs and adding new resources types to the HTCondor-CE]]></title>
      <link href="http://derekweitzel.blogspot.com/2015/08/gpus-and-adding-new-resources-types-to.html"/>
      <updated>2015-08-07T21:45:00Z</updated>
      <id>tag:blogger.com,1999:blog-3007054864987759910.post-6301136022425730449</id>
      <author>
        <name><![CDATA[Derek Weitzel]]></name>
        <uri>http://derekweitzel.blogspot.com</uri>
      </author>
      <content type="html"><![CDATA[]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[The more things change, the more they stay the same]]></title>
      <link href="http://derekweitzel.blogspot.com/2015/07/the-more-things-change-more-they-stay.html"/>
      <updated>2015-07-28T15:00:00Z</updated>
      <id>tag:blogger.com,1999:blog-3007054864987759910.post-8394942129716842708</id>
      <author>
        <name><![CDATA[Derek Weitzel]]></name>
        <uri>http://derekweitzel.blogspot.com</uri>
      </author>
      <content type="html"><![CDATA[]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[fedmsg talk at Spark Summit]]></title>
      <link href="http://chapeau.freevariable.com/2015/06/summit-fedmsg.html"/>
      <updated>2015-06-15T15:05:27Z</updated>
      <id>http://chapeau.freevariable.com/2015/06/summit-fedmsg</id>
      <author>
        <name><![CDATA[William Benton]]></name>
        <uri>http://chapeau.freevariable.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>I&rsquo;m speaking at Spark Summit today about using Spark to analyze operational data from the Fedora project.  Here are some links to further resources related to my talk: [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[HTCondor CacheD: Caching for HTC - Part 2]]></title>
      <link href="http://derekweitzel.blogspot.com/2015/01/htcondor-cached-caching-for-htc-part-2.html"/>
      <updated>2015-01-25T15:59:00Z</updated>
      <id>tag:blogger.com,1999:blog-3007054864987759910.post-5260378956420164105</id>
      <author>
        <name><![CDATA[Derek Weitzel]]></name>
        <uri>http://derekweitzel.blogspot.com</uri>
      </author>
      <content type="html"><![CDATA[]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Condor CacheD: Caching for HTC - Part 1]]></title>
      <link href="http://derekweitzel.blogspot.com/2015/01/condor-cached-caching-for-htc-part-1.html"/>
      <updated>2015-01-22T16:00:00Z</updated>
      <id>tag:blogger.com,1999:blog-3007054864987759910.post-1889975382858537261</id>
      <author>
        <name><![CDATA[Derek Weitzel]]></name>
        <uri>http://derekweitzel.blogspot.com</uri>
      </author>
      <content type="html"><![CDATA[]]></content>
    </entry>
  
</feed>
