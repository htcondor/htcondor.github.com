<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[Planet HTCondor Meta Feed]]></title>
  <link href="http://htcondor.github.com/planet/atom.xml" rel="self"/>
  <link href="http://htcondor.github.com/"/>
  <updated>2015-02-12T06:41:12-07:00</updated>
  <id>http://htcondor.github.com/planet/atom.xml</id>
  <author>
    <name><![CDATA[HTCondor Project]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
    <entry>
      <title type="html"><![CDATA[ HTCondor Week 2015 announced! ( February 11, 2015 )]]></title>
      <link href="http://research.cs.wisc.edu/htcondor/HTCondorWeek2015/"/>
      <updated>2015-02-11T06:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[HTCondor Week 2015 is May 19â22, 2015 in Madison, Wisconsin.  Join other users, administrators, and deveopers for the opportunity to exchange ideas and experiences, to learn about the latest research, to experience live demos, and to influence our short and long term research and development directions.
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[ HTCondor 8.2.7 released! ( February 10, 2015 )]]></title>
      <link href="manual/v8.2.7/10_3Stable_Release.html"/>
      <updated>2015-02-10T06:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[The HTCondor team is pleased to announce the release of HTCondor version 8.2.7.
A stable series release contains significant bug and security fixes.
This version contains:
sendmail is used by default for sending notifications (CVE-2014-8126);
corrected input validation, which prevents daemon crashes;
an update, such that grid jobs work within the current Google Compute Engine;
a bug fix to prevent an infinite loop in the python bindings;
a bug fix to prevent infinite recursion when evaluating ClassAd attributes.
A complete list of fixed bugs can be found in the
Version History.
HTCondor 8.2.7 binaries and source code are available from our
Downloads page.
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[ HTCondor assisted the paleobiology application of DeepDive ( February 5, 2015 )]]></title>
      <link href="http://www.news.wisc.edu/23330"/>
      <updated>2015-02-05T06:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[This  UW Madison news article describes the paleobiology application of the DeepDive system, which used text-mining techniques to extract data from publications and build a database. The quality of the database contents equaled that achieved by scientists. HTCondor helped to provide the million hours of compute time needed to build the database from tens of thousands of publications.
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[HTCondor CacheD: Caching for HTC - Part 2]]></title>
      <link href="http://derekweitzel.blogspot.com/2015/01/htcondor-cached-caching-for-htc-part-2.html"/>
      <updated>2015-01-25T15:59:00Z</updated>
      <id>tag:blogger.com,1999:blog-3007054864987759910.post-5260378956420164105</id>
      <author>
        <name><![CDATA[Derek Weitzel]]></name>
        <uri>http://derekweitzel.blogspot.com</uri>
      </author>
      <content type="html"><![CDATA[In the <a href="http://derekweitzel.blogspot.com/2015/01/condor-cached-caching-for-htc-part-1.html">previous post</a>, I discussed why we decided to make the HTCondor CacheD. &nbsp;This time, we will discuss the operation and design of the CacheD, as well as show an example utilizing a BLAST database.<br /><br />It is important to note that the CacheD is still very much "dissertation-ware." &nbsp;It functions enough to demonstrate the improvements, but not enough to be put into production.<br /><br /><h4>The Cache</h4><div>The fundamental unit that the CacheD works with is an immutable set of files in a cache. &nbsp;A user creates and uploads files into the cache. &nbsp;Once the upload is complete, the cache is committed and may not be altered at any time. &nbsp;The cache has a set of metadata associated with it as well, stored as classads in a durable storage database (using the same techniques as the SchedD job queue).</div><div><br /></div><div>The cache has a 'lease', or an expiration date. &nbsp;This lease is a given amount of time that the cache is guaranteed to be available from a particular CacheD. &nbsp;When creating the cache, the user provides a requested cache lifetime. &nbsp;The CacheD can either accept or reject the requested cache lifetime. &nbsp;Once the cache's lifetime expires, it can be deleted by the CacheD and is no longer guaranteed to be available. &nbsp;The user may request to extend the lifetime of a cache after it has already been committed, which the CacheD may or may not accept.</div><div><br /></div><div>The cache also has properties similar to a job. &nbsp;For example, the cache can have it's own set of requirements for which nodes it can be replicated to. &nbsp;By default, a cache is initialized with the requirement that a CacheD has enough disk space to hold the cache. &nbsp;Analogous to the HTCondor matching with jobs, the cache can have requirements, and the CacheD can have requirements. &nbsp;A CacheD requirements may be that the node has enough disk space to hold the matched cache. &nbsp;This two way matching guarantees that any local policies are enforced.</div><div><br /></div><div>The requirements attribute is especially useful when the user aligns the cache's requirements with the jobs that require the data. &nbsp;For example, if the user knows that their processing requires nodes with 8GB of ram available, then there is no point is replicating the cache to a node with less than 8GB of ram.</div><div><br /></div><h4>The CacheD</h4><div>The CacheD is the daemon that manages caches on the local node. &nbsp;Each CacheD is considered a peer to all other CacheD's, there is no further coordination daemon. &nbsp;Each cache serves multiple functions:</div><div><ol><li>Respond to user requests to create, query, update, and delete caches.</li><li>Send replication requests to CacheD's that match each cache's requirements.</li><li>Respond to replication requests from other CacheD's. &nbsp;Matching is done on the cache before transferring the data</li></ol><div>The CacheD keeps a database storing the metadata for each cache. &nbsp;The database is stored using the same techniques as the SchedD uses for jobs to maintain a durable database store. &nbsp;It also maintains a directory containing all of the caches stored on the node.</div></div><div><br /></div><div>The CacheD's user interface is primarily through python bindings, at least for the time being.</div><div><br /></div><h4>CacheD Usage</h4><div>The CacheD is used in conjunction with glideins. &nbsp;The CacheD is started along with other glidein daemons such as the HTCondor StartD.</div><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://4.bp.blogspot.com/-0t61LNEhNP8/VMK3q55wm3I/AAAAAAAAC28/CdyFV9FXD3I/s1600/CachedInitialization.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" src="http://4.bp.blogspot.com/-0t61LNEhNP8/VMK3q55wm3I/AAAAAAAAC28/CdyFV9FXD3I/s1600/CachedInitialization.png" height="344" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Initialization of cache as well as initial replication requests</td></tr></tbody></table><div>The user initializes the cache by creating and uploading it to the user's submit machine. &nbsp;The CacheD connects to remote CacheD's, sending replication requests.</div><div class="separator" style="clear: both; text-align: center;"></div><div><br /></div><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://1.bp.blogspot.com/-aZhjvd3F_M0/VMK8YMswJ9I/AAAAAAAAC3M/_pCf-OX1Q8k/s1600/CachedReplication.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" src="http://1.bp.blogspot.com/-aZhjvd3F_M0/VMK8YMswJ9I/AAAAAAAAC3M/_pCf-OX1Q8k/s1600/CachedReplication.png" height="314" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">The BitTorrent communication between nodes after accepting the replication request</td></tr></tbody></table><div>Once the CacheD's accept the replication request(s), BitTorrent protocol allows for communication between all nodes inside the cluster, as well with the user's submit machine. &nbsp;This graph only shows a single cluster, but this could be replicated to many clusters as well.</div><div><br /></div><h4>In Action&nbsp;</h4><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://2.bp.blogspot.com/-AVc8_0W-tSk/VMLASPT5t8I/AAAAAAAAC3U/IDN7RG2DvkI/s1600/verbose_group.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" src="http://2.bp.blogspot.com/-AVc8_0W-tSk/VMLASPT5t8I/AAAAAAAAC3U/IDN7RG2DvkI/s1600/verbose_group.png" height="308" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Partial graph showing data transfers. &nbsp;Due to overflowing the event queue, not all downloads are captured.</td></tr></tbody></table><div>The above graph shows the data transfer using the BitTorrent protocol between the nodes that have accepted the replication request and the Cache Origin, which is an external node. &nbsp;In this example, only 5 remote CacheD's where started on the cluster. &nbsp;Because of all of the traffic between nodes, this level of detail graph becomes unreadable very quickly when increasing the number of remote CacheD's.</div><div><br /></div><div>You will notice that the Cache Origin only transfers to 2 nodes inside the cluster. &nbsp;The BitTorrent protocol is complicated and difficult to predict, therefore this could be caused by many factors. &nbsp;For example, the two nodes could have found the CacheD origin first, therefore being the first nodes to download it. &nbsp;The other nodes would then have found the internal cluster nodes with portions of the cache, and begun to download from it.<br /><br />It is important to note that even though the ~15GB cache is transferred to all 5 nodes, totalling 75GB of transferred cache, only ~15Gb is transferred from the cache origin, and all of the rest of the transfers are between nodes in the cluster.<br /><br /><h4>Up Next</h4></div><div>In Part 3 of the series, I will look at timings of the transfers using data analysis of trial runs. &nbsp;As a hint, the BitTorrent protocol is slower than direct transfers for 1 to 1 transfers. &nbsp;But it really shines when increasing the number of downloaders and seeders.</div><div><br /></div>]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Monadic 'break' and 'continue' for Scala Sequence Comprehensions]]></title>
      <link href="http://erikerlandson.github.com/blog/2015/01/24/monadic-break-and-continue-for-scala-sequence-comprehensions/"/>
      <updated>2015-01-24T18:54:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2015/01/24/monadic-break-and-continue-for-scala-sequence-comprehensions</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>Author's note: I've since received some excellent feedback from the Scala community, which I included in some <a href="#notes">end notes</a>.</p>

<p>Author's note the 2nd: I later realized I could apply an implicit conversion and mediator class to preserve the traditional ordering: the code has been updated with that approach.</p>

<p>Author's note the 3rd: This concept has been submitted to the Scala project as JIRA <a href="https://issues.scala-lang.org/browse/SI-9120">SI-9120</a> (PR <a href="https://github.com/scala/scala/pull/4275">#4275</a>)</p>

<p>Scala <a href="http://docs.scala-lang.org/tutorials/tour/sequence-comprehensions.html">sequence comprehensions</a> are an excellent functional programming idiom for looping in Scala.  However, sequence comprehensions encompass much more than just looping -- they represent a powerful syntax for manipulating <em>all</em> monadic structures<a href="#ref1">[1]</a>.</p>

<p>The <code>break</code> and <code>continue</code> looping constructs are a popular framework for cleanly representing multiple loop halting and continuation conditions at differing stages in the execution flow.  Although there is no native support for <code>break</code> or <code>continue</code> in Scala control constructs, it is possible to implement them in a clean and idiomatic way for sequence comprehensions.</p>

<p>In this post I will describe a lightweight and easy-to-use implementation of <code>break</code> and <code>continue</code> for use in Scala sequence comprehensions (aka <code>for</code> statements).  The entire implementation is as follows:</p>

<pre><code>object BreakableGenerators {
  import scala.language.implicitConversions

  type Generator[+A] = Iterator[A]
  type BreakableGenerator[+A] = BreakableIterator[A]

  // Generates a new breakable generator from any traversable object.
  def breakable[A](t1: TraversableOnce[A]): Generator[BreakableGenerator[A]] =
    List(new BreakableIterator(t1.toIterator)).iterator

  // Mediates boolean expression with 'break' and 'continue' invocations
  case class BreakableGuardCondition(cond: Boolean) {
    // Break the looping over one or more breakable generators, if 'cond' 
    // evaluates to true.
    def break(b: BreakableGenerator[_], bRest: BreakableGenerator[_]*): Boolean = {
      if (cond) {
        b.break
        for (x &lt;- bRest) { x.break }
      }
      !cond
    }

    // Continue to next iteration of enclosing generator if 'cond' 
    // evaluates to true.
    def continue: Boolean = !cond
  }

  // implicit conversion of boolean values to breakable guard condition mediary
  implicit def toBreakableGuardCondition(cond: Boolean) =
    BreakableGuardCondition(cond)

  // An iterator that can be halted via its 'break' method.  Not invoked directly
  class BreakableIterator[+A](itr: Iterator[A]) extends Iterator[A] {
    private var broken = false
    private[BreakableGenerators] def break { broken = true }

    def hasNext = !broken &amp;&amp; itr.hasNext
    def next = itr.next
  }
}
</code></pre>

<p>The approach is based on a simple subclass of <code>Iterator</code> -- <code>BreakableIterator</code> -- that can be halted by 'breaking' it.  The function <code>breakable(&lt;traversable-object&gt;)</code> returns an Iterator over a single <code>BreakableIterator</code> object.  Iterators are monad-like structures in that they implement <code>map</code> and <code>flatMap</code>, and so its output can be used with <code>&lt;-</code> at the start of a <code>for</code> construct in the usual way.  Note that this means the result of the <code>for</code> statement will also be an Iterator.</p>

<p>Whenever the boolean expression for an <code>if</code> guard is followed by either <code>break</code> or <code>continue</code>, it is implicitly converted to a "breakable guard condition" that supports those methods.  The function <code>break</code> accepts one or more instances of <code>BreakableIterator</code>.  If it evaluates to <code>true</code>, the loops embodied by the given iterators are immediately halted via the associated <code>if</code> guard, and the iterators are halted via their <code>break</code> method.  The <code>continue</code> function is mostly syntactic sugar for a standard <code>if</code> guard, simply with the condition inverted.</p>

<p>Here is a simple example of <code>break</code> and <code>continue</code> in use:</p>

<pre><code>object Main {
  import BreakableGenerators._

  def main(args: Array[String]) {

    val r = for (
      // generate a breakable sequence from some sequential input
      loop &lt;- breakable(1 to 1000);
      // iterate over the breakable sequence
      j &lt;- loop;
      // print out at each iteration
      _ = { println(s"iteration j= $j") };
      // continue to next iteration when 'j' is even
      if { j % 2 == 0 } continue;
      // break out of the loop when 'j' exceeds 5
      if { j &gt; 5 } break(loop)
    ) yield {
      j
    }
    println(s"result= ${r.toList}")
  }
}
</code></pre>

<p>We can see from the resulting output that <code>break</code> and <code>continue</code> function in the usual way.  The <code>continue</code> clause ignores all subsequent code when <code>j</code> is even.  The <code>break</code> clause halts the loop when it sees its first value > 5, which is 7.  Only odd values &lt;= 5 are output from the <code>yield</code> statement:</p>

<pre><code>$ scalac -d /home/eje/class monadic_break.scala
$ scala -classpath /home/eje/class Main
iteration j= 1
iteration j= 2
iteration j= 3
iteration j= 4
iteration j= 5
iteration j= 6
iteration j= 7
result= List(1, 3, 5)
</code></pre>

<p>Breakable iterators can be nested in the way one would expect.  The following example shows an inner breakable loop nested inside an outer one:</p>

<pre><code>object Main {
  import BreakableGenerators._

  def main(args: Array[String]) {
    val r = for (
      outer &lt;- breakable(1 to 7);
      j &lt;- outer;
      _ = { println(s"outer  j= $j") };
      if { j % 2 == 0 } continue;
      inner &lt;- breakable(List("a", "b", "c", "d", "e"));
      k &lt;- inner;
      _ = { println(s"    inner  j= $j  k= $k") };
      if { k == "d" } break(inner);
      if { j == 5  &amp;&amp;  k == "c" } break(inner, outer)
    ) yield {
      (j, k)
    }
    println(s"result= ${r.toList}")
  }
}
</code></pre>

<p>The output demonstrates that the inner loop breaks whenever <code>k=="d"</code>, and so <code>"e"</code> is never present in the <code>yield</code> result.  When <code>j==5</code> and <code>k=="c"</code>, both the inner and outer loops are broken, and so we see that there is no <code>(5,"c")</code> pair in the result, nor does the outer loop ever iterate over 6 or 7:</p>

<pre><code>$ scalac -d /home/eje/class monadic_break.scala
$ scala -classpath /home/eje/class Main
outer  j= 1
    inner  j= 1  k= a
    inner  j= 1  k= b
    inner  j= 1  k= c
    inner  j= 1  k= d
outer  j= 2
outer  j= 3
    inner  j= 3  k= a
    inner  j= 3  k= b
    inner  j= 3  k= c
    inner  j= 3  k= d
outer  j= 4
outer  j= 5
    inner  j= 5  k= a
    inner  j= 5  k= b
    inner  j= 5  k= c
result= List((1,a), (1,b), (1,c), (3,a), (3,b), (3,c), (5,a), (5,b))
</code></pre>

<p>Using <code>break</code> and <code>continue</code> with <code>BreakableIterator</code> for sequence comprehensions is that easy.  Enjoy!</p>

<p><a name="notesname" id="notes"></a></p>

<h5>Notes</h5>

<p>The helpful community on freenode #scala made some excellent observations:</p>

<p>1: Iterators in Scala are not strictly monadic -- it would be more accurate to say they're "things with a flatMap and map method, also they can use filter or withFilter sometimes."  However, I personally still prefer to think of them as "monadic in spirit if not law."</p>

<p>2: The <code>break</code> function, as described in this post, is not truly functional in the sense of referential transparency, as the invocation <code>if break(loop) { condition }</code> involves a side-effect on the variable <code>loop</code>.  I would say that it does maintain "scoped functionality."  That is, the break in non-referential transparency is scoped by the variables in question.  The <code>for</code> statement containing them is referentially transparent with respect to its inputs (provided no other code is breaking referential transparency, of course).</p>

<h5>References</h5>

<p><a name="ref1name" id="ref1">[1] </a><em><a href="http://www.manning.com/bjarnason/">Functional Programming in Scala</a></em>, Paul Chiusano and Runar Bjarnason, (section 6.6)</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Condor CacheD: Caching for HTC - Part 1]]></title>
      <link href="http://derekweitzel.blogspot.com/2015/01/condor-cached-caching-for-htc-part-1.html"/>
      <updated>2015-01-22T16:00:00Z</updated>
      <id>tag:blogger.com,1999:blog-3007054864987759910.post-1889975382858537261</id>
      <author>
        <name><![CDATA[Derek Weitzel]]></name>
        <uri>http://derekweitzel.blogspot.com</uri>
      </author>
      <content type="html"><![CDATA[A typical job flow in the Open Science Grid is as follows:<br /><ol><li>Stage input files to worker node.</li><li>Start processing...</li><li>Stage output files back to submit host.</li></ol><div>In an ideal world, step 2 would take the longest. &nbsp;But, as data sizes increase, so to does the stage-in and stage-out of the files (primarily the stage-in). &nbsp;Additionally, we continue to recommend the same maximum job length to users, around 8 hours. <br /><br />Lets use a real world example. &nbsp;The nr blast database (Non-redundant GenBank CDS translations + PDB + SwissProt + PIR + PRF, excluding those in env_nr<span style="background-color: white; color: #333333; font-family: arial, tahoma, verdana, sans-serif; font-size: 13px; line-height: 19.9200000762939px;">) (<a href="http://blast.ncbi.nlm.nih.gov/Blast.cgi?CMD=Web&amp;PAGE_TYPE=BlastDocs&amp;DOC_TYPE=ProgSelectionGuide">source</a>)</span>&nbsp;is currently ~15GB. &nbsp;When running blast, each query needs to run over the entire nr database, therefore it is required on every worker node that will run the query.<br /><br />It is easy to say... well a 1 Gbps connection can transfer 15GB in ~120 seconds. &nbsp;Two minutes doesn't seem unreasonable to stage-in data, especially if the job can be 8 hours long. &nbsp;But usually you have many queries, so you will want to run these queries across many jobs. &nbsp;So lets say you submit 10 jobs, each needing the 15GB. &nbsp;Well, that should mean that if they all transfer at the same time, it will take 20 minutes of stage in time before any processing begins. &nbsp;But that is only for 10 jobs, what if you are submitting 1,000 jobs, or 10,000 jobs? &nbsp;At 1,000 jobs, it takes 33 hours to transfer data for input files? &nbsp;Suddenly 2 minutes to transfer a database becomes hours. &nbsp;And your submit machine is doing nothing but transferring input files! &nbsp;Also, using some math (or a simple <a href="http://nbviewer.ipython.org/gist/djw8605/144729e82f0ef8313f6d">simulation</a>), you can see that if the jobs start sequentially, you are limited on the number of jobs that can run simultaneously.<br /><br />This increasing data size and static maximum job length have lead to compromises and innovations on the part of users and sites.<br /><br /><h4>Innovations</h4></div><div>Some users and sites have attempted to solve the larger input files. &nbsp;They can be typically broken into two categories:</div><div><ol><li>Bandwidth increases from the storage nodes.</li><li>Caching near the execution host.</li></ol><div><br /></div></div><div><h3>Lots O' Bandwidth</h3><a href="http://osgconnect.net/">OSG Connect's</a>&nbsp;<a href="http://stash.osgconnect.net/">Stash</a> attempts to ease the input file stage problem by providing a storage service with lots of bandwidth (10 Gbps), and support for site caching through HTTP. &nbsp;Certainly the higher bandwidth solution is the brute force method of decreasing the transfer time for files. &nbsp;But, 10 Gbps only knocks off a factor of 10 from all of the above times. &nbsp;This will certainly decrease the transfer time, but only if you can use all 10 Gbps.<br /><br />Numerous sites and users have tried to use high bandwidth storage services to solve the stage-in problem. &nbsp;Nebraska (and many other sites) even have their storage services connected to 100 Gbps network connections. &nbsp;But they tend to be limited not by the bandwidth available to the storage device, but by the bandwidth bottleneck at the boundary of each cluster to the outside world, usually a NAT.<br /><br /></div><div><h3>Caching</h3></div><div>With Stash's HTTP interface, users can use local site caching. &nbsp;When the site caching was designed, it was meant to be used for calibration data for detectors. &nbsp;This tends to be rather small data that is frequently accessed, the perfect use of a HTTP cache. &nbsp;But what about when you want to transfer 15GBs of files through the HTTP cache? &nbsp;A typical site may only have a few cache servers, therefore limiting the bandwidth available to download not on the available bandwidth of the hosting server, but the available bandwidth on the cache servers. &nbsp;I don't know of many sites that are putting 10Gbps connections on their caching servers.<br /><br />Additionally, site caching simply runs from the submit host to the remote caching host. &nbsp;In the last week, the 95% of OSG VO's CPU hours have been provided by ~25 unique sites (<a href="http://gratiaweb.grid.iu.edu/gratia/xml/facility_hours_bar_smry?facility=.*&amp;probe=.*&amp;resource-type=%5EBatch%24&amp;vo=osg&amp;user=.*&amp;endtime=2015-01-21+23%3A59%3A59&amp;exclude-vo=Unknown%7Cunknown%7Cother&amp;includeFailed=true&amp;exclude-role=NONE&amp;span=86400&amp;vo_set=%5Eosg%24&amp;role=.*&amp;starttime=2015-01-07+00%3A00%3A00&amp;exclude-facility=NONE%7CGeneric%7CObsolete&amp;exclude-user=NONE&amp;includeSuccess=true">source</a>, <a href="https://docs.google.com/spreadsheets/d/1hcOHDaXR-l78pufw4lM_dkAL1nIO68E6Rx0Cp1nOVrM/edit?usp=sharing">calculations</a>). &nbsp;This analogous to increasing the bandwidth of the submit host by 25 times (assuming 1gbps connections standard). &nbsp;This is a very cheap way to increase the bandwidth available to transfer &nbsp;input files. &nbsp;But the VO's usage is not split evenly amongst all 25 sites. &nbsp;6 sites account for ~50% of the OSG VO usage. &nbsp;On those 6 clusters, the transfer bandwidth is limited to what those 6 proxy servers can transfer to their cluster's nodes. &nbsp;Therefore, for 50% of your processing slots, you are only increasing the transfer speed by 6.</div><div><br /></div><div>The HTTP protocol and HTTP caching is not designed for such large files. &nbsp;HTTP will always be designed for the dominant users, browsers downloading relatively small webpages. &nbsp;Software designed to use HTTP are optimized for web sites, lots of small files. &nbsp;Therefore, any software that might be used in conjunction with HTTP may not be ideal for large files.</div><div><br /></div><h4>A New Hope</h4><div>Part of my PhD has been to develop a new way to handle large stage-in datasets. &nbsp;In the above problem statement, two areas where most often used to optimize transfer times, bandwidth and caching. &nbsp;My attempts to optimize both of these approaches using a daemon named the HTCondor CacheD.<br /><br /><h3>Bandwidth</h3></div><div>As noted above, HTTP is great for it's designed use: websites with lots of small files. &nbsp;But it is not designed or well optimized for larger files. &nbsp;Further, the bandwidth from the execution host to the storage servers are typically bottlenecked at the cluster boundary. &nbsp; Therefore another protocol was chosen that could better handle large files, BitTorrent.</div><br /><a href="https://en.wikipedia.org/wiki/BitTorrent">BitTorrent</a>&nbsp;was chosen since it contains many characteristics that make it ideal for transferring large files. &nbsp;In order to bypass the network bottlenecks in the remote clusters, BitTorrent allows for clients to transfer data between them while also downloading from the original source. &nbsp;This allows every worker node to become a cache for the rest of the cluster nodes. &nbsp;As we will see in the next post, BitTorrent works well because the vast majority of the traffic is between nodes inside the cluster, rather than to nodes outside the cluster.<br /><br /><h3>Caching</h3><div>The caching described above utilized a single cache on each site. &nbsp;From the usage breakdown, you can see that VO's that rely on few sites will find this a bottleneck. &nbsp;For campus users, which may only use 1 or 2 clusters, this can be as bad of a bottleneck as not using a cache at all. &nbsp;Also, this requires the site to have a caching server setup, which requires administrator cooperation.<br /><br />The CacheD instead uses local caches on each worker node. &nbsp;This local cache allows very fast transfers when the jobs begin. &nbsp;Further, the local cache acts as a seeder for the BitTorrent transfers described above.<br /><br />When a job is begins running on a node, the stage-in stage will request from the local cache a copy of the data files. &nbsp;If the data files are not already staged at the local node, the local cache will pull the data files using BitTorrent from the submit host (cache origin) and all other nodes in the cluster. &nbsp;When the local cache has cached the stage-in data files, it will transfer the cached files into the jobs sandbox and begin processing. &nbsp;Subsequent jobs that require the same stage-in data files will request then immediately receive the files since they are already cached locally.</div><div><br /></div><h4>Up Next</h4><div>In the next post, I will describe the design of the CacheD in more detail. &nbsp;Further, I will show usage of the CacheD with blast databases, and the improvement in job startup time resulting in optimized stage-in transfers.<br /><br />UPDATE: Link to Part 2, <a href="http://derekweitzel.blogspot.com/2015/01/htcondor-cached-caching-for-htc-part-2.html">Architecture of the CacheD</a>.</div><br />]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[ HTCondor 8.3.2 released! ( December 23, 2014 )]]></title>
      <link href="manual/v8.3.2/10_3Development_Release.html"/>
      <updated>2014-12-23T06:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[The HTCondor team is pleased to announce the release of HTCondor 8.3.2.
This development series release contains new features that are under
development. This release contains all of the bug fixes from the 8.2.6
stable release.
This new version contains:
the next installment of IPv4/IPv6 mixed mode support: a submit node can
simultaneously interact with an IPv4 and an IPv6 HTCondor pool;
scalability improvements: a reduced memory foot-print of daemons,
a reduced number of TCP connections between submit and execute machines,
and an improved responsiveness from a busy condor_schedd to queries.
A complete list of new features and fixed bugs can be found in the
Version History.
HTCondor 8.3.2 binaries and source code are available from our
Downloads page.
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[ HTCondor 8.2.6 released! ( December 16, 2014 )]]></title>
      <link href="manual/v8.2.6/10_3Stable_Release.html"/>
      <updated>2014-12-16T06:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[The HTCondor Team is pleased to announce the release of HTCondor
version 8.2.6.
This new version contains:
a bug fix to the log rotation of the condor_schedd on Linux platforms;
transfer_input_files now works for directories on Windows platforms;
a correction of the flags passed to the mail program on Linux platforms;
a RHEL 7 platform fix of a directory permission that prevented daemons from starting.
A complete list of fixed bugs can be found in the
Version History.
HTCondor 8.2.6 binaries and source code are available from our
Downloads page.
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Caveat censor]]></title>
      <link href="http://chapeau.freevariable.com/2014/12/caveat-censor.html"/>
      <updated>2014-12-02T16:12:22Z</updated>
      <id>http://chapeau.freevariable.com/2014/12/caveat-censor</id>
      <author>
        <name><![CDATA[William Benton]]></name>
        <uri>http://chapeau.freevariable.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>Over eight years ago, <a href="http://rwmj.wordpress.com">Richard WM Jones</a> wrote a <a href="http://web.archive.org/web/20071013190833/http://blog.merjis.com/2006/11/08/practical-ocaml/">great but disheartening article about his experience serving as a technical reviewer</a> for <a href="http://www.amazon.com/Practical-OCaml-Joshua-B-Smith/dp/159059620X">an infamous book about OCaml</a>.  The post made quite an impression on me at the time and I&rsquo;ve often recalled it over the years whenever opportunities to do prepress reviews have landed in my inbox.  Briefly, Jones was asked to use terrible tools (Microsoft Word) to deal with stilted, error-ridden prose surrounding unidiomatic and broken code.  For his trouble, he got an hourly wage that would have represented a slight raise over what I made mowing my neighbors&#8217; lawns in high school. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Spark performance talk at ApacheCon EU]]></title>
      <link href="http://chapeau.freevariable.com/2014/11/apachecon.html"/>
      <updated>2014-11-18T10:12:40Z</updated>
      <id>http://chapeau.freevariable.com/2014/11/apachecon</id>
      <author>
        <name><![CDATA[William Benton]]></name>
        <uri>http://chapeau.freevariable.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>I&rsquo;ll be speaking <a href="http://apacheconeu2014.sched.org/event/d4dac3bfd7e7ec1ed42bc4fb349a070b#.VGsb-UQo4d8">later this afternoon at ApacheCon EU</a>.  The title of my talk is &ldquo;Iteratively Improving Spark Application Performance.&rdquo; The great thing about Apache Spark is that simple prototype applications are very easy to develop, and even a first attempt at realizing a new analysis will usually work well enough so that it&rsquo;s not frustrating to evaluate it on real data.  However, simple prototypes can often exhibit performance problems that aren&rsquo;t obvious until you know where to look. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Algebraic types and schema inference]]></title>
      <link href="http://chapeau.freevariable.com/2014/11/algebraic-types.html"/>
      <updated>2014-11-03T01:55:10Z</updated>
      <id>http://chapeau.freevariable.com/2014/11/algebraic-types</id>
      <author>
        <name><![CDATA[William Benton]]></name>
        <uri>http://chapeau.freevariable.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>My <a href="http://chapeau.freevariable.com/2014/10/fedmsg-and-spark.html">last post</a> covered some considerations for using Spark SQL on a real-world JSON dataset.  In particular, schema inference can suffer when you&rsquo;re ingesting a dataset of heterogeneous objects.  In this post, I&rsquo;d like to sketch out some ways to connect schema inference to type inference, in order to point to automated solutions to some of the problems we&rsquo;ve seen. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[fedmsg data and Spark SQL]]></title>
      <link href="http://chapeau.freevariable.com/2014/10/fedmsg-and-spark.html"/>
      <updated>2014-10-31T16:28:16Z</updated>
      <id>http://chapeau.freevariable.com/2014/10/fedmsg-and-spark</id>
      <author>
        <name><![CDATA[William Benton]]></name>
        <uri>http://chapeau.freevariable.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>In this post, I&rsquo;ll briefly introduce <a href="http://www.fedmsg.com/en/latest/">fedmsg</a>, the federated message bus developed as part of the Fedora project&rsquo;s infrastructure, and discuss how to ingest fedmsg data for processing with Spark SQL.  While I hope you&rsquo;ll find the analytic possibilities of fedmsg data as interesting as I do, this post will also cover some possible pitfalls you might run into while ingesting complex JSON data or the contents of large SQL databases into Spark SQL more generally. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Notes from Strata + Hadoop World 2014]]></title>
      <link href="http://chapeau.freevariable.com/2014/10/notes-from-strata-2014.html"/>
      <updated>2014-10-21T16:38:11Z</updated>
      <id>http://chapeau.freevariable.com/2014/10/notes-from-strata-2014</id>
      <author>
        <name><![CDATA[William Benton]]></name>
        <uri>http://chapeau.freevariable.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>I went to Strata + Hadoop World last week.  This event targets a pretty broad audience and is an interesting mix of trade show, data science conference, and software conference.  However, I&rsquo;ve been impressed by the quality and relevance of the technical program both of the years that I&rsquo;ve gone.  The key to finding good talks at this kind of event is to target talks focusing on applications, visualization, and fundamental techniques, rather than ostensibly technical talks.<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>  In the rest of this post, I&rsquo;ll share some of my notes from the talks and tutorials I enjoyed the most. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Faster Random Samples With Gap Sampling]]></title>
      <link href="http://erikerlandson.github.com/blog/2014/09/11/faster-random-samples-with-gap-sampling/"/>
      <updated>2014-09-11T14:57:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2014/09/11/faster-random-samples-with-gap-sampling</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>Generating a random sample of a collection is, logically, a O(np) operation, where (n) is the sample size and (p) is the sampling probability.  For example, extracting a random sample, without replacement, from an array might look like this in pseudocode: [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Getting Started with Mesos on Fedora 21 and CentOS 7]]></title>
      <link href="http://timothysc.github.com/blog/2014/09/08/mesos-breeze/"/>
      <updated>2014-09-08T15:00:00Z</updated>
      <id>http://timothysc.github.com/blog/2014/09/08/mesos-breeze</id>
      <author>
        <name><![CDATA[Timothy St. Clair]]></name>
        <uri>http://timothysc.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p><img class="left" src="http://timothysc.github.com/images/mesos_logo.png"> [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[The Scala Iterator 'drop' Method Generates a Matryoshka Class Nesting]]></title>
      <link href="http://erikerlandson.github.com/blog/2014/09/03/matryoshka-class-construction-from-the-scala-iterator-drop-method/"/>
      <updated>2014-09-04T00:23:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2014/09/03/matryoshka-class-construction-from-the-scala-iterator-drop-method</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>The Scala Iterator <code>drop</code> method has a complexity bug that shows up when one calls <code>drop</code> repeatedly, for example when traversing over an iterator in a loop. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Implementing Parallel Prefix Scan as a Spark RDD Transform]]></title>
      <link href="http://erikerlandson.github.com/blog/2014/08/12/implementing-parallel-prefix-scan-as-a-spark-rdd-transform/"/>
      <updated>2014-08-12T18:37:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2014/08/12/implementing-parallel-prefix-scan-as-a-spark-rdd-transform</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>In my <a href="/blog/2014/08/09/implementing-an-rdd-scanleft-transform-with-cascade-rdds/">previous post</a>, I described how to implement the Scala <code>scanLeft</code> function as an RDD transform.  By definition <code>scanLeft</code> invokes a sequential-only prefix scan algorithm; it does not assume that either its input function <code>f</code> or its initial-value <code>z</code> can be applied in a parallel fashion.   Its companion function <code>scan</code>, however, computes a <em>parallel</em> prefix scan.  In this post I will describe an implementation of parallel prefix <code>scan</code> as an RDD transform. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Implementing an RDD scanLeft Transform With Cascade RDDs]]></title>
      <link href="http://erikerlandson.github.com/blog/2014/08/09/implementing-an-rdd-scanleft-transform-with-cascade-rdds/"/>
      <updated>2014-08-09T16:10:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2014/08/09/implementing-an-rdd-scanleft-transform-with-cascade-rdds</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>In Scala, sequence (and iterator) data types support the <code>scanLeft</code> method for computing a sequential prefix scan on sequence elements: [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[GPUs on the OSG]]></title>
      <link href="http://derekweitzel.blogspot.com/2014/06/gpus-on-osg.html"/>
      <updated>2014-06-25T18:36:00Z</updated>
      <id>tag:blogger.com,1999:blog-3007054864987759910.post-6396978665247392483</id>
      <author>
        <name><![CDATA[Derek Weitzel]]></name>
        <uri>http://derekweitzel.blogspot.com</uri>
      </author>
      <content type="html"><![CDATA[]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Submitting jobs to HTCondor using Python]]></title>
      <link href="http://osgtech.blogspot.com/2014/03/submitting-jobs-to-htcondor-using-python.html"/>
      <updated>2014-03-19T20:32:00Z</updated>
      <id>tag:blogger.com,1999:blog-8803173202887660937.post-689352737761121268</id>
      <author>
        <name><![CDATA[Brian Bockelman]]></name>
        <uri>http://osgtech.blogspot.com</uri>
      </author>
      <content type="html"><![CDATA[]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Moving from a Globus to an HTCondor Compute Element]]></title>
      <link href="http://derekweitzel.blogspot.com/2014/02/moving-from-globus-to-htcondor-compute.html"/>
      <updated>2014-02-27T23:05:00Z</updated>
      <id>tag:blogger.com,1999:blog-3007054864987759910.post-7950648301232917905</id>
      <author>
        <name><![CDATA[Derek Weitzel]]></name>
        <uri>http://derekweitzel.blogspot.com</uri>
      </author>
      <content type="html"><![CDATA[]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Hot Rod Hadoop with Tachyon on Fedora 21 (Rawhide)]]></title>
      <link href="http://timothysc.github.com/blog/2014/02/17/bdas-tachyon/"/>
      <updated>2014-02-17T16:00:00Z</updated>
      <id>http://timothysc.github.com/blog/2014/02/17/bdas-tachyon</id>
      <author>
        <name><![CDATA[Timothy St. Clair]]></name>
        <uri>http://timothysc.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p><img class="left" src="http://timothysc.github.com/images/Tachyon.jpg" title="" > [...]</p>
]]></content>
    </entry>
  
</feed>
