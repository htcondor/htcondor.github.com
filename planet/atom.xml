<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[Planet HTCondor Meta Feed]]></title>
  <link href="http://htcondor.github.com/planet/atom.xml" rel="self"/>
  <link href="http://htcondor.github.com/"/>
  <updated>2013-06-18T16:32:52-07:00</updated>
  <id>http://htcondor.github.com/planet/atom.xml</id>
  <author>
    <name><![CDATA[HTCondor Project]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
    <entry>
      <title type="html"><![CDATA[Leveraging systemd cgroup integration to provide SLAs on Fedora 18 & 19]]></title>
      <link href="http://timothysc.github.com/blog/2013/06/14/systemd-cgroup-sla/"/>
      <updated>2013-06-14T15:00:00Z</updated>
      <id>http://timothysc.github.com/blog/2013/06/14/systemd-cgroup-sla</id>
      <author>
        <name><![CDATA[Timothy St. Clair]]></name>
        <uri>http://timothysc.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<h2 id="background">Background</h2>
<p>In the not-so-distant past, enterprise data centers would create silos for specific 
services to gaurentee some metric of performance, or “Service Level Agreement” (SLA).  However,
this approach can be costly to create and maintain.</p>

<p>Enter the modern era of cloud computing, and one might wonder, “Why not just put it in VM?”.  For 
some use cases this might work just fine, because the metrics are “good-enough”.   Despite this flexibility, 
there are many cases where this approach simply won’t meet some measure of performance.  I won’t elaborate 
on the details, but it doesn’t take Schrödinger math to figure this out, because sometimes the cat 
is dead even before you peak into the box. ;-) </p>

<p>Thus, in this post we will explore leveraging systemd cgroup integration to provide SLAs on Fedora. </p>

<hr />

<h2 id="references">References</h2>

<ul>
  <li><a href="http://searchitchannel.techtarget.com/definition/service-level-agreement">Definition SLA</a></li>
  <li><a href="https://access.redhat.com/site/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Resource_Management_Guide/">Redhat Cgroup Documentation</a></li>
  <li><a href="http://0pointer.de/blog/projects/resources.html">Resource Management with Systemd</a></li>
  <li><a href="http://0pointer.de/public/systemd-man/systemd.directives.html">Systemd Service Directives</a></li>
</ul>

<hr />

<h2 id="prerequisites">Prerequisites</h2>

<ul>
  <li>Fedora 18 or 19 box(es).</li>
  <li>Make certain you’ve read the references, as I may gloss over some details in this post.</li>
</ul>

<hr />

<h2 id="getting-started">Getting Started</h2>
<p>First you will need to choose a service which has been integrated with systemd that you can plan on tuning.
In this example I will use ‘condor’, but you could use any service that you desire.</p>

<pre><code> sudo yum install condor
</code></pre>

<p><strong>NOTE:</strong> You could do this with raw cgroups, but it becomes difficult to gaurentee performance unless 
every service is in a group.  So systemd does a lot of the heavy lifting for us. </p>

<p>Next you will need to determine the metrics of performance that you want to provide for that service.  For the 
purposes of simplicity, lets say we want to carve off 50% of the CPU for condor.  You can 
also play with disk-io-bandwidth and network settings too, but I think I will leave that for another post
as this can be complicated enough.</p>

<p>In order to divide up your machine you will first need to determine the existing 
shares on your machine.  This can be done by dumping the current cgroup settings to a file which 
can then be analyzed to determine the new settings. </p>

<pre><code>cgsnapshot -s &gt; cgroup_snap.conf 
</code></pre>

<p>If you have a fairly basic setup you will notice the following pattern </p>

<pre><code># Configuration file generated by cgsnapshot
mount {
    cpuset = /sys/fs/cgroup/cpuset;
    cpu = /sys/fs/cgroup/cpu,cpuacct;
    cpuacct = /sys/fs/cgroup/cpu,cpuacct;
    memory = /sys/fs/cgroup/memory;
    devices = /sys/fs/cgroup/devices;
    freezer = /sys/fs/cgroup/freezer;
    net_cls = /sys/fs/cgroup/net_cls;
    blkio = /sys/fs/cgroup/blkio;
    perf_event = /sys/fs/cgroup/perf_event;
}

group system {
    cpu {
            cpu.rt_period_us="1000000";
            cpu.rt_runtime_us="0";
            cpu.cfs_period_us="100000";
            cpu.cfs_quota_us="-1";
            cpu.shares="1024";
    }
    cpuacct {
            cpuacct.usage="147354515620554";
    }
}

group system/condor.service {
    cpu {
            cpu.rt_period_us="1000000";
            cpu.rt_runtime_us="0";
            cpu.cfs_period_us="100000";
            cpu.cfs_quota_us="-1";
            cpu.shares="1024";
    }
    cpuacct {
            cpuacct.usage="146844720798260";
    }
}

... * service look ~= 
</code></pre>

<hr />

<h2 id="analyzing-your-configuration">Analyzing your Configuration</h2>
<p>One thing you will notice is that systemd creates an implied hierarchy on your 
machine by default, where each service has an equal amount of cpu.shares.  This means 
when all services are contending for resources, each “service” gets an equal
share. </p>

<p>Lets elaborate on shares a bit.  Say you had two service S(a) = 1, and S(b) = 3 and each service 
has multiple processes all contending for CPU. </p>

<pre><code>%CPU = service.cpu.share /(sum (service shares @ level)) 
%CPU[S(a)] = 1/4 = 25% 
%CPU[S(b)] = 3/4 = 75% 
</code></pre>

<p>So now lets extend this idea and create a simple hierarchy
where there are two groups, with each group having two services:</p>

<pre><code>            Share   Overall%
Group 1     1       25%
    S(a)        1       12.5%
    S(b)        1       12.5%
Group 2     3       75%
    S(c)        3       56.25%
    S(d)        1       18.75%
</code></pre>

<p>Hopefully this should be intuitive, however it can quickly goto plaid. Therefore, it’s 
important to have a handle on how many services you have planed for a given machine, and
your intended hierarchy.  Thus the cost of reliable performance is extra complexity, which 
isn’t so bad provided you’ve done your math.</p>

<hr />

<h2 id="altering-your-configuration">Altering your Configuration</h2>

<p>So now lets provision condor such that it has 50% of the CPU.  First we need to get a count
of number of services that exist on the machine. </p>

<pre><code>$ cgsnapshot -s | grep [.]service | wc -l
30
</code></pre>

<p>As you can see from the previous example, and from the documentation, the default cpu.shares 
given to a service is 1024.  Thus if we want 50% CPU:</p>

<pre><code>.50 = condor.cpu.shares/(1024*29 + condor.cpu.shares)
512*(29) + .50*condor.cpu.shares = condor.cpu.shares
14848 = (1-.50)*condor.cpu.shares 
condor.cpu.shares = 14848/.50 = 29696
</code></pre>

<p>Now we need to plug this magic number in:</p>

<pre><code>vim /usr/lib/systemd/system/condor.service

[Service]
CPUShares=29696
</code></pre>

<p>Once we exit we will need to restart the daemon and verify it worked.</p>

<pre><code>systemctl daemon-reload
systemctl restart condor.service
cgsnapshot -s &gt; cgroup_snap_2.conf
</code></pre>

<p>Now you can compare the two configuration files.</p>

<p>Next you will want to submit a whole bunch of condor jobs, and try to load down the other services. 
To verify that your machine is behaving as expected you can run: </p>

<pre><code>systemd-cgtop
</code></pre>

<p>In this example it can be difficult when you have 30 services to accurately test that you are 
guaranteed 50% so I would recommend that the reader trim their machine down and create a 
more controlled experiment to verify. </p>

<hr />

<h2 id="in-summary">In Summary</h2>
<p>Systemd’s integration with cgroups is a many splendid thing, and when used correctly can
give administrators and developers another tool in which to help create SLAs in their datacenter.</p>

]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[ HTCondor 8.0.0 released! ( June 6, 2013 )]]></title>
      <link href="manual/v8.0/10_3Stable_Release.html"/>
      <updated>2013-06-06T05:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[The HTCondor team is pleased to announce the release of HTCondor 8.0.0.
This release is the first in the new stable series.
The major version number changed because of the project rename to HTCondor and not because of major incompatibility with the version 7.8 stable series.
This new version contains Bosco, support for EC2 Spot instances, interactive jobs, improved job sandboxing, native python API and several new tools.
A complete list of bugs fixed and features can be found in the 
Version History. HTCondor 8.0.0 binaries
and source code are available from our Downloads page.
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[ HPCwire article highlights running stochastic models on HTCondor ( June 6, 2013 )]]></title>
      <link href="http://www.hpcwire.com/hpcwire/2013-05-30/running_stochastic_models_on_htcondor.html"/>
      <updated>2013-06-06T05:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[An article published at HPCwire highlights research out of Brigham Young University with a goal to 
demonstrate an alternative model to High Performance Computing (HPC) for water resource stakeholders by leveraging
High Throughput Computing (HTC) with HTCondor.
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[ Open Science Grid releases Bosco 1.2, based on HTCondor ( June 6, 2013 )]]></title>
      <link href="http://bosco.opensciencegrid.org/2013/06/bosco-1-2-release/"/>
      <updated>2013-06-06T05:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[Bosco is a client for Linux and Mac operating systems for submitting jobs to 
remote batch systems without administrator assistance. It is designed
for end-users, and only requires ssh access to one or more cluster front-ends.
Target clusters can be HTCondor, LSF, PBS, SGE or SLURM managed resources.  
The new Bosco 1.2 release is much easier to install, will handle more jobs, 
will send clearer error messages, and makes it easier to specify the memory
you need inside the clusters you connect to.  
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[ Atlas project at CERN describes their computing environment ( May 29, 2013 )]]></title>
      <link href="http://www.hpcwire.com/hpcwire/2013-05-21/cern_google_and_the_future_of_global_science_initiatives.html"/>
      <updated>2013-05-29T05:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[The HPCwire: CERN, Google Drive Future of Global Science Initiatives article describes the computing environment of 
the ATLAS project at CERN.  
HTCondor and now Google Compute Engine aid the extensive collision 
analysis effort for ATLAS.
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Submitting R jobs with Bosco]]></title>
      <link href="http://derekweitzel.blogspot.com/2013/05/submitting-r-jobs-with-bosco.html"/>
      <updated>2013-05-20T14:51:00Z</updated>
      <id>tag:blogger.com,1999:blog-3007054864987759910.post-9067247722276578550</id>
      <author>
        <name><![CDATA[Derek Weitzel]]></name>
        <uri>http://derekweitzel.blogspot.com</uri>
      </author>
      <content type="html"><![CDATA[The Bosco team has been working on integrating with the <a href="http://www.r-project.org/">R</a> statistics processing language. &nbsp;We have&nbsp;chosen&nbsp;to modify the <a href="http://cran.r-project.org/web/packages/GridR/index.html">GridR</a> package in order to integrate with R.<br /><br /><h3>How will the R user see Bosco?</h3><div>The goal of the integration is to simplify the method of submitting processing, written in the R&nbsp;language, to remote clusters and grids. &nbsp;The expected steps for the integration are:</div><div><ol><li>Install Bosco</li><li>Install the Bosco'ified GridR package into your local R environment.</li></ol></div><div>After installing the 2 pieces of software above, the user creates a R script, which includes the 'function' that is to be executed on the remote cluster. &nbsp;The user can send any data as input, lists, tables, an entire CSV file (already read into a R variable). &nbsp;The function output will be automatically imported into the environment when the remote job has completed.</div><div><br /></div><div>Below is a demo of the GridR package working with Bosco to submit to a campus cluster here at Nebraska.</div><div><br /></div><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://4.bp.blogspot.com/-WJlIyBopEBk/UZo2UDWMOCI/AAAAAAAAB8E/8jP712IKSMQ/s1600/Screen+Shot+2013-05-18+at+12.55.40+AM.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="433" src="http://4.bp.blogspot.com/-WJlIyBopEBk/UZo2UDWMOCI/AAAAAAAAB8E/8jP712IKSMQ/s640/Screen+Shot+2013-05-18+at+12.55.40+AM.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;"><a href="http://www.rstudio.com/">RStudio</a> IDE showing demo of Bosco + GridR integration</td></tr></tbody></table><div>The steps in the demo are:</div><div><ol><li>Load the GridR library</li><li>Create the function, in this case named simply 'a' that doubles the value of the argument.</li><li>Initialize the GridR integration to talk to Bosco</li><li>"Apply" the function. &nbsp;Run the function 'a', with the input 14, and write the result to the variable "x". &nbsp;Also, wait for the remote job to complete.</li><li>Finally, I printed out the value of x, which is 28, double the 14.&nbsp;</li></ol><div>This is a very simple demo. &nbsp;You could imagine the function sent to the remote machine could parse the a CSV file, or more complex operations...</div></div><div><br /></div><div>The Bosco team expects to have this integration done and in production by Mid-July for the <a href="http://161.67.142.97/congresos/useR-2013/%E2%80%8E">R users meeting</a>.</div><br /><center> <a href="http://bosco.opensciencegrid.org/download/">     <img alt="Bosco Download" src="https://raw.github.com/osg-bosco/bosco-download-images/master/images/download-orange.png" style="border-width: 0;" /> </a>   </center>]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[ HTCondor 7.9.6 released! ( May 08, 2013 )]]></title>
      <link href="manual/v7.9/9_3Development_Release.html"/>
      <updated>2013-05-08T05:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[The HTCondor team is pleased to announce the release of HTCondor 7.9.6.
This is the final release of the 7.9 series. This release contains new
tools, VMware player support, Linux out of memory killer hints, python
bindings, new configuration parameters, and many bug fixes.
A complete list of bugs fixed and features can be found in the
Version History. HTCondor 7.9.6 binaries
and source code are available from our Downloads page.
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Configuring a Personal Hadoop Development Environment on Fedora 18]]></title>
      <link href="http://timothysc.github.com/blog/2013/04/22/personalhadoop/"/>
      <updated>2013-04-22T15:00:00Z</updated>
      <id>http://timothysc.github.com/blog/2013/04/22/personalhadoop</id>
      <author>
        <name><![CDATA[Timothy St. Clair]]></name>
        <uri>http://timothysc.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<h2 id="background">Background</h2>
<p>The following post outlines a setup and configuration of a “personal hadoop” development environment that is much akin to a “personal condor” setup.
The primary purpose is to have a single source for configuration and logs along with a soft-link to development built binaries such that switching 
to a different build is a matter of updating a soft-link while maintaining all other data and configuration. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Installing Spark on Fedora 18]]></title>
      <link href="http://chapeau.freevariable.com/2013/04/installing-spark-on-fedora-18.html"/>
      <updated>2013-04-11T21:37:28Z</updated>
      <id>tag:chapeau.freevariable.com,2013://1.39</id>
      <author>
        <name><![CDATA[Will Benton]]></name>
        <uri>http://willbenton.com</uri>
      </author>
      <content type="html"><![CDATA[The Spark project is an actively-developed open-source engine for data analytics on clusters using Scala, Python, or Java. It offers map, filter, and reduce operations over in-memory collections, data from local files, or data taken from HDFS, but unlike standard...]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Reprocessing CMS events with Bosco]]></title>
      <link href="http://derekweitzel.blogspot.com/2013/04/reprocessing-cms-events-with-bosco.html"/>
      <updated>2013-04-02T19:38:00Z</updated>
      <id>tag:blogger.com,1999:blog-3007054864987759910.post-9221383956864186778</id>
      <author>
        <name><![CDATA[Derek Weitzel]]></name>
        <uri>http://derekweitzel.blogspot.com</uri>
      </author>
      <content type="html"><![CDATA[]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[The Impact of Negotiator Cycle Cadence on Slot Loading]]></title>
      <link href="http://erikerlandson.github.com/blog/2013/03/21/the-impact-of-negotiator-cycle-cadence-on-slot-loading/"/>
      <updated>2013-03-21T22:10:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2013/03/21/the-impact-of-negotiator-cycle-cadence-on-slot-loading</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>The <a href="http://research.cs.wisc.edu/htcondor/manual/v7.8/3_1Introduction.html#8555">HTCondor negotiator</a> assigns jobs (resource requests) to slots (compute resources) at regular intervals, configured by the <a href="http://research.cs.wisc.edu/htcondor/manual/v7.8/3_3Configuration.html#20544">NEGOTIATOR_INTERVAL</a> parameter.  This interval (the cycle <em>cadence</em>) has a fundamental impact on a pool <em>loading factor</em> -- the fraction of time that slots are being productively utilized. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Smooth Gradients for Cubic Hermite Splines]]></title>
      <link href="http://erikerlandson.github.com/blog/2013/03/16/smooth-gradients-for-cubic-hermite-splines/"/>
      <updated>2013-03-16T14:39:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2013/03/16/smooth-gradients-for-cubic-hermite-splines</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>One of the advantages of cubic Hermite splines is that their interval interpolation formula is an explicit function of gradients \( m_0, m_1, ... m_{n-1} \) at knot-points: [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Examining the Modulus of Random Variables]]></title>
      <link href="http://erikerlandson.github.com/blog/2013/03/15/examining-the-modulus-of-random-variables/"/>
      <updated>2013-03-15T19:03:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2013/03/15/examining-the-modulus-of-random-variables</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<h3>Motivation</h3> [...]
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Running Quantum Espresso on the OSG]]></title>
      <link href="http://derekweitzel.blogspot.com/2013/03/running-quantum-espresso-on-osg.html"/>
      <updated>2013-03-05T18:53:00Z</updated>
      <id>tag:blogger.com,1999:blog-3007054864987759910.post-2337400839561942722</id>
      <author>
        <name><![CDATA[Derek Weitzel]]></name>
        <uri>http://derekweitzel.blogspot.com</uri>
      </author>
      <content type="html"><![CDATA[]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Per-Process Mount Namespaces]]></title>
      <link href="http://timothysc.github.com/blog/2013/02/22/perprocess/"/>
      <updated>2013-02-22T16:00:00Z</updated>
      <id>http://timothysc.github.com/blog/2013/02/22/perprocess</id>
      <author>
        <name><![CDATA[Timothy St. Clair]]></name>
        <uri>http://timothysc.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<h2 id="background">Background</h2>
<p>“Isolation” in modern computing comes in many flavors and functions.  Virtual machines, c-groups, chroots/jails,
sanboxing, and namespaces all have a role to play.  In this post we will review process mount namespace isolation, which allows a process to isolate 
its mount points from the outside world.  Furthermore, it enables cleanup of that namespace, which is highly useful in grid applications.   [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Statistic changes in HTCondor 7.7]]></title>
      <link href="http://spinningmatt.wordpress.com/2013/02/12/statistic-changes-in-htcondor-7-7/"/>
      <updated>2013-02-12T11:56:04Z</updated>
      <id>http://spinningmatt.wordpress.com/?p=925</id>
      <author>
        <name><![CDATA[Matthew Farrellee]]></name>
        <uri>http://spinningmatt.wordpress.com</uri>
      </author>
      <content type="html"><![CDATA[Notice to HTCondor 7.8 users - Statistics implemented during the 7.5 series that landed in 7.7.0 were rewritten by the time 7.8 was released. If you were using the original statistics for monitoring and/or reporting, here is a table to help you map old (left column) to new (right column). See – 7.6 -&#62; 7.8 [&#8230;]<img alt="" border="0" src="http://stats.wordpress.com/b.gif?host=spinningmatt.wordpress.com&#038;blog=6870579&#038;post=925&#038;subd=spinningmatt&#038;ref=&#038;feed=1" width="1" height="1" />]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Using Bosco to submit to Amazon EC2]]></title>
      <link href="http://derekweitzel.blogspot.com/2013/02/using-bosco-to-submit-to-amazon-ec2.html"/>
      <updated>2013-02-06T04:27:00Z</updated>
      <id>tag:blogger.com,1999:blog-3007054864987759910.post-4538100752405939638</id>
      <author>
        <name><![CDATA[Derek Weitzel]]></name>
        <uri>http://derekweitzel.blogspot.com</uri>
      </author>
      <content type="html"><![CDATA[]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[How accounting group configuration could work with Wallaby]]></title>
      <link href="http://spinningmatt.wordpress.com/2013/02/05/how-accounting-group-configuration-could-work-with-wallaby/"/>
      <updated>2013-02-05T11:46:28Z</updated>
      <id>http://spinningmatt.wordpress.com/?p=917</id>
      <author>
        <name><![CDATA[Matthew Farrellee]]></name>
        <uri>http://spinningmatt.wordpress.com</uri>
      </author>
      <content type="html"><![CDATA[Configuration of accounting groups in HTCondor is too often an expert task that requires coordination between administrators and their tools. Wallaby provides a coordination point, so long as a little convention is employed, and can provide a task specific interface to simplify configuration. Quick background, Wallaby provides semantic configuration for HTCondor. It models a pool [&#8230;]<img alt="" border="0" src="http://stats.wordpress.com/b.gif?host=spinningmatt.wordpress.com&#038;blog=6870579&#038;post=917&#038;subd=spinningmatt&#038;ref=&#038;feed=1" width="1" height="1" />]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Some htcondor-wiki stats]]></title>
      <link href="http://spinningmatt.wordpress.com/2013/01/29/some-htcondor-wiki-stats/"/>
      <updated>2013-01-29T11:36:06Z</updated>
      <id>http://spinningmatt.wordpress.com/?p=903</id>
      <author>
        <name><![CDATA[Matthew Farrellee]]></name>
        <uri>http://spinningmatt.wordpress.com</uri>
      </author>
      <content type="html"><![CDATA[A few years ago I discovered Web Numbr, a service that will monitor a web page for a number and graph that number over time. I installed a handful of webnumbrs to track things at HTCondor&#8217;s gittrac instance. http://webnumbr.com/search?query=condor Thing such as - Tickets resolved with no destination: tickets that don&#8217;t indicate what version they [&#8230;]<img alt="" border="0" src="http://stats.wordpress.com/b.gif?host=spinningmatt.wordpress.com&#038;blog=6870579&#038;post=903&#038;subd=spinningmatt&#038;ref=&#038;feed=1" width="1" height="1" />]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Introducing the HTCondor-CE]]></title>
      <link href="http://osgtech.blogspot.com/2013/01/introducing-htcondor-ce.html"/>
      <updated>2013-01-28T15:33:00Z</updated>
      <id>tag:blogger.com,1999:blog-8803173202887660937.post-1124494645797252707</id>
      <author>
        <name><![CDATA[Brian Bockelman]]></name>
        <uri>http://osgtech.blogspot.com</uri>
      </author>
      <content type="html"><![CDATA[]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Concurrency Limits: Group defaults]]></title>
      <link href="http://spinningmatt.wordpress.com/2013/01/21/concurrency-limits-group-defaults/"/>
      <updated>2013-01-21T12:47:07Z</updated>
      <id>http://spinningmatt.wordpress.com/?p=895</id>
      <author>
        <name><![CDATA[Matthew Farrellee]]></name>
        <uri>http://spinningmatt.wordpress.com</uri>
      </author>
      <content type="html"><![CDATA[Concurrency limits allow for protecting resources by providing a way to cap the number of jobs requiring a specific resource that can run at one time. For instance, limit licenses and filer access at four regional data centers. Notice the repetition. In addition to the repetition, every license.* and filer.* must be known and recorded [&#8230;]<img alt="" border="0" src="http://stats.wordpress.com/b.gif?host=spinningmatt.wordpress.com&#038;blog=6870579&#038;post=895&#038;subd=spinningmatt&#038;ref=&#038;feed=1" width="1" height="1" />]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Bosco 1.1.1 Release]]></title>
      <link href="http://derekweitzel.blogspot.com/2013/01/bosco-111-release.html"/>
      <updated>2013-01-14T18:59:00Z</updated>
      <id>tag:blogger.com,1999:blog-3007054864987759910.post-1926902995750303001</id>
      <author>
        <name><![CDATA[Derek Weitzel]]></name>
        <uri>http://derekweitzel.blogspot.com</uri>
      </author>
      <content type="html"><![CDATA[]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Your API is a feature, give it real resource management]]></title>
      <link href="http://spinningmatt.wordpress.com/2013/01/14/your-api-is-a-feature-give-it-real-resource-management/"/>
      <updated>2013-01-14T12:17:29Z</updated>
      <id>http://spinningmatt.wordpress.com/?p=872</id>
      <author>
        <name><![CDATA[Matthew Farrellee]]></name>
        <uri>http://spinningmatt.wordpress.com</uri>
      </author>
      <content type="html"><![CDATA[So much these days is about distributed resource management. That&#8217;s anything that can be created and destroyed in the cloud[0]. Proper management is especially important when the resource&#8217;s existence is tied to a real economy, e.g. your user&#8217;s credit card[1]. Above is a state machine required to ensure that resources created in AWS EC2 are [&#8230;]<img alt="" border="0" src="http://stats.wordpress.com/b.gif?host=spinningmatt.wordpress.com&#038;blog=6870579&#038;post=872&#038;subd=spinningmatt&#038;ref=&#038;feed=1" width="1" height="1" />]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Fun with ClassAds]]></title>
      <link href="http://osgtech.blogspot.com/2013/01/fun-with-classads.html"/>
      <updated>2013-01-05T22:58:00Z</updated>
      <id>tag:blogger.com,1999:blog-8803173202887660937.post-1674994974092153801</id>
      <author>
        <name><![CDATA[Brian Bockelman]]></name>
        <uri>http://osgtech.blogspot.com</uri>
      </author>
      <content type="html"><![CDATA[]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[The Mean of the Modulus Does Not Equal the Modulus of the Mean]]></title>
      <link href="http://erikerlandson.github.com/blog/2013/01/02/the-mean-of-the-modulus-does-not-equal-the-modulus-of-the-mean/"/>
      <updated>2013-01-02T15:55:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2013/01/02/the-mean-of-the-modulus-does-not-equal-the-modulus-of-the-mean</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>I've been considering models for the effects of HTCondor negotiation cycle cadence on pool loading and accounting group starvation, which led me to thinking about the effects of taking the modulus of a random variable, for reasons I plan to discuss in future posts. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[A Demonstration of Negotiator-Side Resource Consumption]]></title>
      <link href="http://erikerlandson.github.com/blog/2012/12/03/a-demonstration-of-negotiator-side-resource-consumption/"/>
      <updated>2012-12-03T15:25:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2012/12/03/a-demonstration-of-negotiator-side-resource-consumption</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>HTCondor supports a notion of aggregate compute resources known as partitionable slots (p-slots), which may be consumed by multiple jobs.   Historically, at most one job could be matched against such a slot in a single negotiation cycle, which limited the rate at which partitionable slot resources could be utilized.  More recently, the scheduler has been enhanced with logic to allow it to acquire multiple claims against a partitionable slot, which increases the p-slot utilization rate. However, as this potentially bypasses the negotiator's accounting of global pool resources such as accounting group quotas and concurrency limits, it places some contraints on what jobs can can safely acquire multiple claims against any particular p-slot: for example, only other jobs on the same scheduler can be considered.  Additionally, candidate job requirements must match the requirements of the job that originally matched in the negotiator.  Another significant impact is that the negotiator is still forced to match an entire p-slot, which may have a large match cost (weight): these large match costs cause <a href="https://htcondor-wiki.cs.wisc.edu/index.cgi/tktview?tn=3013">accounting difficulties</a> when submitter shares and/or group quotas drop below the cost of a slot.  This particular problem is growing steadily larger, as machines with ever-larger numbers of cores and other resources appear in HTCondor pools. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Role enforcement in Cumin]]></title>
      <link href="http://tmckayus.github.com/blog/2012/11/12/role-enforcement-in-cumin/"/>
      <updated>2012-11-12T20:20:00Z</updated>
      <id>http://tmckayus.github.com/blog/2012/11/12/role-enforcement-in-cumin</id>
      <author>
        <name><![CDATA[Trevor McKay]]></name>
        <uri>http://tmckayus.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>Roles in Cumin scope activities and content in the UI.  There are currently two roles defined in Cumin, <code>admin</code> and <code>user</code>.  The <code>admin</code> role is a superset of the <code>user</code> role, and every new account has the <code>user</code> role by default. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Override HTCondor installation with sudo]]></title>
      <link href="http://timothysc.github.com/blog/2012/11/12/condor-sudo/"/>
      <updated>2012-11-12T09:00:00Z</updated>
      <id>http://timothysc.github.com/blog/2012/11/12/condor-sudo</id>
      <author>
        <name><![CDATA[Timothy St. Clair]]></name>
        <uri>http://timothysc.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<h2 id="background">Background</h2>
<p>As a developer, I often find myself wanting to iterate on a build, and test in a sandboxed environment.  Isolating the environment ensures that your changes work well in a controlled experiment, but it has one fundamental flaw, it’s not realistic.  In order to truly test a complicated system it’s best to put it into some production environment.  This is great for testing, but it has been known to give admins a headache, because you are mucking with a known good installation.  So in this post we will set about the task of overriding an installtion of HTCondor using sudo while keeping the following requirements in mind: [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Best practices for Wallaby's default group]]></title>
      <link href="http://chapeau.freevariable.com/2012/11/best-practices-for-wallabys-default-group.html"/>
      <updated>2012-11-01T20:14:53Z</updated>
      <id>tag:chapeau.freevariable.com,2012://1.38</id>
      <author>
        <name><![CDATA[Will Benton]]></name>
        <uri>http://willbenton.com</uri>
      </author>
      <content type="html"><![CDATA[Recall that Wallaby applies partial configurations to groups of nodes. Groups can be either explicit —- that is, a named subset of nodes created by the user, or special groups that are built-in to Wallaby; each node’s group memberships have...]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Welcome To The HTCondor Project Github Site]]></title>
      <link href="http://htcondor.github.com/blog/2012/10/29/welcome-to-the-condor-project-github-site/"/>
      <updated>2012-10-29T20:15:00Z</updated>
      <id>http://htcondor.github.com/blog/2012/10/29/welcome-to-the-condor-project-github-site</id>
      <author>
        <name><![CDATA[HTCondor Team GitHub]]></name>
        <uri>http://htcondor.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>Welcome to the HTCondor Project GitHub website!  This site is the github web and blog presence for the HTCondor project. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Configuring high-availability Condor central managers with Wallaby]]></title>
      <link href="http://chapeau.freevariable.com/2012/10/configuring-high-availability-condor-central-managers-with-wallaby.html"/>
      <updated>2012-10-23T04:34:58Z</updated>
      <id>tag:chapeau.freevariable.com,2012://1.37</id>
      <author>
        <name><![CDATA[Will Benton]]></name>
        <uri>http://willbenton.com</uri>
      </author>
      <content type="html"><![CDATA[Rob Rati and I gave a tutorial on highly-available job queues at Condor Week this year. While it was not a Wallaby-specific tutorial, we did point out that configuring highly-available job queues is easier for users who manage and deploy...]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Using Cluster Suite's GUI to configure High Availability Schedulers ]]></title>
      <link href="http://rrati.github.com/blog/2012/10/18/using-cluster-suites-gui-to-configure-high-availability-schedulers/"/>
      <updated>2012-10-18T17:20:00Z</updated>
      <id>http://rrati.github.com/blog/2012/10/18/using-cluster-suites-gui-to-configure-high-availability-schedulers</id>
      <author>
        <name><![CDATA[Robert Rati]]></name>
        <uri>http://rrati.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>In an <a href="http://rrati.github.com/blog/2012/09/26/using-cluster-suite-to-manage-a-high-availability-scheduler/">earlier post</a> I talked about using Cluster Suite
to manage high availability schedulers and referenced the command line tools
available perform the configuration.  I'd like to focus on using the GUI that
is part of Cluster Suite to configure an HA schedd.  It's a pretty simple
process but does require you run a wallaby shell command to complete the
configuration. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Credentials in LDAP URLs when Anonymous Search is Disabled]]></title>
      <link href="http://tmckayus.github.com/blog/2012/10/10/ldap-credentials/"/>
      <updated>2012-10-10T20:55:00Z</updated>
      <id>http://tmckayus.github.com/blog/2012/10/10/ldap-credentials</id>
      <author>
        <name><![CDATA[Trevor McKay]]></name>
        <uri>http://tmckayus.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>Cumin authenticates logins against LDAP using a two step process: [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Using Cluster Suite to Manage a High Availability Scheduler]]></title>
      <link href="http://rrati.github.com/blog/2012/09/26/using-cluster-suite-to-manage-a-high-availability-scheduler/"/>
      <updated>2012-09-26T19:53:00Z</updated>
      <id>http://rrati.github.com/blog/2012/09/26/using-cluster-suite-to-manage-a-high-availability-scheduler</id>
      <author>
        <name><![CDATA[Robert Rati]]></name>
        <uri>http://rrati.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>Condor provides simple and easy to configure HA functionality for the schedd
that relies upon shared storage (usually NFS).  The shared store is used to
store the job queue log and coordinate which node is running the schedd.  This
means that each node that can run a particular schedd not only have condor
configured but the node needs to be configured to access the shared storage. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Integrating Cumin with LDAP for Authentication]]></title>
      <link href="http://tmckayus.github.com/blog/2012/09/24/ldap-auth/"/>
      <updated>2012-09-24T16:41:00Z</updated>
      <id>http://tmckayus.github.com/blog/2012/09/24/ldap-auth</id>
      <author>
        <name><![CDATA[Trevor McKay]]></name>
        <uri>http://tmckayus.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>Past versions of Cumin have relied on a local database for storing user accounts.  However, that solution adds extra maintenance for site administrators who already have or plan to have a central authentication mechanism for their users.  Consequently, development is ongoing to integrate Cumin with common central auth mechanisms.  LDAP integration is available now, with support for other technologies planned for the future. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[So What is Cumin Anyway?]]></title>
      <link href="http://tmckayus.github.com/blog/2012/09/24/new-post/"/>
      <updated>2012-09-24T16:07:00Z</updated>
      <id>http://tmckayus.github.com/blog/2012/09/24/new-post</id>
      <author>
        <name><![CDATA[Trevor McKay]]></name>
        <uri>http://tmckayus.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>Cumin is a Python web UI developed in the Fedora community for managing Condor pools and Qpid messaging brokers.  It is packaged for Fedora but may be run from sources and would probably be easy to port to other Linux distributions (or just run Fedora on a node or two in a heterogeneous environment!)  The current development focus for Cumin is on expanding the Condor management facilities. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Elastic Grid with Condor and oVirt Integration]]></title>
      <link href="http://timothysc.github.com/blog/2012/09/21/condor-n-overt/"/>
      <updated>2012-09-21T08:50:00Z</updated>
      <id>http://timothysc.github.com/blog/2012/09/21/condor-n-overt</id>
      <author>
        <name><![CDATA[Timothy St. Clair]]></name>
        <uri>http://timothysc.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<h2 id="background">Background</h2>
<p>Gone are the days where an IT administrator could procure a dedicated compute cluster for a single task, so it is often the case where admins are asked to do more with existing resources where possible, especially those which are underutilized.  There are several existing solutions to oversubscription, but few that remain “general purpose” while adapting to the environment as the load within the cluster changes.  Enter Condor, which has been most well known for its batch processing capabilities, but can also be leveraged in many ways as an IaaS tool when coupled with oVirt.  [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Putting It Together]]></title>
      <link href="http://rrati.github.com/blog/2012/09/18/putting-it-together/"/>
      <updated>2012-09-18T12:59:00Z</updated>
      <id>http://rrati.github.com/blog/2012/09/18/putting-it-together</id>
      <author>
        <name><![CDATA[Robert Rati]]></name>
        <uri>http://rrati.github.com/</uri>
      </author>
      <content type="html"><![CDATA[]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Authorization for Wallaby clients]]></title>
      <link href="http://getwallaby.com/2012/09/authorization-for-wallaby-clients/"/>
      <updated>2012-09-12T22:30:00Z</updated>
      <id>http://getwallaby.com/2012/09/authorization-for-wallaby-clients</id>
      <author>
        <name><![CDATA[William Benton]]></name>
        <uri>http://getwallaby.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>Wallaby 0.16.0, which updates the Wallaby API version to 20101031.6, includes support for authorizing broker users with various roles that can interact with Wallaby in different ways.  This post will explain how the authorization support works and show how to get started using it.  If you just want to get started using Wallaby with authorization support as quickly as possible, skip ahead to the section titled &#8220;Getting Started&#8221; below.  Detailed information about which role is required for each Wallaby API method is <a href="http://getwallaby.com/api-roles/">available here</a>. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Authorization for Wallaby clients]]></title>
      <link href="http://chapeau.freevariable.com/2012/09/authorization-for-wallaby-clients.html"/>
      <updated>2012-09-12T22:23:18Z</updated>
      <id>tag:chapeau.freevariable.com,2012://1.36</id>
      <author>
        <name><![CDATA[Will Benton]]></name>
        <uri>http://willbenton.com</uri>
      </author>
      <content type="html"><![CDATA[Wallaby 0.16.0, which updates the Wallaby API version to 20101031.6, includes support for authorizing broker users with various roles that can interact with Wallaby in different ways. This post will explain how the authorization support works and show how to...]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Highly-available configuration data with Wallaby]]></title>
      <link href="http://chapeau.freevariable.com/2012/08/highly-available-configuration-data-with-wallaby.html"/>
      <updated>2012-08-29T21:03:00Z</updated>
      <id>tag:chapeau.freevariable.com,2012://1.35</id>
      <author>
        <name><![CDATA[Will Benton]]></name>
        <uri>http://willbenton.com</uri>
      </author>
      <content type="html"><![CDATA[Many Condor users are interested in high-availability (HA) services: they don't want their compute resources to become unavailable due to the failure of a single machine that is running an important Condor daemon. (See this talk that Rob Rati and...]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Highly-available configuration data with Wallaby]]></title>
      <link href="http://getwallaby.com/2012/08/live-backup/"/>
      <updated>2012-08-29T14:40:00Z</updated>
      <id>http://getwallaby.com/2012/08/live-backup</id>
      <author>
        <name><![CDATA[William Benton]]></name>
        <uri>http://getwallaby.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>Many Condor users are interested in <em>high-availability</em> (HA) services:  they don&#8217;t want their compute resources to become unavailable due to the failure of a single machine that is running an important Condor daemon.  (See <a href="http://research.cs.wisc.edu/condor/CondorWeek2012/presentations/rati-benton-condor-ha.pdf">this talk</a> that Rob Rati and I gave at Condor Week this year for a couple of solutions to HA with the Condor <code>schedd</code>.)  So it&#8217;s only natural that Condor users who are interested in configuring their pools with <a href="http://getwallaby.com">Wallaby</a> might wonder how Wallaby responds in the face of failure. [...]</p>
]]></content>
    </entry>
  
</feed>
