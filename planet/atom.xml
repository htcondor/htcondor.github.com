<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[Planet HTCondor Meta Feed]]></title>
  <link href="http://htcondor.github.com/planet/atom.xml" rel="self"/>
  <link href="http://htcondor.github.com/"/>
  <updated>2015-04-26T09:49:45-07:00</updated>
  <id>http://htcondor.github.com/planet/atom.xml</id>
  <author>
    <name><![CDATA[HTCondor Project]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
    <entry>
      <title type="html"><![CDATA[Effective continuous integration for Spark projects]]></title>
      <link href="http://chapeau.freevariable.com/2015/04/spark-apps-and-ci.html"/>
      <updated>2015-04-21T20:57:24Z</updated>
      <id>http://chapeau.freevariable.com/2015/04/spark-apps-and-ci</id>
      <author>
        <name><![CDATA[William Benton]]></name>
        <uri>http://chapeau.freevariable.com/</uri>
      </author>
      <content type="html"><![CDATA[<p><a href="http://silex.freevariable.com">Silex</a> is a small library of helper code intended to make it easier to build real-world Spark applications;<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> most of it is factored out from applications we&rsquo;ve developed internally at Red Hat.  We have a couple of long-term goals for the Silex project:</p>

<ul>
<li>We want to make it very easy for us to spin up on new data-processing problems without spending a lot of time dealing with accidental complexity, and</li>
<li>we want to have a generally-useful standard library atop Spark that provides primitives and solutions to simplify common tasks and reflects best practices as we discover them.</li>
</ul>


<p>Part of having a library that people will want to use is having a quality test suite and continuous integration.  There are some tricks to developing automated tests for Spark applications, but even keeping those in mind may not be sufficient to let you test your Spark apps in a hosted CI environment.  This post will show you how to bridge the gap.</p>

<h3>Continuous integration challenges</h3>

<p>Since we want to make it easy to review contributions to Silex, I set up <a href="https://travis-ci.org/willb/silex">Travis CI</a> to watch branches and pull requests.  Travis CI is a great service, but if you&rsquo;re used to running tests locally, you might have some problems running Spark-based tests under hosted CI environments.  Here&rsquo;s what we learned:</p>

<ul>
<li>When running tests locally, creating a Spark context with <code>local[*]</code> (in order to use all of the available cores on your machine) might be a good idea.  However, a hosted CI environment may offer you a lot of cores but relatively little memory, so your tests might be killed because each Spark executor has nontrivial memory overhead.  Instead, consider limiting your code to use fewer cores.</li>
<li>If you&rsquo;re using Kryo serialization and have set the <code>spark.kryoserializer.buffer.mb</code> property to something large (perhaps because you often have serialization buffer crashes in production), don&rsquo;t be surprised if you run out of memory while running in CI.  Spark doesn&rsquo;t share serializers between threads, so you could be allocating a huge buffer for each thread even if your test code doesn&rsquo;t need to serialize anything all that large.<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup></li>
<li>Spark SQL (at least as of version 1.3.1) defaults to creating 200 partitions for shuffles.  This is probably a good starting place for real-world data, but it&rsquo;s overkill for test cases running on a single machine.  Furthermore, since each partition has some extra memory overhead, it&rsquo;s another possible cause of OOMEs.</li>
</ul>


<h3>Fixing the problems</h3>

<p>Silex provides an <a href="http://silex.freevariable.com/latest/api/#com.redhat.et.silex.app.AppCommon">application skeleton trait</a>, and we use a <a href="https://github.com/willb/silex/blob/c61995689ca6c76e920af64d7fe9306cdcbcc98d/src/main/scala/com/redhat/et/silex/app/app.scala#L121">class extending this trait</a> to package up Spark and SQL contexts for test cases.  That class isn&rsquo;t currently part of the public Silex API, but you can see what it looks like here:</p>

<figure class='code'><figcaption><span>excerpt from app.scala</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">private</span> <span class="o">[</span><span class="kt">silex</span><span class="o">]</span> <span class="k">class</span> <span class="nc">TestConsoleApp</span><span class="o">(</span><span class="k">val</span> <span class="n">suppliedMaster</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=</span> <span class="s">&quot;local[2]&quot;</span><span class="o">)</span> <span class="k">extends</span> <span class="nc">AppCommon</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">master</span> <span class="k">=</span> <span class="n">suppliedMaster</span>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">appName</span> <span class="k">=</span> <span class="s">&quot;console&quot;</span>
</span><span class='line'>
</span><span class='line'>  <span class="n">addConfig</span><span class="o">(</span> <span class="o">{(</span><span class="n">conf</span><span class="k">:</span> <span class="kt">SparkConf</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="o">(</span><span class="s">&quot;spark.kryoserializer.buffer.mb&quot;</span><span class="o">,</span> <span class="s">&quot;2&quot;</span><span class="o">)})</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">def</span> <span class="n">appMain</span><span class="o">(</span><span class="n">args</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span> <span class="o">{</span>
</span><span class='line'>    <span class="c1">// this never runs</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>As you can see, we allow users to specify a Spark master URL but default to using two cores locally.  Furthermore, we use the <code>addConfig</code> function from <code>AppCommon</code> &mdash; which takes a <code>SparkConf</code> and returns a modified <code>SparkConf</code> &mdash; to ensure that our Kryo buffer size is the Spark default of 2 mb, rather than the larger Silex default.</p>

<p>If you&rsquo;re used to writing test code that exercises Spark, you probably already have boilerplate (using something like ScalaTest&rsquo;s <code>BeforeAndAfterEach</code>) to set up and tear down a Spark context for each test case.  We set the Spark SQL property to control parallelism in data frame and SQL shuffles in the <a href="https://github.com/willb/silex/blob/3155174e52c392086b44c438fbf5c29d7af9fd3e/src/test/scala/com/redhat/et/silex/testing/sparkFixtures.scala">test setup code</a> itself:</p>

<figure class='code'><figcaption><span>excerpt from app.scala</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">import</span> <span class="nn">org.scalatest._</span>
</span><span class='line'>
</span><span class='line'><span class="k">import</span> <span class="nn">com.redhat.et.silex.app.TestConsoleApp</span>
</span><span class='line'>
</span><span class='line'><span class="k">trait</span> <span class="nc">PerTestSparkContext</span> <span class="k">extends</span> <span class="nc">BeforeAndAfterEach</span> <span class="o">{</span>
</span><span class='line'>  <span class="n">self</span><span class="k">:</span> <span class="kt">BeforeAndAfterEach</span> <span class="kt">with</span> <span class="kt">Suite</span> <span class="o">=&gt;</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">private</span> <span class="k">var</span> <span class="n">app</span><span class="k">:</span> <span class="kt">TestConsoleApp</span> <span class="o">=</span> <span class="kc">null</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">def</span> <span class="n">context</span> <span class="k">=</span> <span class="n">app</span><span class="o">.</span><span class="n">context</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">sqlContext</span> <span class="k">=</span> <span class="n">app</span><span class="o">.</span><span class="n">sqlContext</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">beforeEach</span><span class="o">()</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">app</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">TestConsoleApp</span><span class="o">()</span>
</span><span class='line'>    <span class="nc">System</span><span class="o">.</span><span class="n">clearProperty</span><span class="o">(</span><span class="s">&quot;spark.master.port&quot;</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">app</span><span class="o">.</span><span class="n">sqlContext</span><span class="o">.</span><span class="n">setConf</span><span class="o">(</span><span class="s">&quot;spark.sql.shuffle.partitions&quot;</span><span class="o">,</span> <span class="s">&quot;10&quot;</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">app</span><span class="o">.</span><span class="n">context</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">afterEach</span><span class="o">()</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">app</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">stop</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<h3>Conclusion</h3>

<p>Good CI is one of those things that makes it fun to develop software, but the accidental differences between a hosted environment and your local environment can be totally frustrating.  By taking into account the constraints you&rsquo;re likely to see when running in a resource-limited container on a CI server, you can help to ensure that your tests only fail when your code is actually broken.</p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p>&ldquo;Silex&rdquo; is Latin for &ldquo;flint,&rdquo; which seemed like a good name for something that was intended to help people make things with sparks.<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
<li id="fn:2">
<p>Thanks to <a href="http://erikerlandson.github.io">Erik Erlandson</a> for noticing this (it even affected him in a test case he was running locally).<a href="#fnref:2" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[ HTCondor 8.3.5 released! ( April 20, 2015 )]]></title>
      <link href="manual/v8.3.5/10_3Development_Release.html"/>
      <updated>2015-04-20T05:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[The HTCondor team is pleased to announce the release of HTCondor 8.3.5.
A development series release contains new features that are under
development. This release contains all of the bug fixes from the 8.2.8
stable release.
A few of the enhancements in this release include:
new features that increase the power of job specification in the
submit description file;
RPMs for Red Hat Enterprise Linux 6 and 7 are modularized and only
distributed via our YUM repository;
The new condor-all RPM requires
the other HTCondor RPMs of a typical HTCondor installation.
Further details can be found in the
Version History.
HTCondor 8.3.5 binaries and source code are available from our
Downloads page.
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Natural join for data frames in Spark]]></title>
      <link href="http://chapeau.freevariable.com/2015/04/natural-join-for-spark-dataframes.html"/>
      <updated>2015-04-08T21:21:21Z</updated>
      <id>http://chapeau.freevariable.com/2015/04/natural-join-for-spark-dataframes</id>
      <author>
        <name><![CDATA[William Benton]]></name>
        <uri>http://chapeau.freevariable.com/</uri>
      </author>
      <content type="html"><![CDATA[<p><a href="http://en.wikipedia.org/wiki/Relational_algebra#Natural_join_.28.E2.8B.88.29">Natural join</a> is a useful special case of the relational join operation (and is extremely common when denormalizing data pulled in from a relational database).  Spark&rsquo;s DataFrame API provides an expressive way to specify arbitrary joins, but it would be nice to have some machinery to make the simple case of natural join as easy as possible.  Here&rsquo;s what a natural join needs to do:</p>

<ol>
<li>For relations <em>R</em> and <em>S</em>, identify the columns they have in common, say <em>c1</em> and <em>c2</em>;</li>
<li>join <em>R</em> and <em>S</em> on the condition that <em>R.c1</em> == <em>S.c1</em> and <em>R.c2</em> == <em>S.c2</em>; and</li>
<li>project away the duplicated columns.</li>
</ol>


<p>so, in Spark, a natural join would look like this:</p>

<figure class='code'><figcaption><span>natJoinExample.scala</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="cm">/* r and s are DataFrames, declared elsewhere */</span>
</span><span class='line'>
</span><span class='line'><span class="k">val</span> <span class="n">joined</span> <span class="k">=</span> <span class="n">r</span><span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">s</span><span class="o">,</span> <span class="n">r</span><span class="o">(</span><span class="s">&quot;c1&quot;</span><span class="o">)</span> <span class="o">==</span> <span class="n">s</span><span class="o">(</span><span class="s">&quot;c1&quot;</span><span class="o">)</span> <span class="o">&amp;&amp;</span> <span class="n">r</span><span class="o">(</span><span class="s">&quot;c2&quot;</span><span class="o">)</span> <span class="o">==</span> <span class="n">s</span><span class="o">(</span><span class="s">&quot;c2&quot;</span><span class="o">))</span>
</span><span class='line'><span class="k">val</span> <span class="n">common</span> <span class="k">=</span> <span class="nc">Set</span><span class="o">(</span><span class="s">&quot;c1&quot;</span><span class="o">,</span> <span class="s">&quot;c2&quot;</span><span class="o">)</span>
</span><span class='line'><span class="k">val</span> <span class="n">outputColumns</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="n">r</span><span class="o">(</span><span class="s">&quot;c1&quot;</span><span class="o">),</span> <span class="n">r</span><span class="o">(</span><span class="s">&quot;c2&quot;</span><span class="o">))</span> <span class="o">++</span>
</span><span class='line'>                    <span class="n">r</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">collect</span> <span class="o">{</span> <span class="k">case</span> <span class="n">c</span> <span class="k">if</span> <span class="o">!</span><span class="n">common</span><span class="o">.</span><span class="n">contains</span><span class="o">(</span><span class="n">c</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">r</span><span class="o">(</span><span class="n">c</span><span class="o">)</span> <span class="o">}</span> <span class="o">++</span>
</span><span class='line'>                    <span class="n">s</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">collect</span> <span class="o">{</span> <span class="k">case</span> <span class="n">c</span> <span class="k">if</span> <span class="o">!</span><span class="n">common</span><span class="o">.</span><span class="n">contains</span><span class="o">(</span><span class="n">c</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">s</span><span class="o">(</span><span class="n">c</span><span class="o">)</span> <span class="o">}</span>
</span><span class='line'><span class="k">val</span> <span class="n">projected</span> <span class="k">=</span> <span class="n">joined</span><span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="n">outputColumns</span> <span class="k">:</span> <span class="k">_</span><span class="kt">*</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>We can generalize this as follows (note that joining two frames with no columns in common will produce an empty frame):</p>

<figure class='code'><figcaption><span>natjoin.scala</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">import</span> <span class="nn">org.apache.spark.sql.DataFrame</span>
</span><span class='line'><span class="k">import</span> <span class="nn">scala.language.implicitConversions</span>
</span><span class='line'>
</span><span class='line'><span class="k">trait</span> <span class="nc">NaturalJoining</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">import</span> <span class="nn">org.apache.spark.sql.functions._</span>
</span><span class='line'>  <span class="k">import</span> <span class="nn">org.apache.spark.sql.types._</span>
</span><span class='line'>
</span><span class='line'>  <span class="cm">/**</span>
</span><span class='line'><span class="cm">   * Performs a natural join of two data frames.</span>
</span><span class='line'><span class="cm">   *</span>
</span><span class='line'><span class="cm">   * The frames are joined by equality on all of the columns they have in common.</span>
</span><span class='line'><span class="cm">   * The resulting frame has the common columns (in the order they appeared in &lt;code&gt;left&lt;/code&gt;), </span>
</span><span class='line'><span class="cm">   * followed by the columns that only exist in &lt;code&gt;left&lt;/code&gt;, followed by the columns that </span>
</span><span class='line'><span class="cm">   * only exist in &lt;code&gt;right&lt;/code&gt;.</span>
</span><span class='line'><span class="cm">   */</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">natjoin</span><span class="o">(</span><span class="n">left</span><span class="k">:</span> <span class="kt">DataFrame</span><span class="o">,</span> <span class="n">right</span><span class="k">:</span> <span class="kt">DataFrame</span><span class="o">)</span><span class="k">:</span> <span class="kt">DataFrame</span> <span class="o">=</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">leftCols</span> <span class="k">=</span> <span class="n">left</span><span class="o">.</span><span class="n">columns</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">rightCols</span> <span class="k">=</span> <span class="n">right</span><span class="o">.</span><span class="n">columns</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">val</span> <span class="n">commonCols</span> <span class="k">=</span> <span class="n">leftCols</span><span class="o">.</span><span class="n">toSet</span> <span class="n">intersect</span> <span class="n">rightCols</span><span class="o">.</span><span class="n">toSet</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">if</span><span class="o">(</span><span class="n">commonCols</span><span class="o">.</span><span class="n">isEmpty</span><span class="o">)</span>
</span><span class='line'>      <span class="n">left</span><span class="o">.</span><span class="n">limit</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="n">join</span><span class="o">(</span><span class="n">right</span><span class="o">.</span><span class="n">limit</span><span class="o">(</span><span class="mi">0</span><span class="o">))</span>
</span><span class='line'>    <span class="k">else</span>
</span><span class='line'>      <span class="n">left</span>
</span><span class='line'>        <span class="o">.</span><span class="n">join</span><span class="o">(</span><span class="n">right</span><span class="o">,</span> <span class="n">commonCols</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span><span class="n">col</span> <span class="k">=&gt;</span> <span class="n">left</span><span class="o">(</span><span class="n">col</span><span class="o">)</span> <span class="o">===</span> <span class="n">right</span><span class="o">(</span><span class="n">col</span><span class="o">)</span> <span class="o">}.</span><span class="n">reduce</span><span class="o">(</span><span class="k">_</span> <span class="o">&amp;&amp;</span> <span class="k">_</span><span class="o">))</span>
</span><span class='line'>        <span class="o">.</span><span class="n">select</span><span class="o">(</span><span class="n">leftCols</span><span class="o">.</span><span class="n">collect</span> <span class="o">{</span> <span class="k">case</span> <span class="n">c</span> <span class="k">if</span> <span class="n">commonCols</span><span class="o">.</span><span class="n">contains</span><span class="o">(</span><span class="n">c</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">left</span><span class="o">(</span><span class="n">c</span><span class="o">)</span> <span class="o">}</span> <span class="o">++</span>
</span><span class='line'>                <span class="n">leftCols</span><span class="o">.</span><span class="n">collect</span> <span class="o">{</span> <span class="k">case</span> <span class="n">c</span> <span class="k">if</span> <span class="o">!</span><span class="n">commonCols</span><span class="o">.</span><span class="n">contains</span><span class="o">(</span><span class="n">c</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">left</span><span class="o">(</span><span class="n">c</span><span class="o">)</span> <span class="o">}</span> <span class="o">++</span>
</span><span class='line'>                <span class="n">rightCols</span><span class="o">.</span><span class="n">collect</span> <span class="o">{</span> <span class="k">case</span> <span class="n">c</span> <span class="k">if</span> <span class="o">!</span><span class="n">commonCols</span><span class="o">.</span><span class="n">contains</span><span class="o">(</span><span class="n">c</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">right</span><span class="o">(</span><span class="n">c</span><span class="o">)</span> <span class="o">}</span> <span class="k">:</span> <span class="k">_</span><span class="kt">*</span><span class="o">)</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Furthermore, we can make this operation available to any <code>DataFrame</code> via implicit conversions:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">case</span> <span class="k">class</span> <span class="nc">DFWithNatJoin</span><span class="o">(</span><span class="n">df</span><span class="k">:</span> <span class="kt">DataFrame</span><span class="o">)</span> <span class="k">extends</span> <span class="nc">NaturalJoining</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">def</span> <span class="n">natjoin</span><span class="o">(</span><span class="n">other</span><span class="k">:</span> <span class="kt">DataFrame</span><span class="o">)</span><span class="k">:</span> <span class="kt">DataFrame</span> <span class="o">=</span> <span class="k">super</span><span class="o">.</span><span class="n">natjoin</span><span class="o">(</span><span class="n">df</span><span class="o">,</span> <span class="n">other</span><span class="o">)</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="cm">/** </span>
</span><span class='line'><span class="cm"> * Module for natural join functionality.  Import &lt;code&gt;NaturalJoin._&lt;/code&gt; for static access </span>
</span><span class='line'><span class="cm"> * to the &lt;code&gt;natjoin&lt;/code&gt; method, or import &lt;code&gt;NaturalJoin.implicits._&lt;/code&gt; to pimp </span>
</span><span class='line'><span class="cm"> * Spark DataFrames with a &lt;code&gt;natjoin&lt;/code&gt; member method. </span>
</span><span class='line'><span class="cm"> */</span>
</span><span class='line'><span class="k">object</span> <span class="nc">NaturalJoin</span> <span class="k">extends</span> <span class="nc">NaturalJoining</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">object</span> <span class="nc">implicits</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">implicit</span> <span class="k">def</span> <span class="n">dfWithNatJoin</span><span class="o">(</span><span class="n">df</span><span class="k">:</span> <span class="kt">DataFrame</span><span class="o">)</span> <span class="k">=</span> <span class="nc">DFWithNatJoin</span><span class="o">(</span><span class="n">df</span><span class="o">)</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>If you&rsquo;re interested in using this code in your own projects, simply add <a href="https://github.com/willb/silex">the Silex library</a> to your project and <code>import com.redhat.et.silex.frame._</code>.  (You can also get Silex via <a href="https://bintray.com/willb/maven/silex/view">bintray</a>.)</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[ HTCondor 8.2.8 released! ( April 7, 2015 )]]></title>
      <link href="manual/v8.2.8/10_3Stable_Release.html"/>
      <updated>2015-04-07T05:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[The HTCondor team is pleased to announce the release of HTCondor version 8.2.8.
A stable series release contains significant bug and security fixes.
This version contains:
a bug fix to reconnect a TCP session when an HTCondorView collector restarts;
a bug fix to avoid starting too many jobs, only to kill some chosen at random.
A complete list of fixed bugs can be found in the
Version History.
HTCondor 8.2.8 binaries and source code are available from our
Downloads page.
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Interactively using Spark SQL and DataFrames from sbt projects]]></title>
      <link href="http://chapeau.freevariable.com/2015/04/spark-sql-repl.html"/>
      <updated>2015-04-02T16:12:17Z</updated>
      <id>http://chapeau.freevariable.com/2015/04/spark-sql-repl</id>
      <author>
        <name><![CDATA[William Benton]]></name>
        <uri>http://chapeau.freevariable.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>One of the great things about Apache Spark is that you can experiment with new analyses interactively.  In the past, I&rsquo;ve used the <code>sbt</code> console to try out new data transformations and models; the console is especially convenient since you <a href="http://chapeau.freevariable.com/2014/09/interactive-sbt.html">can set it up as a custom Scala REPL with your libraries loaded and some test fixtures already created</a>.</p>

<p>However, some of Spark&rsquo;s coolest new functionality depends on some aspects of Scala reflection that <a href="http://mail-archives.apache.org/mod_mbox/spark-user/201503.mbox/%3cCAAswR-62WgqYMt=HNLd3-JQmWgW5p+_6-hLDngjC3ABH1Lun0A@mail.gmail.com%3e">aren&rsquo;t compatible with how <code>sbt</code> uses classloaders for tasks</a>, so you&rsquo;re liable to see <code>MissingRequirementError</code> exceptions when you try and run code that exercises parts of Spark SQL or the DataFrame API from the <code>sbt</code> console.<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup></p>

<p>You can certainly run a regular Scala REPL or the <code>spark-shell</code>, but doing so sacrifices a lot of the flexibility of running from <code>sbt</code>:  every time your code or dependencies change, you&rsquo;ll need to package your application and set your classpath, ensuring that all of your application classes and dependencies are available to the REPL application.</p>

<p>Fortunately, there&rsquo;s an easier way:  you can make a small application that runs a Scala REPL set up the way you like and ask <code>sbt</code> how to set its classpath.  First, write up a simple custom Scala REPL, like this one:</p>

<figure class='code'><figcaption><span>ReplApp.scala</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">object</span> <span class="nc">ReplApp</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">import</span> <span class="nn">scala.tools.nsc.interpreter._</span>
</span><span class='line'>  <span class="k">import</span> <span class="nn">scala.tools.nsc.Settings</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">def</span> <span class="n">main</span><span class="o">(</span><span class="n">args</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[</span><span class="kt">String</span><span class="o">])</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">repl</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">ILoop</span> <span class="o">{</span>
</span><span class='line'>      <span class="k">override</span> <span class="k">def</span> <span class="n">loop</span><span class="o">()</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
</span><span class='line'>        <span class="c1">// ConsoleApp is just a simple container for a Spark context</span>
</span><span class='line'>        <span class="c1">// and configuration</span>
</span><span class='line'>        <span class="k">val</span> <span class="n">app</span> <span class="k">=</span> <span class="k">new</span> <span class="n">com</span><span class="o">.</span><span class="n">redhat</span><span class="o">.</span><span class="n">et</span><span class="o">.</span><span class="n">silex</span><span class="o">.</span><span class="n">app</span><span class="o">.</span><span class="nc">ConsoleApp</span><span class="o">()</span>
</span><span class='line'>        <span class="n">intp</span><span class="o">.</span><span class="n">addImports</span><span class="o">(</span><span class="s">&quot;org.apache.spark.SparkConf&quot;</span><span class="o">)</span>
</span><span class='line'>        <span class="n">intp</span><span class="o">.</span><span class="n">addImports</span><span class="o">(</span><span class="s">&quot;org.apache.spark.SparkContext&quot;</span><span class="o">)</span>
</span><span class='line'>        <span class="n">intp</span><span class="o">.</span><span class="n">addImports</span><span class="o">(</span><span class="s">&quot;org.apache.spark.SparkContext._&quot;</span><span class="o">)</span>
</span><span class='line'>        <span class="n">intp</span><span class="o">.</span><span class="n">addImports</span><span class="o">(</span><span class="s">&quot;org.apache.spark.rdd.RDD&quot;</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>        <span class="n">intp</span><span class="o">.</span><span class="n">bind</span><span class="o">(</span><span class="s">&quot;app&quot;</span><span class="o">,</span> <span class="n">app</span><span class="o">)</span>
</span><span class='line'>        <span class="n">intp</span><span class="o">.</span><span class="n">bind</span><span class="o">(</span><span class="s">&quot;spark&quot;</span><span class="o">,</span> <span class="n">app</span><span class="o">.</span><span class="n">context</span><span class="o">)</span>
</span><span class='line'>        <span class="n">intp</span><span class="o">.</span><span class="n">bind</span><span class="o">(</span><span class="s">&quot;sqlc&quot;</span><span class="o">,</span> <span class="n">app</span><span class="o">.</span><span class="n">sqlContext</span><span class="o">)</span>
</span><span class='line'>        <span class="n">intp</span><span class="o">.</span><span class="n">addImports</span><span class="o">(</span><span class="s">&quot;sqlc._&quot;</span><span class="o">)</span>
</span><span class='line'>
</span><span class='line'>        <span class="k">super</span><span class="o">.</span><span class="n">loop</span><span class="o">()</span>
</span><span class='line'>      <span class="o">}</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">val</span> <span class="n">settings</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Settings</span>
</span><span class='line'>    <span class="n">settings</span><span class="o">.</span><span class="nc">Yreplsync</span><span class="o">.</span><span class="n">value</span> <span class="k">=</span> <span class="kc">true</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">settings</span><span class="o">.</span><span class="n">usejavacp</span><span class="o">.</span><span class="n">value</span> <span class="k">=</span> <span class="kc">true</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">repl</span><span class="o">.</span><span class="n">process</span><span class="o">(</span><span class="n">settings</span><span class="o">)</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>The <code>ReplApp</code> application sets up a Scala REPL with imports for some common Spark classes and bindings to a <code>SparkContext</code> and <code>SqlContext</code>.  (The <code>ConsoleApp</code> object is just a simple wrapper for Spark context and configuration; see the <a href="https://github.com/willb/silex">Silex project</a>, where my team is collecting and generalizing infrastructure code from Spark applications, for more details &mdash; or just change this code to set up a <code>SparkContext</code> as you see fit.)</p>

<p>In order to run this application, you&rsquo;ll need to set its classpath, and <code>sbt</code> gives you a way to do find out exactly what environment it would be using so you can run the application manually.<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>  First, make sure you have a copy of <a href="https://github.com/paulp/sbt-extras/blob/master/sbt"><code>sbt-extras</code></a> either in your repository or somewhere pointed to by <code>SBT</code> in your environment.  Then, create a shell script that looks like this:</p>

<figure class='code'><figcaption><span>repl.sh</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c">#!/bin/sh</span>
</span><span class='line'>
</span><span class='line'><span class="c"># set SBT to the location of a current sbt-extras script,</span>
</span><span class='line'><span class="c"># or bundle one in your repository</span>
</span><span class='line'><span class="nb">export </span><span class="nv">SBT</span><span class="o">=</span><span class="k">${</span><span class="nv">SBT</span><span class="k">:-</span><span class="p">./sbt</span><span class="k">}</span>
</span><span class='line'><span class="nb">export </span><span class="nv">SCALA_VERSION</span><span class="o">=</span><span class="k">$(${</span><span class="nv">SBT</span><span class="k">}</span> <span class="s2">&quot;export scalaVersion&quot;</span> <span class="p">|</span> tail -1<span class="k">)</span>
</span><span class='line'><span class="nb">export </span><span class="nv">APP_CP</span><span class="o">=</span><span class="k">$(${</span><span class="nv">SBT</span><span class="k">}</span> -batch -q <span class="s2">&quot;export compile:dependencyClasspath&quot;</span> <span class="p">|</span> tail -1<span class="k">)</span>
</span><span class='line'><span class="nb">export </span><span class="nv">JLINE_CP</span><span class="o">=</span><span class="k">$(</span>find <span class="nv">$HOME</span>/.ivy2 <span class="p">|</span> grep org.scala-lang/jline <span class="p">|</span> grep <span class="k">${</span><span class="nv">SCALA_VERSION</span><span class="k">}</span>.jar<span class="nv">$ </span><span class="p">|</span> tail -1<span class="k">)</span>
</span><span class='line'>
</span><span class='line'><span class="k">${</span><span class="nv">SBT</span><span class="k">}</span> package <span class="o">&amp;&amp;</span> java -cp <span class="k">${</span><span class="nv">APP_CP</span><span class="k">}</span>:<span class="k">${</span><span class="nv">JLINE_CP</span><span class="k">}</span> ReplApp
</span><span class='line'>stty sane
</span></code></pre></td></tr></table></div></figure>


<p>You can then run <code>repl.sh</code> and get a Scala REPL that has all of your app&rsquo;s dependencies and classes loaded <em>and</em> will let you experiment with structured data manipulation in Spark.</p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p>Frustratingly, apps that use these features will work, since the classes Scala reflection depends on will be loaded by the bootstrap classloader, and test cases will work as long as you have <code>sbt</code> fork a new JVM to execute them!  Unfortunately, <code>sbt</code> <a href="https://github.com/sbt/sbt/issues/1918">cannot currently fork a new JVM to run a console</a>.<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
<li id="fn:2">
<p>The <code>run-main</code> task is the right way to run most applications from <code>sbt</code>, but it seems to be somewhat flaky when launching interactive console applications.<a href="#fnref:2" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Hygienic Closures for Scala Function Serialization]]></title>
      <link href="http://erikerlandson.github.com/blog/2015/03/31/hygienic-closures-for-scala-function-serialization/"/>
      <updated>2015-03-31T13:06:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2015/03/31/hygienic-closures-for-scala-function-serialization</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>In most use cases of Scala closures, what you see is what you get, but there are exceptions where looks can be deceiving and this can have a big impact on closure serialization.  Closure serialization is of more than academic interest.  Tools like Apache Spark cannot operate without serializing functions over the network.  In this post I'll describe some scenarios where closures include more than what is evident in the code, and then a technique for preventing unwanted inclusions.</p>

<p>To establish a bit of context, consider this simple example that obtains a function and serializes it to disk, and which <em>does</em> behave as expected:</p>

<pre><code>object Demo extends App {
  def write[A](obj: A, fname: String) {
    import java.io._
    new ObjectOutputStream(new FileOutputStream(fname)).writeObject(obj)
  }

  object foo {
    val v = 42
    // The returned function includes 'v' in its closure
    def f() = (x: Int) =&gt; v * x
  }

  // The function 'f' will serialize as expected
  val f = foo.f
  write(f, "/tmp/demo.f")
}
</code></pre>

<p>When this app is compiled and run, it will serialize <code>f</code> to "/tmp/demo.f1", which of course includes the value of <code>v</code> as part of the closure for <code>f</code>.</p>

<pre><code>$ scalac -d /tmp closures.scala
$ scala -cp /tmp Demo
$ ls /tmp/demo*
/tmp/demo.f
</code></pre>

<p>Now, imagine you wanted to make a straightforward change, where <code>object foo</code> becomes <code>class foo</code>:</p>

<pre><code>object Demo extends App {
  def write[A](obj: A, fname: String) {
    import java.io._
    new ObjectOutputStream(new FileOutputStream(fname)).writeObject(obj)
  }

  // foo is a class instead of an object
  class foo() {
    val v = 42
    // The returned function includes 'v' in its closure, but also a secret surprise
    def f() = (x: Int) =&gt; v * x
  }

  // This will throw an exception!
  val f = new foo().f
  write(f, "/tmp/demo.f")
}
</code></pre>

<p>It would be reasonable to expect that this minor variation behaves exactly as the previous one, but instead it throws an exception!</p>

<pre><code>$ scalac -d /tmp closures.scala
$ scala -cp /tmp Demo
java.io.NotSerializableException: Demo$foo
</code></pre>

<p>If we look at the exception message, we see that it's complaining about not knowing how to serialize objects of class <code>foo</code>.  But we weren't including any values of <code>foo</code> in the closure for <code>f</code>, only a particular member 'v'!  What gives?  Scala is not very helpful with diagnosing this problem, but when a class member value shows up in a closure that is defined <em>inside</em> the class body, the <em>entire instance</em>, including any and all other member values, is included in the closure.  Presumably this is because a class may have any number of instances, and the compiler is including the entire instance in the closure to properly resolve the correct member value.</p>

<p>One straightforward way to fix this is to simply make class <code>foo</code> serializable:</p>

<pre><code>class foo() extends Serializable {
  // ...
}
</code></pre>

<p>If you make this change to the above code, the example with <code>class foo</code> now works correctly, but it is working by serializing the entire <code>foo</code> instance, not just the value of <code>v</code>.</p>

<p>In many cases, this is not a problem and will work fine.  Serializing a few additional members may be inexpensive.  In other cases, however, it can be an impractical or impossible option.  For example, <code>foo</code> might include other very large members, which will be expensive or outright impossible to serialize:</p>

<pre><code>class foo() extends Serializable {
  val v = 42    // easy to serialize
  val w = 4.5   // easy to serialize
  val data = (1 to 1000000000).toList  // serialization landmine hiding in your closure

  // The returned function includes all of 'foo' instance in its closure
  def f() = (x: Int) =&gt; v * x
}
</code></pre>

<p>A variation on the above problem is class members that are small or moderate in size, but serialized many times.  In this case, the serialization cost can become intractable via repetition of unwanted inclusions.</p>

<p>Another potential problem is class members that are not serializable, and perhaps not under your control:</p>

<pre><code>class foo() extends Serializable {
  import some.class.NotSerializable

  val v = 42                      // easy to serialize
  val x = new NotSerializable     // I'll hide in your closure and fail to serialize

  // The returned function includes all of 'foo' instance in its closure
  def f() = (x: Int) =&gt; v * x
}
</code></pre>

<p>There is a relatively painless way to decouple values from their parent instance, so that only desired values are included in a closure.  Passing desired values as parameters to a shim function whose job is to assemble the closure will prevent the parent instance from being pulled into the closure.  In the following example, a shim function named <code>closureFunction</code> is defined for this purpose:</p>

<pre><code>object Demo extends App {
  def write[A](obj: A, fname: String) {
    import java.io._
    new ObjectOutputStream(new FileOutputStream(fname)).writeObject(obj)
  }

  // apply a generator to create a function with safe decoupled closures
  def closureFunction[E,D,R](enclosed: E)(gen: E =&gt; (D =&gt; R)) = gen(enclosed)

  class NotSerializable {}

  class foo() {
    val v1 = 42
    val v2 = 73
    val n = new NotSerializable

    // use shim function to enclose *only* the values of 'v1' and 'v2'
    def f() = closureFunction((v1, v2)) { enclosed =&gt;
      val (v1, v2) = enclosed
      (x: Int) =&gt; (v1 + v2) * x   // Desired function, with 'v1' and 'v2' enclosed
    }
  }

  // This will work!
  val f = new foo().f
  write(f, "/tmp/demo.f")
}
</code></pre>

<p>Being aware of the scenarios where parent instances are pulled into closures, and how to keep your closures clean, can save some frustration and wasted time.  Happy programming!</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[ HTCondor helps astronomers with the hydrogen location problem ( March 26, 2015 )]]></title>
      <link href="http://www.news.wisc.edu/23594"/>
      <updated>2015-03-26T05:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[This  UW Madison news article discusses how a new computational approach permits evaluation of hydrogen data using software, which may replace the time consuming manual approach. Putting HTCondor into the mix scales well given the vast quantities of data expected as Square Kilometer Array radio telescope is realized.
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[ HTCondor Week 2015 Registration Open (March 25, 2015 )]]></title>
      <link href="http://research.cs.wisc.edu/htcondor/HTCondorWeek2015/"/>
      <updated>2015-03-25T05:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[We invite HTCondor users, administrators, and developers to HTCondor Week 2015, our annual HTCondor user conference, in beautiful Madison, Wisconsin, May 19-22, 2015. HTCondor Week features tutorials and talks from HTCondor developers, administrators, and users.  It also provides an opportunity for one-on-one or small group collaborations throughout the week.
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[ HTCondor 8.3.4 released! ( March 5, 2015 )]]></title>
      <link href="manual/v8.3.4/10_3Development_Release.html"/>
      <updated>2015-03-05T06:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[The HTCondor team is pleased to announce the release of HTCondor 8.3.4.
This development release contains a single bug fix for a problem introduced
in version 8.3.3 that can cause jobs to not be matched to resources when the
condor_schedd is flocking.
This 8.3.4 release also has a known issue which prevents a mixed mode,
IPv4 and IPv6 HTCondor installation from starting jobs. If using IPv4
and IPv6 in mixed mode communication is required, please continue using
HTCondor version 8.3.2. The issue will be fixed in version 8.3.5.
Further details can be found in the
Version History.
HTCondor 8.3.4 binaries and source code are available from our
Downloads page.
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Your next favorite collaboration tool]]></title>
      <link href="http://chapeau.freevariable.com/2015/02/shared-decks.html"/>
      <updated>2015-02-13T16:50:17Z</updated>
      <id>http://chapeau.freevariable.com/2015/02/shared-decks</id>
      <author>
        <name><![CDATA[William Benton]]></name>
        <uri>http://chapeau.freevariable.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>Last night I had a crazy realization:  I could probably replace the majority of what my team hopes to accomplish with standup meetings, design documents, project management apps, and social code sharing features with a single tool. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[HTCondor CacheD: Caching for HTC - Part 2]]></title>
      <link href="http://derekweitzel.blogspot.com/2015/01/htcondor-cached-caching-for-htc-part-2.html"/>
      <updated>2015-01-25T15:59:00Z</updated>
      <id>tag:blogger.com,1999:blog-3007054864987759910.post-5260378956420164105</id>
      <author>
        <name><![CDATA[Derek Weitzel]]></name>
        <uri>http://derekweitzel.blogspot.com</uri>
      </author>
      <content type="html"><![CDATA[]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Monadic 'break' and 'continue' for Scala Sequence Comprehensions]]></title>
      <link href="http://erikerlandson.github.com/blog/2015/01/24/monadic-break-and-continue-for-scala-sequence-comprehensions/"/>
      <updated>2015-01-24T18:54:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2015/01/24/monadic-break-and-continue-for-scala-sequence-comprehensions</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>Author's note: I've since received some excellent feedback from the Scala community, which I included in some <a href="#notes">end notes</a>. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Condor CacheD: Caching for HTC - Part 1]]></title>
      <link href="http://derekweitzel.blogspot.com/2015/01/condor-cached-caching-for-htc-part-1.html"/>
      <updated>2015-01-22T16:00:00Z</updated>
      <id>tag:blogger.com,1999:blog-3007054864987759910.post-1889975382858537261</id>
      <author>
        <name><![CDATA[Derek Weitzel]]></name>
        <uri>http://derekweitzel.blogspot.com</uri>
      </author>
      <content type="html"><![CDATA[]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Caveat censor]]></title>
      <link href="http://chapeau.freevariable.com/2014/12/caveat-censor.html"/>
      <updated>2014-12-02T16:12:22Z</updated>
      <id>http://chapeau.freevariable.com/2014/12/caveat-censor</id>
      <author>
        <name><![CDATA[William Benton]]></name>
        <uri>http://chapeau.freevariable.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>Over eight years ago, <a href="http://rwmj.wordpress.com">Richard WM Jones</a> wrote a <a href="http://web.archive.org/web/20071013190833/http://blog.merjis.com/2006/11/08/practical-ocaml/">great but disheartening article about his experience serving as a technical reviewer</a> for <a href="http://www.amazon.com/Practical-OCaml-Joshua-B-Smith/dp/159059620X">an infamous book about OCaml</a>.  The post made quite an impression on me at the time and I&rsquo;ve often recalled it over the years whenever opportunities to do prepress reviews have landed in my inbox.  Briefly, Jones was asked to use terrible tools (Microsoft Word) to deal with stilted, error-ridden prose surrounding unidiomatic and broken code.  For his trouble, he got an hourly wage that would have represented a slight raise over what I made mowing my neighbors&#8217; lawns in high school. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Faster Random Samples With Gap Sampling]]></title>
      <link href="http://erikerlandson.github.com/blog/2014/09/11/faster-random-samples-with-gap-sampling/"/>
      <updated>2014-09-11T14:57:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2014/09/11/faster-random-samples-with-gap-sampling</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>Generating a random sample of a collection is, logically, a O(np) operation, where (n) is the sample size and (p) is the sampling probability.  For example, extracting a random sample, without replacement, from an array might look like this in pseudocode: [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Getting Started with Mesos on Fedora 21 and CentOS 7]]></title>
      <link href="http://timothysc.github.com/blog/2014/09/08/mesos-breeze/"/>
      <updated>2014-09-08T15:00:00Z</updated>
      <id>http://timothysc.github.com/blog/2014/09/08/mesos-breeze</id>
      <author>
        <name><![CDATA[Timothy St. Clair]]></name>
        <uri>http://timothysc.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p><img class="left" src="http://timothysc.github.com/images/mesos_logo.png"> [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[The Scala Iterator 'drop' Method Generates a Matryoshka Class Nesting]]></title>
      <link href="http://erikerlandson.github.com/blog/2014/09/03/matryoshka-class-construction-from-the-scala-iterator-drop-method/"/>
      <updated>2014-09-04T00:23:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2014/09/03/matryoshka-class-construction-from-the-scala-iterator-drop-method</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>The Scala Iterator <code>drop</code> method has a complexity bug that shows up when one calls <code>drop</code> repeatedly, for example when traversing over an iterator in a loop. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Implementing Parallel Prefix Scan as a Spark RDD Transform]]></title>
      <link href="http://erikerlandson.github.com/blog/2014/08/12/implementing-parallel-prefix-scan-as-a-spark-rdd-transform/"/>
      <updated>2014-08-12T18:37:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2014/08/12/implementing-parallel-prefix-scan-as-a-spark-rdd-transform</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>In my <a href="/blog/2014/08/09/implementing-an-rdd-scanleft-transform-with-cascade-rdds/">previous post</a>, I described how to implement the Scala <code>scanLeft</code> function as an RDD transform.  By definition <code>scanLeft</code> invokes a sequential-only prefix scan algorithm; it does not assume that either its input function <code>f</code> or its initial-value <code>z</code> can be applied in a parallel fashion.   Its companion function <code>scan</code>, however, computes a <em>parallel</em> prefix scan.  In this post I will describe an implementation of parallel prefix <code>scan</code> as an RDD transform. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[GPUs on the OSG]]></title>
      <link href="http://derekweitzel.blogspot.com/2014/06/gpus-on-osg.html"/>
      <updated>2014-06-25T18:36:00Z</updated>
      <id>tag:blogger.com,1999:blog-3007054864987759910.post-6396978665247392483</id>
      <author>
        <name><![CDATA[Derek Weitzel]]></name>
        <uri>http://derekweitzel.blogspot.com</uri>
      </author>
      <content type="html"><![CDATA[]]></content>
    </entry>
  
</feed>
