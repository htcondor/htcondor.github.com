<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[Planet HTCondor Meta Feed]]></title>
  <link href="http://htcondor.github.com/planet/atom.xml" rel="self"/>
  <link href="http://htcondor.github.com/"/>
  <updated>2015-08-09T03:07:13-07:00</updated>
  <id>http://htcondor.github.com/planet/atom.xml</id>
  <author>
    <name><![CDATA[HTCondor Project]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
    <entry>
      <title type="html"><![CDATA[GPUs and adding new resources types to the HTCondor-CE]]></title>
      <link href="http://derekweitzel.blogspot.com/2015/08/gpus-and-adding-new-resources-types-to.html"/>
      <updated>2015-08-07T21:45:00Z</updated>
      <id>tag:blogger.com,1999:blog-3007054864987759910.post-6301136022425730449</id>
      <author>
        <name><![CDATA[Derek Weitzel]]></name>
        <uri>http://derekweitzel.blogspot.com</uri>
      </author>
      <content type="html"><![CDATA[In the past, it has been difficult to add new resource types to the OSG CE, whether it was a Globus GRAM CE or a HTCondor-CE. &nbsp;But, it has just gotten a little bit easier. &nbsp;Today I added HCC's GPU resources to the OSG with this new method.<br /><br />With a (yet unapproved) <a href="https://github.com/opensciencegrid/htcondor-ce/pull/36">pull request</a>, the HTCondor-CE is able to add new resource types by modifying only 2 files, the routes table and scheduler attributes customization script. &nbsp;Previously, it required editing a third python script which had very tricky syntax (python, which spit out ClassAds...). &nbsp;In the following examples, I will demonstrate how to use this new feature with GPUs.      <br /><br /><h3>The Routes</h3><div>Each job submitted to a HTCondor-CE must follow a route from the original job, to the final job submitted to the local batch system. &nbsp;The HTCondor JobRouter is in charge of translating the original job to the final job, according to rules specified in the router configuration. &nbsp;Crane's GPU route is:</div><div><script src="https://gist.github.com/djw8605/1c73cd0fc53c7bf3560b.js?file=CraneGpuRoute.conf"></script></div><div><br /></div>The route submit the job to the local PBS (actually Slurm) scheduler to the grid_gpu partition. &nbsp;Further, it adds a special new attribute:<br /><span style="font-family: Courier New, Courier, monospace;">default_remote_cerequirements = "RequestGpus == 1"</span><br /><span style="font-family: inherit;">This attribute is used in the next section, the local submit attributes script.</span><br /><span style="font-family: inherit;"><br /></span><br /><h3><span style="font-family: inherit;">Local Submit Attributes Script</span></h3><span style="font-family: inherit;">The local submit attributes script translates the remote_cerequirements to the actual scheduler language used at the site. &nbsp;For Crane's GPU configuration, the snippet added for GPUs is:</span><br /><script src="https://gist.github.com/djw8605/1c73cd0fc53c7bf3560b.js?file=pbs_local_submit_attributes.sh"></script><br /><span style="font-family: inherit;">This snippet checks for the&nbsp;</span>existence<span style="font-family: inherit;">&nbsp;of the RequestGpus attribute from the environment, and if detected, will insert several lines into the submit script. &nbsp;It will first add the SLURM line to request a GPU, then it will source the module setup script and load the cuda module.</span><br /><span style="font-family: inherit;"><br /></span><h3><span style="font-family: inherit;">Next Steps</span></h3><div><span style="font-family: inherit;">The next steps for using GPUs on the OSG is to use one of the many frontends that are capable of submitting glideins to the GPU resources at HCC. &nbsp;Currently, the HCC, OSG, <a href="https://osgconnect.net/">OSGConnect</a>, and GLOW frontends are capable of submitting to the GPU resources.</span></div>]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[The more things change, the more they stay the same]]></title>
      <link href="http://derekweitzel.blogspot.com/2015/07/the-more-things-change-more-they-stay.html"/>
      <updated>2015-07-28T15:00:00Z</updated>
      <id>tag:blogger.com,1999:blog-3007054864987759910.post-8394942129716842708</id>
      <author>
        <name><![CDATA[Derek Weitzel]]></name>
        <uri>http://derekweitzel.blogspot.com</uri>
      </author>
      <content type="html"><![CDATA[A lot has happened since I last posted in January.<br /><br /><ol><li>I have successfully defended and submitted my dissertation:&nbsp;<i><a href="http://digitalcommons.unl.edu/computerscidiss/88/">Enabling Distributed Scientific Computing on the Campus</a></i>. &nbsp;I will formally graduate on August 15.</li><li>I have been offered, and accepted, a position with the University of Nebraska - Lincoln Holland Computing Center. &nbsp;I will be working with the Open Science Grid's software &amp; investigations team.</li><li>On a personal note, I am now engaged.</li><li>And I am moving later this year to the HCC Omaha office.</li></ol>Now that I have graduated, I hope to write more blog posts about what I am doing, as well as what is happening in the OSG teams that I am working with.<br /><br />]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[ HTCondor 8.3.6 released! ( June 23, 2015 )]]></title>
      <link href="manual/v8.3.6/10_3Development_Release.html"/>
      <updated>2015-06-23T05:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[The HTCondor team is pleased to announce the release of HTCondor 8.3.6.
This development series release contains new features that are under
development.

Enhancements in the release include:
initial Docker universe support;
IPv4/IPv6 mixed mode support.

Further details can be found in the
Version History.
HTCondor 8.3.6 binaries and source code are available from our
Downloads page.
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[fedmsg talk at Spark Summit]]></title>
      <link href="http://chapeau.freevariable.com/2015/06/summit-fedmsg.html"/>
      <updated>2015-06-15T15:05:27Z</updated>
      <id>http://chapeau.freevariable.com/2015/06/summit-fedmsg</id>
      <author>
        <name><![CDATA[William Benton]]></name>
        <uri>http://chapeau.freevariable.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>I&rsquo;m speaking at Spark Summit today about using Spark to analyze operational data from the Fedora project.  Here are some links to further resources related to my talk: [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Using Spark ML Pipeline transformers]]></title>
      <link href="http://chapeau.freevariable.com/2015/06/using-sparks-ml-pipeline-transformers.html"/>
      <updated>2015-06-14T00:07:58Z</updated>
      <id>http://chapeau.freevariable.com/2015/06/using-sparks-ml-pipeline-transformers</id>
      <author>
        <name><![CDATA[William Benton]]></name>
        <uri>http://chapeau.freevariable.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>In this post, we&rsquo;ll see how to make a simple transformer for <a href="https://spark.apache.org/docs/latest/ml-guide.html">Spark ML Pipelines</a>.  The transformer we&rsquo;ll design will generate a sparse binary feature vector from an array-valued field representing a set. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Bokeh plots from Spark]]></title>
      <link href="http://chapeau.freevariable.com/2015/05/bokeh-plots-from-spark.html"/>
      <updated>2015-05-21T16:05:10Z</updated>
      <id>http://chapeau.freevariable.com/2015/05/bokeh-plots-from-spark</id>
      <author>
        <name><![CDATA[William Benton]]></name>
        <uri>http://chapeau.freevariable.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>This post will show you an extremely simple way to make quick-and-dirty <a href="http://bokeh.pydata.org/en/latest/">Bokeh</a> plots from data you&rsquo;ve generated in Spark, but the basic technique is generally applicable to any data that you&rsquo;re generating in some application that doesn&rsquo;t necessarily link in the Bokeh libraries. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Planning your career like a racing season]]></title>
      <link href="http://chapeau.freevariable.com/2015/05/process-goals.html"/>
      <updated>2015-05-14T21:04:20Z</updated>
      <id>http://chapeau.freevariable.com/2015/05/process-goals</id>
      <author>
        <name><![CDATA[William Benton]]></name>
        <uri>http://chapeau.freevariable.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>Most people set personal and professional goals.  If you work in software, your near-term professional goals might sound like this: [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Parallel K-Medoids Using Scala ParSeq]]></title>
      <link href="http://erikerlandson.github.com/blog/2015/05/06/parallel-k-medoids-using-scala-parseq/"/>
      <updated>2015-05-06T23:33:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2015/05/06/parallel-k-medoids-using-scala-parseq</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>Scala supplies a <a href="http://docs.scala-lang.org/overviews/parallel-collections/overview.html">parallel collections library</a> that was designed to make it easy for a programmer to add parallel computing over the elements in a collection.  In this post, I will describe a case study of applying Scala's parallel collections to cleanly implement multithreading support for training a K-Medoids clustering model. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Elasticsearch and Spark 1.3]]></title>
      <link href="http://chapeau.freevariable.com/2015/04/elasticsearch-and-spark-1-dot-3.html"/>
      <updated>2015-04-30T20:34:37Z</updated>
      <id>http://chapeau.freevariable.com/2015/04/elasticsearch-and-spark-1-dot-3</id>
      <author>
        <name><![CDATA[William Benton]]></name>
        <uri>http://chapeau.freevariable.com/</uri>
      </author>
      <content type="html"><![CDATA[<p><a href="https://www.elastic.co/products/elasticsearch">Elasticsearch</a> has offered Hadoop <code>InputFormat</code> and <code>OutputFormat</code> implementations for quite some time.  These made it possible to process Elasticsearch indices with Spark just as you would any other Hadoop data source.  Here&rsquo;s an example of this in action, taken from <a href="http://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html">Elastic&rsquo;s documentation</a>: [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[ HTCondor 8.3.5 released! ( April 20, 2015 )]]></title>
      <link href="manual/v8.3.5/10_3Development_Release.html"/>
      <updated>2015-04-20T05:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[The HTCondor team is pleased to announce the release of HTCondor 8.3.5.
A development series release contains new features that are under
development. This release contains all of the bug fixes from the 8.2.8
stable release.
A few of the enhancements in this release include:
new features that increase the power of job specification in the
submit description file;
RPMs for Red Hat Enterprise Linux 6 and 7 are modularized and only
distributed via our YUM repository;
The new condor-all RPM requires
the other HTCondor RPMs of a typical HTCondor installation.
Further details can be found in the
Version History.
HTCondor 8.3.5 binaries and source code are available from our
Downloads page.
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[ HTCondor 8.2.8 released! ( April 7, 2015 )]]></title>
      <link href="manual/v8.2.8/10_3Stable_Release.html"/>
      <updated>2015-04-07T05:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[The HTCondor team is pleased to announce the release of HTCondor version 8.2.8.
A stable series release contains significant bug and security fixes.
This version contains:
a bug fix to reconnect a TCP session when an HTCondorView collector restarts;
a bug fix to avoid starting too many jobs, only to kill some chosen at random.
A complete list of fixed bugs can be found in the
Version History.
HTCondor 8.2.8 binaries and source code are available from our
Downloads page.
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Hygienic Closures for Scala Function Serialization]]></title>
      <link href="http://erikerlandson.github.com/blog/2015/03/31/hygienic-closures-for-scala-function-serialization/"/>
      <updated>2015-03-31T13:06:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2015/03/31/hygienic-closures-for-scala-function-serialization</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>In most use cases of Scala closures, what you see is what you get, but there are exceptions where looks can be deceiving and this can have a big impact on closure serialization.  Closure serialization is of more than academic interest.  Tools like Apache Spark cannot operate without serializing functions over the network.  In this post I'll describe some scenarios where closures include more than what is evident in the code, and then a technique for preventing unwanted inclusions. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[ HTCondor helps astronomers with the hydrogen location problem ( March 26, 2015 )]]></title>
      <link href="http://www.news.wisc.edu/23594"/>
      <updated>2015-03-26T05:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[This  UW Madison news article discusses how a new computational approach permits evaluation of hydrogen data using software, which may replace the time consuming manual approach. Putting HTCondor into the mix scales well given the vast quantities of data expected as Square Kilometer Array radio telescope is realized.
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[ HTCondor Week 2015 Registration Open (March 25, 2015 )]]></title>
      <link href="http://research.cs.wisc.edu/htcondor/HTCondorWeek2015/"/>
      <updated>2015-03-25T05:00:00Z</updated>
      <id></id>
      <author>
        <name><![CDATA[HTCondor Team]]></name>
        <uri>http://research.cs.wisc.edu/htcondor</uri>
      </author>
      <content type="html"><![CDATA[We invite HTCondor users, administrators, and developers to HTCondor Week 2015, our annual HTCondor user conference, in beautiful Madison, Wisconsin, May 19-22, 2015. HTCondor Week features tutorials and talks from HTCondor developers, administrators, and users.  It also provides an opportunity for one-on-one or small group collaborations throughout the week.
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[HTCondor CacheD: Caching for HTC - Part 2]]></title>
      <link href="http://derekweitzel.blogspot.com/2015/01/htcondor-cached-caching-for-htc-part-2.html"/>
      <updated>2015-01-25T15:59:00Z</updated>
      <id>tag:blogger.com,1999:blog-3007054864987759910.post-5260378956420164105</id>
      <author>
        <name><![CDATA[Derek Weitzel]]></name>
        <uri>http://derekweitzel.blogspot.com</uri>
      </author>
      <content type="html"><![CDATA[]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Monadic 'break' and 'continue' for Scala Sequence Comprehensions]]></title>
      <link href="http://erikerlandson.github.com/blog/2015/01/24/monadic-break-and-continue-for-scala-sequence-comprehensions/"/>
      <updated>2015-01-24T18:54:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2015/01/24/monadic-break-and-continue-for-scala-sequence-comprehensions</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>Author's note: I've since received some excellent feedback from the Scala community, which I included in some <a href="#notes">end notes</a>. [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Condor CacheD: Caching for HTC - Part 1]]></title>
      <link href="http://derekweitzel.blogspot.com/2015/01/condor-cached-caching-for-htc-part-1.html"/>
      <updated>2015-01-22T16:00:00Z</updated>
      <id>tag:blogger.com,1999:blog-3007054864987759910.post-1889975382858537261</id>
      <author>
        <name><![CDATA[Derek Weitzel]]></name>
        <uri>http://derekweitzel.blogspot.com</uri>
      </author>
      <content type="html"><![CDATA[]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Faster Random Samples With Gap Sampling]]></title>
      <link href="http://erikerlandson.github.com/blog/2014/09/11/faster-random-samples-with-gap-sampling/"/>
      <updated>2014-09-11T14:57:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2014/09/11/faster-random-samples-with-gap-sampling</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>Generating a random sample of a collection is, logically, a O(np) operation, where (n) is the sample size and (p) is the sampling probability.  For example, extracting a random sample, without replacement, from an array might look like this in pseudocode: [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[Getting Started with Mesos on Fedora 21 and CentOS 7]]></title>
      <link href="http://timothysc.github.com/blog/2014/09/08/mesos-breeze/"/>
      <updated>2014-09-08T15:00:00Z</updated>
      <id>http://timothysc.github.com/blog/2014/09/08/mesos-breeze</id>
      <author>
        <name><![CDATA[Timothy St. Clair]]></name>
        <uri>http://timothysc.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p><img class="left" src="http://timothysc.github.com/images/mesos_logo.png"> [...]</p>
]]></content>
    </entry>
  
    <entry>
      <title type="html"><![CDATA[The Scala Iterator 'drop' Method Generates a Matryoshka Class Nesting]]></title>
      <link href="http://erikerlandson.github.com/blog/2014/09/03/matryoshka-class-construction-from-the-scala-iterator-drop-method/"/>
      <updated>2014-09-04T00:23:00Z</updated>
      <id>http://erikerlandson.github.com/blog/2014/09/03/matryoshka-class-construction-from-the-scala-iterator-drop-method</id>
      <author>
        <name><![CDATA[Erik Erlandson]]></name>
        <uri>http://erikerlandson.github.com/</uri>
      </author>
      <content type="html"><![CDATA[<p>The Scala Iterator <code>drop</code> method has a complexity bug that shows up when one calls <code>drop</code> repeatedly, for example when traversing over an iterator in a loop. [...]</p>
]]></content>
    </entry>
  
</feed>
